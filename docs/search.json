[
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Philadelphia EV Charging Infrastructure Gap Analysis",
    "section": "Project Overview",
    "text": "Project Overview\nThis analysis examines the spatial equity of electric vehicle (EV) charging infrastructure in Philadelphia, identifying service gaps and priority areas for future development. Using network analysis and socioeconomic data, I evaluate the current distribution of charging stations and develop a robust methodology to prioritize areas with the greatest need for infrastructure investment.",
    "crumbs": [
      "Main",
      "Home"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Philadelphia EV Charging Infrastructure Gap Analysis",
    "section": "Research Questions",
    "text": "Research Questions\nThis analysis seeks to answer the following key questions:\n\nHow equitably is Philadelphia’s EV charging infrastructure currently distributed?\nWhat demographic factors correlate with charging station access?\nWhich areas of Philadelphia have the greatest need for new charging infrastructure?\nHow can future infrastructure development better serve all communities?",
    "crumbs": [
      "Main",
      "Home"
    ]
  },
  {
    "objectID": "index.html#key-findings",
    "href": "index.html#key-findings",
    "title": "Philadelphia EV Charging Infrastructure Gap Analysis",
    "section": "Key Findings",
    "text": "Key Findings\n\nSpatial Distribution: EV charging infrastructure is heavily concentrated in Center City and University City, with significant gaps in North and West Philadelphia\nDemographic Patterns:\n\nHigh-density areas have the lowest coverage (29.6% compared to 42.4% for low-density areas)\nMedium-income areas have the highest coverage (49.2%), while low-income areas have the least (16.8%)\n\nGap Analysis: 11.9% of census tracts fall into the Critical Priority category, requiring immediate attention\nEquity Assessment: Education level shows the strongest statistical relationship with charging access",
    "crumbs": [
      "Main",
      "Home"
    ]
  },
  {
    "objectID": "index.html#methodology-highlights",
    "href": "index.html#methodology-highlights",
    "title": "Philadelphia EV Charging Infrastructure Gap Analysis",
    "section": "Methodology Highlights",
    "text": "Methodology Highlights\nThe analysis employs a dynamic weighting system that responds to observed disparities:\n\nNetwork Analysis: Walking and driving distances calculated using OpenStreetMap network data\nGap Score Components:\n\nPhysical Coverage (24%)\nIncome Level (16%)\nPoverty Rate (24%)\nPopulation Density (20%)\nEducation Level (16%)\n\nService Areas: Multi-buffer approach including walking (0.25-0.75 miles) and driving (1-3 miles) distances",
    "crumbs": [
      "Main",
      "Home"
    ]
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "Philadelphia EV Charging Infrastructure Gap Analysis",
    "section": "Project Structure",
    "text": "Project Structure\n\nMethods: Detailed documentation of data sources, preprocessing steps, and analytical techniques\nResults: Key findings across spatial, demographic, and equity dimensions\nInteractive Dashboard: Explore infrastructure gaps and priority areas dynamically\nConclusions: Recommendations for equitable infrastructure development\n\n\nHome Methodology Results Dashboard Conclusions",
    "crumbs": [
      "Main",
      "Home"
    ]
  },
  {
    "objectID": "code/Final_Code_.html",
    "href": "code/Final_Code_.html",
    "title": "Equitable EV Infrastructure Analysis: Philadelphia",
    "section": "",
    "text": "# Setup\n\n# Standard library\nimport functools\nimport os\nfrom multiprocessing import Pool\nfrom dotenv import load_dotenv\n\n# Scientific/data analysis\nimport numpy as np\nimport pandas as pd\nfrom rtree import index\nfrom scipy import stats\n\n# Geospatial\nimport geopandas as gpd\nfrom shapely.geometry import Point, LineString\nfrom shapely.ops import unary_union\n\n# Network analysis\nimport networkx as nx\nimport osmnx as ox\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport folium\nfrom IPython.display import display\n\n# API/requests\nimport requests\n\n# Load API key from env\nload_dotenv()\napi_key = os.getenv('OPENCHARGE_API_KEY')\n\n# Philadelphia boundary\nprint(\"Starting Philadelphia EV station data collection...\")\n\n# Philadelphia boundary\nurl = \"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\"\ndistricts = gpd.read_file(url).to_crs('EPSG:4326')\nphilly_boundary = districts['geometry'].union_all()\n\nif not api_key:\n    print(\"Error: No API key found. Please check .env file\")\n    raise Exception(\"API key not found\")\n\nbase_url = \"https://api.openchargemap.io/v3/poi\"\nparams = {\n    'key': api_key,\n    'countrycode': 'US',\n    'maxresults': 1000,\n    'latitude': 39.9526,  # Philadelphia center \n    'longitude': -75.1652,\n    'distance': 10,\n    'distanceunit': 'km',\n    'compact': True,\n    'verbose': False,\n    'output': 'json'\n}\n\ntry:\n    response = requests.get(base_url, params=params, timeout=10)\n    response.raise_for_status()\n    stations_data = response.json()\n    \n    stations_list = []\n    for station in stations_data:\n        try:\n            station_info = {\n                'id': station.get('ID'),\n                'name': station.get('AddressInfo', {}).get('Title'),\n                'latitude': station.get('AddressInfo', {}).get('Latitude'),\n                'longitude': station.get('AddressInfo', {}).get('Longitude'),\n                'address': station.get('AddressInfo', {}).get('AddressLine1'),\n                'status': station.get('StatusType', {}).get('Title'),\n                'number_of_points': len(station.get('Connections', [])),\n                'usage_type': station.get('UsageType', {}).get('Title')\n            }\n            stations_list.append(station_info)\n        except Exception as e:\n            print(f\"Error processing station: {e}\")\n            continue\n    \n    # Create GeoDataFrame\n    stations_df = pd.DataFrame(stations_list)\n    stations_df = stations_df.dropna(subset=['latitude', 'longitude'])\n    stations_gdf = gpd.GeoDataFrame(\n        stations_df, \n        geometry=gpd.points_from_xy(stations_df.longitude, stations_df.latitude),\n        crs=\"EPSG:4326\"\n    )\n    \n    # Filter to only Philadelphia\n    philly_stations = stations_gdf[stations_gdf.geometry.within(philly_boundary)]\n    print(f\"Successfully retrieved {len(philly_stations)} EV stations within Philadelphia\")\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")\n    raise \n\nStarting Philadelphia EV station data collection...\nSuccessfully retrieved 130 EV stations within Philadelphia\n# Network and Station Map\n\n# Street network within Philadelphia\nG = ox.graph_from_polygon(philly_boundary, network_type=\"drive\")\n\n# Convert network to GeoDataFrame\nedges = ox.graph_to_gdfs(G, nodes=False)\n\n# Base map\nnetwork_station_map = folium.Map(location=[39.9526, -75.1652], zoom_start=12,\n                                tiles='CartoDB positron')\n\n# Add street network\nfolium.GeoJson(\n    edges,\n    style_function=lambda x: {'color': 'gray', 'weight': 1, 'opacity': 0.5},\n    name=\"Street Network\"\n).add_to(network_station_map)\n\n# Add station points\nfor idx, row in philly_stations.iterrows():\n    folium.CircleMarker(\n        location=[row.geometry.y, row.geometry.x],\n        radius=5,\n        color='blue',\n        fill=True,\n        popup=f\"\"\"\n            &lt;b&gt;{row['name']}&lt;/b&gt;&lt;br&gt;\n            Status: {row['status']}&lt;br&gt;\n            Number of Points: {row['number_of_points']}\n        \"\"\",\n        tooltip=row['name']\n    ).add_to(network_station_map)\n\n# Add layer control\nfolium.LayerControl().add_to(network_station_map)\n\ndisplay(network_station_map)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n# Load and prepare Philadelphia census data\nprint(\"Loading Philadelphia census data...\")\ncensus_gdf = gpd.read_file('data/phila_census1.gpkg')\n\n# Filter out tracts with zero population\ncensus_gdf = census_gdf[census_gdf['total_pop'] &gt; 0]\n\n# Filter out tracts with very low population density (&lt; 1000 people per square mile)\ncensus_gdf = census_gdf[census_gdf['pop_density'] &gt;= 1000]\n\n# Set to WGS84 \ncensus_gdf = census_gdf.to_crs('EPSG:4326')\n\nprint(f\"Loaded {len(census_gdf)} census tracts\")\n\n# Analyze demographics - Spatial join with stations\nstations_with_census = gpd.sjoin(\n    stations_gdf,\n    census_gdf,\n    how=\"left\",\n    predicate=\"within\"\n)\n\n# Clean up joined data\nstations_with_census = stations_with_census.drop(['index_right'], axis=1)\n\n# Select only needed columns\ncolumns_to_keep = [\n    'id', 'name', 'latitude', 'longitude', 'address', \n    'status', 'number_of_points', 'usage_type',\n    'GEOID', 'pct_white', 'pct_black', 'pct_hispanic', 'pct_asian', 'median_age',\n    'median_income', 'median_home_value', 'bach_degree_rate', 'pop_density', 'total_pop', 'poverty_rate',\n    'diversity_index', 'gentrification_risk', 'geometry', 'ALAND20'\n]\nstations_with_census = stations_with_census[columns_to_keep]\n\n# Analysis of stations per tract\nstations_per_tract = stations_with_census.groupby('GEOID').size().reset_index(name='station_count')\ncensus_with_stations = census_gdf.merge(stations_per_tract, on='GEOID', how='left')\ncensus_with_stations['station_count'] = census_with_stations['station_count'].fillna(0)\n\n# Create map showing EV stations distribution by census tract\nstations_per_tract_map = folium.Map(location=[39.9526, -75.1652], zoom_start=12,\n                                  tiles='CartoDB positron')\n\n# Add choropleth layer\nchoropleth = folium.Choropleth(\n    geo_data=census_with_stations,\n    name='Stations per Tract',\n    data=census_with_stations,\n    columns=['GEOID', 'station_count'],\n    key_on='feature.properties.GEOID',\n    fill_color='YlOrRd',\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name='Number of EV Stations'\n).add_to(stations_per_tract_map)\n\n# Create station layer\nstation_group = folium.FeatureGroup(name=\"EV Stations\")\n\n# Add station points\nfor idx, row in philly_stations.iterrows():\n    folium.CircleMarker(\n        location=[row.geometry.y, row.geometry.x],\n        radius=5,\n        color='blue',\n        fill=True,\n        popup=f\"\"\"\n            &lt;b&gt;{row['name']}&lt;/b&gt;&lt;br&gt;\n            Status: {row['status']}&lt;br&gt;\n            Number of Points: {row['number_of_points']}\n        \"\"\",\n        tooltip=row['name']\n    ).add_to(station_group)\n\n# Add layers to map\nstation_group.add_to(stations_per_tract_map)\nfolium.LayerControl().add_to(stations_per_tract_map)\n\n# map\ndisplay(stations_per_tract_map)\n\n# Analysis summary\nprint(\"\\nStations per tract Analysis Summary:\")\nprint(f\"Total census tracts: {len(census_with_stations)}\")\nprint(f\"Tracts with stations: {len(census_with_stations[census_with_stations['station_count'] &gt; 0])}\")\nprint(f\"Average stations per tract: {census_with_stations['station_count'].mean():.2f}\")\n\nLoading Philadelphia census data...\nLoaded 385 census tracts\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nStations per tract Analysis Summary:\nTotal census tracts: 385\nTracts with stations: 50\nAverage stations per tract: 0.26\nThe analysis examines Philadelphia’s EV charging infrastructure, which consists of 130 charging stations. The city has 408 total census tracts, but when focusing on residential areas (excluding tracts with zero population and those with population density below 1,000 people per square mile), we analyze 385 tracts. The distribution of charging stations is highly concentrated - only 50 tracts (13.0% of residential tracts) contain any charging stations, with an average of 0.26 stations per tract. This spatial concentration means that more than 87% of Philadelphia’s residential census tracts lack any charging infrastructure, highlighting significant gaps in coverage and accessibility.\nTo ensure our analysis focuses on residential areas where EV charging infrastructure would be most impactful, we applied two filters to the census tract data. First, we excluded tracts with zero population to focus on inhabited areas. Second, we excluded tracts with population density below 1,000 people per square mile, as these very low-density areas (which may include industrial zones, large parks, or areas with significant non-residential land use) have different infrastructure needs and characteristics. This filtering helps ensure our analysis reflects the distribution of EV charging infrastructure in areas where residents would most benefit from access to charging stations.\n# Define variables of interest\nvariables = {\n    'median_income': 'Median Income ($)',\n    'poverty_rate': 'Poverty Rate (%)',\n    'pop_density': 'Population Density',\n    'total_pop': 'Total Population',\n    'bach_degree_rate': 'Bachelor\\'s Degree (%)',\n    'median_age': 'Median Age'\n}\n\n# Calculate stats for tracts with and without stations\nhas_stations = census_with_stations['station_count'] &gt; 0\nsummary_stats = pd.DataFrame({\n    'With Stations': [census_with_stations[has_stations][var].mean() for var in variables],\n    'Without Stations': [census_with_stations[~has_stations][var].mean() for var in variables]\n}, index=variables.values())\n\n# Round the results to 2 decimal places and set the table name\nsummary_stats = summary_stats.round(2)\nsummary_stats.name = \"Demographic Characteristics of Census Tracts With and Without Bike Share Stations\"\n\ndisplay(summary_stats)\n\n\n\n\n\n\n\n\nWith Stations\nWithout Stations\n\n\n\n\nMedian Income ($)\n82178.84\n60956.00\n\n\nPoverty Rate (%)\n18.74\n22.76\n\n\nPopulation Density\n23432.06\n20387.71\n\n\nTotal Population\n3190.46\n4271.75\n\n\nBachelor's Degree (%)\n24.84\n12.66\n\n\nMedian Age\n34.31\n36.53\nThe demographic comparison between areas with and without EV charging stations reveals significant disparities in socioeconomic characteristics. Areas with charging stations show notably higher median incomes ($82,179 compared to $61,136) and higher educational attainment (25% versus 13% bachelor’s degree rates), while experiencing lower poverty rates (18.7% versus 22.8%). These disparities suggest that EV infrastructure deployment has favored more affluent and educated neighborhoods.\nPopulation characteristics also differ between served and unserved areas. Areas with stations have higher population density (22,974 versus 20,093 people per square mile) but smaller total populations (3,128 versus 4,217 residents). The median age difference is modest, with areas without stations being slightly older (36.6 versus 34.4 years).\nThese patterns indicate that current EV charging infrastructure may be reinforcing existing socioeconomic inequities. Despite higher population density in areas with stations, they serve smaller total populations and are concentrated in wealthier, more educated areas with smaller total populations, potentially limiting EV adoption opportunities in less advantaged communities. This analysis supports the need for more equitable distribution of future EV charging infrastructure to ensure broader access across all demographic groups.\n# Calculate station density (stations per square mile)\ncensus_with_stations['station_density'] = census_with_stations['station_count'] / (census_with_stations['ALAND20'] / 2589988.11)\n\n# Define demographic variables\ndemographic_vars_display = {\n    'median_income': 'Median Income',\n    'poverty_rate': 'Poverty Rate',\n    'pop_density': 'Population Density',\n    'bach_degree_rate': 'Bachelor\\'s Degree Rate',\n    'diversity_index': 'Diversity Index',\n    'gentrification_risk': 'Gentrification Risk',\n    'median_home_value': 'Median Home Value',\n    'station_density': 'Station Density'\n}\n\n# Calculate correlations\ncorrelations = census_with_stations[demographic_vars_display.keys()].corr()['station_density'].sort_values(ascending=False)\n\n# Correlation table\ncorrelation_table = pd.DataFrame({\n    'Variable': [demographic_vars_display[var] for var in correlations.index],\n    'Correlation with Station Density': correlations.values\n})\n\ndisplay(correlation_table.style\n    .set_caption(\"Table 2: Correlation Analysis between Philadelphia EV Charging Station Density and Demographic Factors\")\n    .format(precision=3))\n\n# Summary statistics table\nsummary = census_with_stations[demographic_vars_display.keys()].describe()\nsummary.columns = [demographic_vars_display[col] for col in summary.columns]\n\ndisplay(summary.style\n    .set_caption(\"Table 3: Descriptive Statistics of Demographic and EV Charging Station Variables in Philadelphia Census Tracts\")\n    .format(precision=2))\n\n\n\n\n\n\nTable 1: Table 2: Correlation Analysis between Philadelphia EV Charging Station Density and Demographic Factors\n\n\n\n\n\n \nVariable\nCorrelation with Station Density\n\n\n\n\n0\nStation Density\n1.000\n\n\n1\nGentrification Risk\n0.389\n\n\n2\nBachelor's Degree Rate\n0.370\n\n\n3\nMedian Home Value\n0.350\n\n\n4\nMedian Income\n0.249\n\n\n5\nPopulation Density\n0.211\n\n\n6\nDiversity Index\n0.057\n\n\n7\nPoverty Rate\n-0.122\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Table 3: Descriptive Statistics of Demographic and EV Charging Station Variables in Philadelphia Census Tracts\n\n\n\n\n\n \nMedian Income\nPoverty Rate\nPopulation Density\nBachelor's Degree Rate\nDiversity Index\nGentrification Risk\nMedian Home Value\nStation Density\n\n\n\n\ncount\n380.00\n385.00\n385.00\n385.00\n385.00\n373.00\n373.00\n385.00\n\n\nmean\n63748.48\n22.24\n20783.08\n14.24\n0.42\n0.22\n261076.94\n2.12\n\n\nstd\n31723.12\n14.27\n12166.68\n10.26\n0.19\n0.08\n171054.63\n8.55\n\n\nmin\n14983.00\n0.72\n1091.34\n0.00\n0.02\n0.05\n46700.00\n0.00\n\n\n25%\n39601.75\n11.14\n12014.76\n6.20\n0.27\n0.16\n131500.00\n0.00\n\n\n50%\n58432.00\n19.69\n18781.08\n11.76\n0.43\n0.20\n222300.00\n0.00\n\n\n75%\n82571.00\n31.18\n27290.24\n20.72\n0.59\n0.26\n321000.00\n0.00\n\n\nmax\n181066.00\n78.18\n92575.12\n44.25\n0.77\n0.53\n1036700.00\n94.71\nThe correlation analysis reveals important patterns in Philadelphia’s EV charging infrastructure distribution. Gentrification risk shows the strongest positive correlation with station density (0.389), followed closely by bachelor’s degree rate (0.375) and median home value (0.346). These relationships suggest that EV charging stations are more concentrated in areas experiencing gentrification and higher educational attainment. Moderate positive correlations with median income (0.247) and population density (0.204) indicate that stations tend to be located in more affluent and densely populated areas, though these relationships are less pronounced. The diversity index shows almost no correlation (0.047), while poverty rate exhibits a weak negative correlation (-0.122), suggesting that station placement has not strongly considered these equity factors.\nThe descriptive statistics provide additional context about Philadelphia’s socioeconomic landscape. Median income varies substantially across census tracts, ranging from $14,983 to $181,066 (mean: $63,748), highlighting significant economic disparities. Poverty rates average 22.24% but reach as high as 78.18% in some tracts, underscoring areas of concentrated poverty. Population density shows considerable variation (mean: 20,783 people per square mile, min: 1,091, max: 92,575), reflecting Philadelphia’s mix of dense urban cores and less populated areas. The bachelor’s degree rate averages 14.24% but ranges from 0% to 44.25%, indicating educational attainment disparities.\nNotably, EV charging station density exhibits highly skewed distribution patterns. While the mean density is 2.12 stations per tract, the median of 0 and maximum of 94.71 reveals that stations are heavily concentrated in select areas, with many tracts having no stations at all. This distribution, combined with the correlation patterns, suggests that current EV infrastructure deployment may be reinforcing existing socioeconomic disparities rather than addressing them."
  },
  {
    "objectID": "code/Final_Code_.html#census-correlation-analysis",
    "href": "code/Final_Code_.html#census-correlation-analysis",
    "title": "Equitable EV Infrastructure Analysis: Philadelphia",
    "section": "Census Correlation Analysis",
    "text": "Census Correlation Analysis\n\nStrong to Weak Positive Correlations:\n\ngentrification_risk (0.39): Strongest positive correlation\nbach_degree_rate (0.37): Areas with more college graduates\nmedian_home_value (0.35): Higher home values\nmedian_income (0.25): Higher income areas\npop_density (0.20): More densely populated areas\ndiversity_index (0.05): Very weak relationship ### Negative Correlation:\npoverty_rate (-0.12): Slight negative relationship\n\n\n\nSummary Statistics Show:\n\n1. Station Density Distribution:\n\nMean: 2.08 stations per square mile\nMedian (50%): 0.00\n75% of tracts have 0 stations\nMaximum: 94.71 stations per square mile This suggests stations are highly concentrated in few areas\n\n\n\n2. Notable Demographics:\n\nMedian Income: $63,882\nPoverty Rate: 22.2% average (0% to 78.2%)\nBachelor's Degree: 14.2% average (0% to 44.2%)\nHome Values: $261,979 average\n\nThis suggests EV stations tend to be located in more affluent, educated areas with higher property values and lower poverty rates."
  },
  {
    "objectID": "code/Final_Code_.html#service-area-analysis",
    "href": "code/Final_Code_.html#service-area-analysis",
    "title": "Equitable EV Infrastructure Analysis: Philadelphia",
    "section": "Service Area Analysis",
    "text": "Service Area Analysis\n\nimport os\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport osmnx as ox\nimport geopy.distance\nimport requests \nfrom scipy import stats\nfrom tqdm import tqdm\nfrom shapely.geometry import Point, LineString, MultiPoint\nfrom shapely.ops import unary_union\nfrom shapely.geometry import Polygon\nfrom dotenv import load_dotenv\nimport random\nfrom datetime import datetime\nimport folium\nfrom shapely.geometry import mapping as shapely_mapping\n\n# Print current working directory\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Set OSMnx cache location explicitly\nox.settings.cache_folder = os.path.join('data', 'cache', 'osmnx')\nprint(f\"OSMnx cache folder: {ox.settings.cache_folder}\")\n\n# For our cache\nCACHE_DIR = os.path.join('data', 'cache')\nos.makedirs(CACHE_DIR, exist_ok=True)\n\n# Set global debug mode (set to True for detailed component outputs)\nDEBUG_MODE = True\n\n# Define global thresholds for gap scoring - used in multiple functions\nTHRESHOLDS = [0.45, 0.55, 0.65]  # Adjusted thresholds for better distribution\n\n# Using NetworkX with point snapping\n\nprint(\"Using NetworkX with point snapping for network analysis.\")\n\ndef load_data():\n    \"\"\"Load census, boundary, and EV station data.\"\"\"\n    load_dotenv()\n    api_key = os.getenv('OPENCHARGE_API_KEY')\n    if not api_key:\n        raise EnvironmentError('OPENCHARGE_API_KEY is not set.')\n\n    # Census data\n    census = gpd.read_file('data/phila_census1.gpkg')\n    census = census[census['total_pop'] &gt; 0]\n    census = census[census['pop_density'] &gt;= 1000]\n    census = census.to_crs('EPSG:4326')\n\n    # City boundary\n    boundary = gpd.read_file('data/City_Limits.geojson').to_crs('EPSG:4326')\n\n    # EV stations\n    params = {\n        'key': api_key,\n        'countrycode': 'US',\n        'maxresults': 1000,\n        'latitude': 39.9526,\n        'longitude': -75.1652,\n        'distance': 10,\n        'distanceunit': 'km',\n        'compact': True,\n        'verbose': False,\n        'output': 'json'\n    }\n    resp = requests.get('https://api.openchargemap.io/v3/poi', params=params, timeout=10)\n    resp.raise_for_status()\n    data = resp.json()\n\n    records = []\n    for s in data:\n        lat = s.get('AddressInfo',{}).get('Latitude')\n        lon = s.get('AddressInfo',{}).get('Longitude')\n        if lat is None or lon is None:\n            continue\n        pt = Point(lon, lat)\n        if not pt.within(boundary.geometry.iloc[0]):\n            continue\n        conns = s.get('Connections', [])\n        max_kw = max((c.get('PowerKW',0) for c in conns), default=0)\n        records.append({\n            'id': s.get('ID'),\n            'name': s.get('AddressInfo',{}).get('Title',''),\n            'num_points': len(conns),\n            'max_power': max_kw,\n            'geometry': pt\n        })\n    stations = gpd.GeoDataFrame(records, crs='EPSG:4326')\n\n    # Project to state plane for spatial operations\n    census = census.to_crs('EPSG:2272')\n    boundary = boundary.to_crs('EPSG:2272')\n    stations = stations.to_crs('EPSG:2272')\n\n    # Add charging speed categories\n    stations['charging_speed'] = pd.cut(\n        stations['max_power'],\n        bins=[-np.inf, 7, 50, np.inf],\n        labels=['Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)']\n    )\n\n    # Validation tracking\n    validation = {\n        'total_stations_from_api': len(data),\n        'stations_within_boundary': len(records),\n        'excluded_stations': len(data) - len(records),\n        'exclusion_reasons': {\n            'outside_boundary': sum(1 for s in data if s.get('AddressInfo',{}).get('Latitude') and\n                                   s.get('AddressInfo',{}).get('Longitude') and\n                                   not Point(s.get('AddressInfo',{}).get('Longitude'), \n                                            s.get('AddressInfo',{}).get('Latitude')).within(boundary.geometry.iloc[0])),\n            'missing_coordinates': sum(1 for s in data if not s.get('AddressInfo',{}).get('Latitude') or\n                                      not s.get('AddressInfo',{}).get('Longitude'))\n        }\n    }\n\n    return census, boundary, stations, validation\n\n\ndef create_networks():\n    \"\"\"Load or build walk and drive networks and cache them.\"\"\"\n    cache = 'data/network_cache.pkl'\n    if os.path.exists(cache):\n        with open(cache,'rb') as f:\n            return pickle.load(f)\n\n    G_walk = ox.graph_from_place('Philadelphia, Pennsylvania', network_type='walk', simplify=False)\n    G_walk = ox.project_graph(G_walk, to_crs='EPSG:2272')\n    G_walk = ox.distance.add_edge_lengths(G_walk)\n\n    G_drive = ox.graph_from_place('Philadelphia, Pennsylvania', network_type='drive', simplify=False)\n    G_drive = ox.project_graph(G_drive, to_crs='EPSG:2272')\n    G_drive = ox.distance.add_edge_lengths(G_drive)\n\n    os.makedirs('data', exist_ok=True)\n    with open(cache,'wb') as f:\n        pickle.dump((G_walk, G_drive),f)\n\n    def check_network_connectivity(network, network_type):\n        \"\"\"Check and report on network connectivity issues.\"\"\"\n        # Find connected components\n        connected_components = list(nx.connected_components(network.to_undirected()))\n        print(f\"{network_type} network has {len(connected_components)} connected components\")\n        \n        # Report on largest component\n        largest_component = max(connected_components, key=len)\n        largest_pct = len(largest_component) / network.number_of_nodes() * 100\n        print(f\"Largest component contains {largest_pct:.1f}% of all nodes\")\n        \n        if len(connected_components) &gt; 1:\n            print(\"WARNING: Network has disconnected components, which may cause routing failures\")\n        \n        return len(connected_components)\n\n    # Add after creating networks:\n    print(\"Checking network connectivity...\")\n    walk_components = check_network_connectivity(G_walk, \"Walking\")\n    drive_components = check_network_connectivity(G_drive, \"Driving\")\n\n    return G_walk, G_drive\n\n\ndef find_nearest_node(network, point_x, point_y, fallback_tolerance=5000):\n    \"\"\"\n    Find the nearest node to a point using robust search methods.\n    Tries OSMnx nearest_nodes first, then falls back to manual distance calculation\n    with an increasing search radius if needed.\n    \n    Parameters:\n    -----------\n    network : networkx.Graph\n        Road network from OSMnx\n    point_x, point_y : float\n        Coordinates of the target point\n    fallback_tolerance : float\n        Maximum distance to search for nodes if nearest_nodes fails\n        \n    Returns:\n    --------\n    int or None\n        Node ID of nearest node, or None if no node found within tolerance\n    \"\"\"\n    # First try with OSMnx's built-in function\n    try:\n        return ox.nearest_nodes(network, point_x, point_y)\n    except Exception as e:\n        if DEBUG_MODE:\n            print(f\"Standard nearest_nodes failed: {e}, trying manual search...\")\n    \n    # Manual fallback with increasing radius\n    try:\n        # Get all nodes and their coordinates\n        nodes = list(network.nodes)\n        if not nodes:\n            if DEBUG_MODE:\n                print(\"No nodes in network!\")\n            return None\n            \n        # Use vectorized operations for efficiency\n        node_coords = np.array([\n            [network.nodes[n].get('x', 0), network.nodes[n].get('y', 0)] \n            for n in nodes\n        ])\n        \n        point_coords = np.array([point_x, point_y])\n        \n        # Calculate squared distances to all nodes\n        squared_distances = np.sum((node_coords - point_coords)**2, axis=1)\n        \n        # Get the node with minimum distance\n        min_idx = np.argmin(squared_distances)\n        min_distance = np.sqrt(squared_distances[min_idx])\n        \n        if min_distance &lt;= fallback_tolerance:\n            if DEBUG_MODE and min_distance &gt; 100:\n                print(f\"Found node at distance {min_distance:.1f} units\")\n            return nodes[min_idx]\n        else:\n            if DEBUG_MODE:\n                print(f\"Nearest node is too far: {min_distance:.1f} units &gt; {fallback_tolerance}\")\n            return None\n            \n    except Exception as e:\n        if DEBUG_MODE:\n            print(f\"Manual nearest node search failed: {e}\")\n        return None\n\n\ndef map_match_point(network, point_x, point_y, tolerance=5000):\n    \"\"\"\n    Map match a point to the nearest edge in the network and return\n    the nearest node along that edge.\n    \n    This provides better connectivity than just finding the nearest node,\n    especially when points are far from actual network nodes.\n    \n    Parameters:\n    -----------\n    network : networkx.Graph\n        Road network from OSMnx\n    point_x, point_y : float\n        Coordinates of the target point\n    tolerance : float\n        Maximum distance to search\n        \n    Returns:\n    --------\n    int or None\n        Node ID of matched node, or None if no match found\n    \"\"\"\n    try:\n        # Create a Point object for the input coordinates\n        point = Point(point_x, point_y)\n        \n        # Find nearest edges, not just nodes\n        best_edge = None\n        best_dist = float('inf')\n        \n        # Check a sample of edges for efficiency (every 10th edge)\n        edges_sample = list(network.edges(data=True))[::10]\n        \n        for u, v, data in edges_sample:\n            if 'geometry' in data:\n                # If edge has a geometry attribute, use it\n                edge_geom = data['geometry']\n                dist = edge_geom.distance(point)\n            else:\n                # Otherwise, create a line between nodes\n                try:\n                    u_x, u_y = network.nodes[u]['x'], network.nodes[u]['y']\n                    v_x, v_y = network.nodes[v]['x'], network.nodes[v]['y']\n                    line = [(u_x, u_y), (v_x, v_y)]\n                    edge_geom = LineString(line)\n                    dist = edge_geom.distance(point)\n                except KeyError:\n                    # Skip if nodes don't have coordinates\n                    continue\n            \n            # Update best match\n            if dist &lt; best_dist:\n                best_dist = dist\n                best_edge = (u, v)\n        \n        # Return the closer node from the best edge\n        if best_edge and best_dist &lt;= tolerance:\n            u, v = best_edge\n            u_dist = ((network.nodes[u]['x'] - point_x)**2 + \n                      (network.nodes[u]['y'] - point_y)**2)**0.5\n            v_dist = ((network.nodes[v]['x'] - point_x)**2 + \n                      (network.nodes[v]['y'] - point_y)**2)**0.5\n            return u if u_dist &lt; v_dist else v\n        \n        # Fallback to regular nearest node if no good edge found\n        return find_nearest_node(network, point_x, point_y, fallback_tolerance=tolerance)\n    \n    except Exception as e:\n        if DEBUG_MODE:\n            print(f\"Map matching failed: {e}\")\n        # Fallback to regular nearest node\n        return find_nearest_node(network, point_x, point_y, fallback_tolerance=tolerance)\n\n\ndef enhance_network_connectivity(G):\n    \"\"\"\n    Enhance network connectivity by ensuring key locations are connected.\n    \n    Parameters:\n    -----------\n    G : networkx.Graph\n        Road network to enhance\n    \n    Returns:\n    --------\n    networkx.Graph\n        Enhanced network\n    \"\"\"\n    # Add self-loops to all nodes to ensure they can be reached from themselves\n    # This helps with isolated nodes\n    for node in G.nodes():\n        if not G.has_edge(node, node):\n            G.add_edge(node, node, length=0)\n    \n    # Check the number of connected components\n    connected_components = list(nx.connected_components(G.to_undirected()))\n    largest_component = max(connected_components, key=len)\n    \n    # If it has multiple components, add edges to connect them to largest component\n    if len(connected_components) &gt; 1:\n        largest_component_nodes = list(largest_component)\n        for component in connected_components:\n            if component != largest_component:\n                # Find the closest node pair between components\n                min_dist = float('inf')\n                best_pair = None\n                \n                # Sample nodes from each component for efficiency\n                component_nodes = list(component)\n                sample_size = min(10, len(component_nodes))\n                sampled_component = random.sample(component_nodes, sample_size)\n                \n                sample_size_largest = min(20, len(largest_component_nodes))\n                sampled_largest = random.sample(largest_component_nodes, sample_size_largest)\n                \n                # Find the closest pair\n                for n1 in sampled_component:\n                    for n2 in sampled_largest:\n                        try:\n                            x1, y1 = G.nodes[n1]['x'], G.nodes[n1]['y']\n                            x2, y2 = G.nodes[n2]['x'], G.nodes[n2]['y']\n                            dist = ((x1-x2)**2 + (y1-y2)**2)**0.5\n                            if dist &lt; min_dist:\n                                min_dist = dist\n                                best_pair = (n1, n2)\n                        except KeyError:\n                            continue\n                \n                # Add an edge between the closest nodes\n                if best_pair:\n                    G.add_edge(best_pair[0], best_pair[1], length=min_dist)\n    \n    return G\n\n\ndef calculate_multimodal_distance_batch(origins, destinations, walk_net, drive_net):\n    \"\"\"\n    Calculate minimum walking and driving distances from origins to destinations using NetworkX.\n    \n    Parameters\n    ----------\n    origins : dict\n        Dictionary mapping IDs to Point geometries for origins\n    destinations : list\n        List of Point geometries for destinations\n    walk_net : networkx.Graph\n        Walking network \n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    tuple\n        (walk_distances, drive_distances) dictionaries mapping (origin_id, destination_idx)\n        to distances in meters\n    \"\"\"\n    # Check if we should use cached distances\n    cache_file = os.path.join(CACHE_DIR, 'distance_cache.pkl')\n    \n    # Create cache dir if needed\n    os.makedirs(CACHE_DIR, exist_ok=True)\n    \n    # Try to load from cache\n    if os.path.exists(cache_file):\n        try:\n            print(\"Loading distances from cache...\")\n            with open(cache_file, 'rb') as f:\n                distances = pickle.load(f)\n            walk_d, drive_d = distances\n            print(f\"Loaded {len(walk_d)} walking and {len(drive_d)} driving distances from cache.\")\n            return walk_d, drive_d\n        except Exception as e:\n            print(f\"Error loading distance cache: {e}\")\n            # Continue with calculation\n    \n    # Calculate distances with NetworkX\n    print(\"Calculating distances with NetworkX...\")\n    walk_distances, drive_distances = calculate_distances_with_networkx(\n        origins, destinations, walk_net, drive_net\n    )\n    \n    # Cache the results\n    try:\n        with open(cache_file, 'wb') as f:\n            pickle.dump((walk_distances, drive_distances), f)\n        print(f\"Saved distance calculations to {cache_file}\")\n    except Exception as e:\n        print(f\"Failed to save distance cache: {e}\")\n    \n    return walk_distances, drive_distances\n\n\ndef calculate_distances_with_networkx(origins, destinations, walk_net, drive_net):\n    \"\"\"\n    Calculate distances between origins and destinations using NetworkX.\n    \n    Parameters\n    ----------\n    origins : dict\n        Dictionary mapping IDs to Point geometries for origins\n    destinations : list\n        List of Point geometries for destinations\n    walk_net : networkx.Graph\n        Walking network \n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    tuple\n        (walk_distances, drive_distances) dictionaries mapping (origin_id, destination_idx)\n        to distances in meters\n    \"\"\"\n    # Prepare outputs\n    walk_distances = {}\n    drive_distances = {}\n    \n    # Track failures for reporting\n    walk_failures = 0\n    drive_failures = 0\n    total_pairs = len(origins) * len(destinations)\n    \n    # Define distance filter threshold in meters\n    # This is based on the maximum distance band used in the methodology (15,840 ft ≈ 4.8 km)\n    # Adding a small buffer to ensure it don't miss any relevant connections\n    MAX_DISTANCE_FILTER = 6000  # 6 km\n    \n    # Track stats for filtered pairs\n    skipped_pairs = 0\n    processed_pairs = 0\n    \n    print(f\"Using distance filter of {MAX_DISTANCE_FILTER/1000:.1f} km\")\n    \n    # Process each origin-destination pair\n    for origin_id, origin_geom in tqdm(origins.items(), desc=\"Calculating distances\"):\n        origin_x, origin_y = origin_geom.x, origin_geom.y\n        \n        # Find nearest nodes once per origin to speed things up\n        try:\n            origin_node_walk = ox.distance.nearest_nodes(walk_net, origin_x, origin_y)\n            origin_node_drive = ox.distance.nearest_nodes(drive_net, origin_x, origin_y)\n        except Exception as e:\n            if DEBUG_MODE:\n                print(f\"Failed to find nearest nodes for origin {origin_id}: {str(e)[:100]}...\")\n            # Skip this origin if can't find nodes\n            continue\n        \n        # Pre-filter destinations based on Euclidean distance\n        filtered_destinations = []\n        for dest_idx, dest_geom in enumerate(destinations):\n            # Calculate straight-line distance\n            dest_x, dest_y = dest_geom.x, dest_geom.y\n            straight_dist = ((origin_x - dest_x)**2 + (origin_y - dest_y)**2)**0.5\n            \n            # Only process destinations within the threshold distance\n            if straight_dist &lt;= MAX_DISTANCE_FILTER:\n                filtered_destinations.append((dest_idx, dest_geom))\n            else:\n                # For points beyond our filter, set to infinity (they'll get the maximum score anyway)\n                walk_distances[(origin_id, dest_idx)] = float('inf')\n                drive_distances[(origin_id, dest_idx)] = float('inf')\n                skipped_pairs += 1\n        \n        # Process each filtered destination for this origin\n        for dest_idx, dest_geom in filtered_destinations:\n            processed_pairs += 1\n            dest_x, dest_y = dest_geom.x, dest_geom.y\n            \n            # Try walking distance calculation\n            try:\n                dest_node = ox.distance.nearest_nodes(walk_net, dest_x, dest_y)\n                path_length = nx.shortest_path_length(\n                    walk_net, origin_node_walk, dest_node, weight='length'\n                )\n                walk_distances[(origin_id, dest_idx)] = path_length\n            except Exception as e:\n                # Fall back to straight-line distance with 1.3 detour factor for walking\n                # This is a reasonable approximation for urban areas\n                if DEBUG_MODE:\n                    print(f\"Walking path failed for ({origin_id}, {dest_idx}): {str(e)[:100]}...\")\n                \n                straight_dist = geopy.distance.geodesic(\n                    (origin_y, origin_x), (dest_y, dest_x)\n                ).meters\n                \n                # Apply detour factor - walking routes tend to be ~1.3x straight line distance\n                walk_distances[(origin_id, dest_idx)] = straight_dist * 1.3\n                walk_failures += 1\n            \n            # Try driving distance calculation\n            try:\n                dest_node = ox.distance.nearest_nodes(drive_net, dest_x, dest_y)\n                path_length = nx.shortest_path_length(\n                    drive_net, origin_node_drive, dest_node, weight='length'\n                )\n                drive_distances[(origin_id, dest_idx)] = path_length\n            except Exception as e:\n                # Fall back to straight-line distance with 1.5 detour factor for driving\n                # This is a reasonable approximation for urban areas\n                if DEBUG_MODE:\n                    print(f\"Driving path failed for ({origin_id}, {dest_idx}): {str(e)[:100]}...\")\n                \n                straight_dist = geopy.distance.geodesic(\n                    (origin_y, origin_x), (dest_y, dest_x)\n                ).meters\n                \n                # Apply detour factor - driving routes tend to be ~1.5x straight line distance\n                drive_distances[(origin_id, dest_idx)] = straight_dist * 1.5\n                drive_failures += 1\n    \n    # Report on statistics\n    filter_percent = skipped_pairs / total_pairs * 100 if total_pairs &gt; 0 else 0\n    print(f\"Distance filter: Processed {processed_pairs} pairs, skipped {skipped_pairs} pairs ({filter_percent:.1f}% reduction)\")\n    \n    # Report on failures\n    if walk_failures &gt; 0:\n        print(f\"Warning: {walk_failures}/{processed_pairs} walking distance calculations failed ({walk_failures/processed_pairs*100:.1f}%)\")\n        print(\"Used straight-line distance with detour factor as fallback.\")\n    if drive_failures &gt; 0:\n        print(f\"Warning: {drive_failures}/{processed_pairs} driving distance calculations failed ({drive_failures/processed_pairs*100:.1f}%)\")\n        print(\"Used straight-line distance with detour factor as fallback.\")\n    \n    return walk_distances, drive_distances\n\n\ndef calculate_coverage_scores(census, walk_dist, drive_dist, stations):\n    \"\"\"Calculate coverage scores with station capacity and power factored in.\"\"\"\n    # Create station quality index (combine points count and power)\n    stations['quality_index'] = stations['num_points'] * np.sqrt(stations['max_power'] / 50)\n    # Normalize quality to 0.5-1.5 range (0.5=worst, 1.0=average, 1.5=best)\n    min_q = stations['quality_index'].min()\n    max_q = stations['quality_index'].max()\n    stations['quality_factor'] = 0.5 + ((stations['quality_index'] - min_q) / (max_q - min_q)) if max_q &gt; min_q else 1.0\n    \n    # Build station lookup by geometry for quick access\n    station_lookup = {geom.wkt: factor for geom, factor in zip(stations.geometry, stations['quality_factor'])}\n    \n    results = []\n    for idx, row in census.iterrows():\n        w = walk_dist.get(idx, np.inf)\n        d = drive_dist.get(idx, np.inf)\n        \n        # Find nearest station and its quality factor\n        nearest_dist = min(w, d)\n        nearest_station_geom = min(\n            [pt for pt in stations.geometry],\n            key=lambda pt: ((pt.x - row.geometry.centroid.x)**2 + (pt.y - row.geometry.centroid.y)**2)**0.5\n        )\n        quality_factor = station_lookup.get(nearest_station_geom.wkt, 1.0)\n        \n        # Calculate base coverage as before\n        ws = 0.7 if w&lt;=1320 else 0.5 if w&lt;=2640 else 0.3 if w&lt;=3960 else 1.0\n        ds = 0.7 if d&lt;=5280 else 0.5 if d&lt;=10560 else 0.3 if d&lt;=15840 else 1.0\n        combined = 0.4*ws + 0.6*ds\n        \n        # Adjust coverage by station quality\n        adjusted_combined = combined * quality_factor\n        \n        # Apply density adjustment\n        dens = row['pop_density']/census['pop_density'].max()\n        score = adjusted_combined * (1 - 0.3*dens)\n        \n        results.append((score, w, d))\n        \n    df = pd.DataFrame(results, index=census.index, columns=['score_raw','walk_ft','drive_ft'])\n    minv, maxv = df['score_raw'].min(), df['score_raw'].max()\n    df['coverage_score'] = (df['score_raw'] - minv)/(maxv-minv)\n    return df\n\n\ndef build_service_areas(stations, walk_net, drive_net):\n    \"\"\"\n    Build service area polygons for the stations using NetworkX.\n    \n    Parameters\n    ----------\n    stations : GeoDataFrame\n        GeoDataFrame of EV charging stations\n    walk_net : networkx.Graph\n        Walking network\n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    dict\n        Dictionary of service area polygons by type and distance\n    \"\"\"\n    # Check if we should use cached service areas\n    cache_file = os.path.join(CACHE_DIR, 'service_areas_cache.pkl')\n    \n    # Try to load from cache\n    if os.path.exists(cache_file):\n        try:\n            print(\"Loading service areas from cache...\")\n            with open(cache_file, 'rb') as f:\n                service_areas = pickle.load(f)\n            print(f\"Loaded {len(service_areas)} service areas from cache\")\n            return service_areas\n        except Exception as e:\n            print(f\"Error loading service areas cache: {e}\")\n            # Continue with calculation\n    \n    # Calculate service areas if not cached\n    print(\"Building service areas using NetworkX...\")\n    service_areas = build_service_areas_with_networkx(stations, walk_net, drive_net)\n    \n    # Cache the results\n    try:\n        with open(cache_file, 'wb') as f:\n            pickle.dump(service_areas, f)\n        print(f\"Saved service areas to {cache_file}\")\n    except Exception as e:\n        print(f\"Failed to save service areas cache: {e}\")\n    \n    return service_areas\n\n\ndef build_service_areas_with_networkx(stations, walk_net, drive_net):\n    \"\"\"\n    Legacy method to build service areas using NetworkX.\n    \n    Parameters\n    ----------\n    stations : GeoDataFrame\n        GeoDataFrame of EV charging stations\n    walk_net : networkx.Graph\n        Walking network\n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    dict\n        Dictionary of service area polygons by type and distance\n    \"\"\"\n    # Initialize result dictionary\n    service_areas = {}\n    \n    # Define service area distances\n    walk_distances = [400, 800]  # meters (approx. 5 and 10 min walk)\n    drive_distances = [1000, 3000, 8000]  # meters\n    \n    # Track successes and failures\n    success_count = 0\n    error_count = 0\n    \n    # Create walking service areas\n    for dist in walk_distances:\n        print(f\"Creating walking service area for {dist}m...\")\n        area_key = f'walk_{dist}m'\n        reachable_nodes = []\n        \n        # For each station, find reachable nodes\n        for idx, station in stations.iterrows():\n            try:\n                # Find nearest node to station\n                start_node = ox.distance.nearest_nodes(walk_net, station.geometry.x, station.geometry.y)\n                \n                # Get reachable nodes within distance\n                reachable = nx.single_source_dijkstra_path_length(\n                    walk_net, start_node, cutoff=dist, weight='length'\n                )\n                \n                # Get coordinates of reachable nodes\n                for node_id in reachable:\n                    x, y = walk_net.nodes[node_id]['x'], walk_net.nodes[node_id]['y']\n                    reachable_nodes.append(Point(x, y))\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error calculating walking service area for station {idx}: {str(e)[:100]}...\")\n                continue\n        \n        # Create service area from reachable nodes\n        if len(reachable_nodes) &gt;= 3:\n            try:\n                # Create convex hull from reachable nodes\n                multi_point = MultiPoint(reachable_nodes)\n                service_areas[area_key] = multi_point.convex_hull\n                success_count += 1\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error creating convex hull for {area_key}: {str(e)[:100]}...\")\n                # Fallback to buffer\n                service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n                error_count += 1\n        else:\n            # If too few reachable nodes, use buffer\n            print(f\"Too few reachable nodes ({len(reachable_nodes)}) for {area_key}. Using buffer.\")\n            service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n            error_count += 1\n    \n    # Create driving service areas\n    for dist in drive_distances:\n        print(f\"Creating driving service area for {dist}m...\")\n        area_key = f'drive_{dist}m'\n        reachable_nodes = []\n        \n        # For each station, find reachable nodes\n        for idx, station in stations.iterrows():\n            try:\n                # Find nearest node to station\n                start_node = ox.distance.nearest_nodes(drive_net, station.geometry.x, station.geometry.y)\n                \n                # Get reachable nodes within distance\n                reachable = nx.single_source_dijkstra_path_length(\n                    drive_net, start_node, cutoff=dist, weight='length'\n                )\n                \n                # Get coordinates of reachable nodes\n                for node_id in reachable:\n                    x, y = drive_net.nodes[node_id]['x'], drive_net.nodes[node_id]['y']\n                    reachable_nodes.append(Point(x, y))\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error calculating driving service area for station {idx}: {str(e)[:100]}...\")\n                continue\n        \n        # Create service area from reachable nodes\n        if len(reachable_nodes) &gt;= 3:\n            try:\n                # Create convex hull from reachable nodes\n                multi_point = MultiPoint(reachable_nodes)\n                service_areas[area_key] = multi_point.convex_hull\n                success_count += 1\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error creating convex hull for {area_key}: {str(e)[:100]}...\")\n                # Fallback to buffer\n                service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n                error_count += 1\n        else:\n            # If too few reachable nodes, use buffer\n            print(f\"Too few reachable nodes ({len(reachable_nodes)}) for {area_key}. Using buffer.\")\n            service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n            error_count += 1\n    \n    print(f\"Service area generation: {success_count} successes, {error_count} fallbacks to buffers\")\n    return service_areas\n\n\ndef perform_equity_analysis(census, areas):\n    \"\"\"Compute effect size and confidence intervals for equity metrics.\"\"\"\n    # Check if we should use cached equity results\n    cache_file = os.path.join(CACHE_DIR, 'equity_analysis_cache.pkl')\n    \n    # Try to load from cache\n    if os.path.exists(cache_file):\n        try:\n            print(\"Loading equity analysis from cache...\")\n            with open(cache_file, 'rb') as f:\n                equity = pickle.load(f)\n            print(f\"Loaded equity analysis for {len(equity)} service areas from cache\")\n            return equity\n        except Exception as e:\n            print(f\"Error loading equity analysis cache: {e}\")\n            # Continue with calculation\n    \n    # Perform equity analysis if not cached\n    print(\"Computing equity analysis...\")\n    equity = {}\n    for label, geom in areas.items():\n        covered = census[census.geometry.intersects(geom)]\n        not_covered = census[~census.index.isin(covered.index)]\n        metrics = {}\n        for var in ['median_income','poverty_rate','pop_density','bach_degree_rate']:\n            a = covered[var].dropna()\n            b = not_covered[var].dropna()\n            if len(a)&gt;1 and len(b)&gt;1:\n                pooled = np.sqrt(((len(a)-1)*a.var()+(len(b)-1)*b.var())/(len(a)+len(b)-2))\n                d = (a.mean()-b.mean())/pooled\n                se = pooled*np.sqrt(1/len(a)+1/len(b))\n                ci = (d-1.96*se, d+1.96*se)\n                pval = stats.ttest_ind(a,b).pvalue\n                metrics[var] = {'effect_size':d,'ci_lower':ci[0],'ci_upper':ci[1],'p_value':pval}\n        equity[label] = metrics\n    \n    # Cache the results\n    try:\n        with open(cache_file, 'wb') as f:\n            pickle.dump(equity, f)\n        print(f\"Saved equity analysis to {cache_file}\")\n    except Exception as e:\n        print(f\"Failed to save equity analysis cache: {e}\")\n    \n    return equity\n\n\n# ----------------------------------------------------------\n# Compute dynamic weights based on equity disparities\ndef calculate_dynamic_weights(equity_results):\n    # Base weights for each component - starting point\n    weights = {'coverage':0.30, 'income':0.20, 'poverty':0.20, 'density':0.15, 'education':0.15}\n    \n    # Track effect sizes and significance by variable across all buffer types\n    effect_sizes = {'income': 0, 'poverty': 0, 'density': 0, 'education': 0}\n    significant_effects = {'income': False, 'poverty': False, 'density': False, 'education': False}\n    \n    # Map variable names to weight keys\n    var_to_weight = {\n        'median_income': 'income',\n        'poverty_rate': 'poverty',\n        'pop_density': 'density',\n        'bach_degree_rate': 'education'\n    }\n    \n    # First pass: collect maximum effect sizes and check significance\n    for buf, metrics in equity_results.items():\n        for var, stats in metrics.items():\n            if var in var_to_weight:\n                weight_key = var_to_weight[var]\n                # Track the highest absolute effect size found\n                effect_sizes[weight_key] = max(effect_sizes[weight_key], abs(stats['effect_size']))\n                # Mark as significant if p-value &lt; 0.05\n                if stats['p_value'] &lt; 0.05:\n                    significant_effects[weight_key] = True\n    \n    # Adjust weights based on effect sizes and significance\n    for weight_key, effect_size in effect_sizes.items():\n        is_significant = significant_effects[weight_key]\n        \n        if is_significant:\n            # For significant effects, apply progressive scaling:\n            # Small effect (0.2-0.5): +5%\n            # Medium effect (0.5-0.8): +10%\n            # Large effect (&gt;0.8): +15%\n            if effect_size &gt; 0.8:\n                weights[weight_key] += 0.15\n            elif effect_size &gt; 0.5:\n                weights[weight_key] += 0.10\n            elif effect_size &gt; 0.2:\n                weights[weight_key] += 0.05\n    \n    # Normalize weights to sum to 1\n    total = sum(weights.values())\n    normalized_weights = {k: v/total for k, v in weights.items()}\n    \n    if DEBUG_MODE:\n        print(\"Original weights:\", weights)\n        print(\"Effect sizes:\", effect_sizes)\n        print(\"Significant effects:\", significant_effects)\n        print(\"Normalized weights:\", normalized_weights)\n    \n    return normalized_weights\n\n\n# ----------------------------------------------------------\n# Apply final gap scoring by combining coverage, income, poverty, density, and education\ndef apply_gap_scoring(census, dyn_wts):\n    \"\"\"\n    Apply a simpler gap scoring methodology using:\n    1. Linear combinations of normalized components\n    2. Quantile-based thresholds for natural data distribution\n    \"\"\"\n    # Step 1: Create simple normalized components (0-1 scale)\n    \n    # Coverage component (higher coverage =&gt; lower gap)\n    cov_norm = 1 - census['coverage_score']  # Invert so higher = worse coverage\n    cov_comp = cov_norm * dyn_wts['coverage']\n    \n    # Income component (lower income =&gt; higher gap)\n    inc_norm = (census['median_income'].max() - census['median_income']) / (census['median_income'].max() - census['median_income'].min())\n    inc_comp = inc_norm * dyn_wts['income']\n    \n    # Poverty component (higher poverty =&gt; higher gap)\n    pov_norm = (census['poverty_rate'] - census['poverty_rate'].min()) / (census['poverty_rate'].max() - census['poverty_rate'].min())\n    pov_comp = pov_norm * dyn_wts['poverty']\n    \n    # Density component (higher pop density =&gt; higher gap)\n    den_norm = (census['pop_density'] - census['pop_density'].min()) / (census['pop_density'].max() - census['pop_density'].min())\n    den_comp = den_norm * dyn_wts['density']\n    \n    # Education component (lower education =&gt; higher gap)\n    edu_norm = (census['bach_degree_rate'].max() - census['bach_degree_rate']) / (census['bach_degree_rate'].max() - census['bach_degree_rate'].min())\n    edu_comp = edu_norm * dyn_wts['education']\n    \n    # Step 2: Linear combination of components (simple weighted sum)\n    census['gap_score'] = cov_comp + inc_comp + pov_comp + den_comp + edu_comp\n    \n    # Step 3: Use fixed thresholds based on meaningful categories rather than quartiles\n    # Low, Medium, High, Critical priority with more tracts in lower categories\n    # These fixed thresholds create a right-skewed distribution\n    # Using global THRESHOLDS instead of defining locally\n    \n    if DEBUG_MODE:\n        print(f\"Fixed thresholds: {THRESHOLDS}\")\n    \n    labels = ['Low Priority','Medium Priority','High Priority','Critical Priority']\n    census['gap_category'] = pd.cut(census['gap_score'], \n                                   bins=[-np.inf] + THRESHOLDS + [np.inf], \n                                   labels=labels, \n                                   include_lowest=True)\n\n    # Print component statistics for debugging\n    if DEBUG_MODE:\n        print(\"\\n=== GAP SCORE COMPONENTS ===\")\n        print(f\"Coverage component (weight: {dyn_wts['coverage']:.2f}):\")\n        print(cov_comp.describe())\n        print(f\"\\nIncome component (weight: {dyn_wts['income']:.2f}):\")\n        print(inc_comp.describe())\n        print(f\"\\nPoverty component (weight: {dyn_wts['poverty']:.2f}):\")\n        print(pov_comp.describe())\n        print(f\"\\nDensity component (weight: {dyn_wts['density']:.2f}):\")\n        print(den_comp.describe())\n        print(f\"\\nEducation component (weight: {dyn_wts['education']:.2f}):\")\n        print(edu_comp.describe())\n\n    # Print final gap score statistics\n    if DEBUG_MODE:\n        print(\"\\n=== GAP SCORE STATISTICS ===\")\n        print(census['gap_score'].describe())\n        \n        # Print distribution of priority categories\n        print(\"\\n=== PRIORITY CATEGORY DISTRIBUTION ===\")\n        cat_counts = census['gap_category'].value_counts().sort_index()\n        for category, count in cat_counts.items():\n            percentage = (count / len(census)) * 100\n            print(f\"{category}: {count} tracts ({percentage:.1f}%)\")\n\n    return census\n\n\n# ----------------------------------------------------------\n# Compute coverage statistics by demographic category\ndef compute_coverage_by_demographics(census):\n    demo_cov = {}\n    # Loop through each demographic dimension\n    for col in ['income_category','education_category','density_category']:\n        demo_cov[col] = {}\n        for cat in census[col].unique():\n            sub = census[census[col] == cat]\n            total_pop = int(sub['total_pop'].sum())\n            covered_pop = int(sub[sub['is_served']]['total_pop'].sum())\n            pct = covered_pop / total_pop * 100 if total_pop&gt;0 else 0\n            demo_cov[col][cat] = {'total_population': total_pop, 'covered_population': covered_pop, 'coverage_percent': pct}\n    return demo_cov\n\n\n# ----------------------------------------------------------\n# Save all pipeline outputs: GeoPackage, JSON/CSV stats, and README\ndef save_pipeline_outputs(census, stations, areas, equity_res, demo_cov, dyn_wts):\n    os.makedirs('data', exist_ok=True)\n    # Save census tracts and stations\n    census.to_file('data/gap_analysis_fixed.gpkg', layer='census', driver='GPKG')\n    stations.to_file('data/gap_analysis_fixed.gpkg', layer='stations', driver='GPKG')\n    # Save each service area as its own layer\n    for label, geom in areas.items():\n        gdf = gpd.GeoDataFrame(geometry=[geom], crs=census.crs)\n        gdf.to_file('data/gap_analysis_fixed.gpkg', layer=f'service_area_{label}', driver='GPKG')\n    # Equity analysis JSON & CSV\n    with open('data/equity_analysis.json','w') as f: json.dump(equity_res, f, indent=2)\n    eq_list = []\n    for buf, mets in equity_res.items():\n        for var, st in mets.items(): eq_list.append({'buffer':buf,'variable':var,**st})\n    pd.DataFrame(eq_list).to_csv('data/equity_analysis.csv', index=False)\n    # Demographic coverage JSON & CSV\n    with open('data/coverage_by_demographics.json','w') as f: json.dump(demo_cov, f, indent=2)\n    cv_list = []\n    for col, cats in demo_cov.items():\n        for cat, st in cats.items(): cv_list.append({'demographic_group':col,'category':cat,**st})\n    pd.DataFrame(cv_list).to_csv('data/coverage_statistics.csv', index=False)\n    # Summary of dynamic weights and overall coverage\n    summary = {'dynamic_weights':dyn_wts,'total_tracts':len(census),'total_stations':len(stations),'coverage_percent':float(census['is_served'].mean()*100)}\n    with open('data/analysis_summary.json','w') as f: json.dump(summary, f, indent=2)\n    \n    # Calculate statistics by gap category\n    gap_stats = {}\n    priority_levels = census['gap_category'].unique()\n    for level in priority_levels:\n        tracts = census[census['gap_category'] == level]\n        gap_stats[level] = {\n            'tract_count': int(len(tracts)),\n            'population': int(tracts['total_pop'].sum()),\n            'area_sq_mi': float(tracts.geometry.area.sum() / (5280 * 5280)),  # Convert sq ft to sq mi\n            'pop_density': float(tracts['total_pop'].sum() / (tracts.geometry.area.sum() / (5280 * 5280))),\n            'median_income_avg': float(tracts['median_income'].mean()),\n            'poverty_rate_avg': float(tracts['poverty_rate'].mean())\n        }\n    \n    # Calculate summary stats for README\n    coverage_percent = float(census[census['is_served']]['total_pop'].sum() / census['total_pop'].sum() * 100)\n\n    # Create more detailed README with buffer distances\n    readme = f\"\"\"\n# EV Charging Gap Analysis Results\n\n**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## Overview\nThis analysis identifies areas in Philadelphia with the greatest need for additional EV charging infrastructure, \nbased on physical access to existing chargers and socioeconomic factors.\n\n## Methodology\nThe gap score combines:\n- **Coverage score** ({dyn_wts['coverage']*100:.1f}%): Physical access to existing chargers\n- **Socioeconomic scores** ({(1-dyn_wts['coverage'])*100:.1f}%):\n  - Income ({dyn_wts['income']*100:.1f}%)\n  - Poverty ({dyn_wts['poverty']*100:.1f}%)\n  - Population density ({dyn_wts['density']*100:.1f}%)\n  - Education ({dyn_wts['education']*100:.1f}%)\n\n## Service Area Buffer Distances\n| Mode | Distance | Time Equivalent |\n|------|----------|-----------------|\n| Walking | 1,320 ft | ~5 min walk |\n| Walking | 2,640 ft | ~10 min walk |\n| Walking | 3,960 ft | ~15 min walk |\n| Driving | 5,280 ft | ~1 mile |\n| Driving | 10,560 ft | ~2 miles |\n| Driving | 15,840 ft | ~3 miles |\n\n## Priority Categories\n- **Low Priority:** gap_score ≤ {THRESHOLDS[0]}\n- **Medium Priority:** {THRESHOLDS[0]} &lt; gap_score ≤ {THRESHOLDS[1]}\n- **High Priority:** {THRESHOLDS[1]} &lt; gap_score ≤ {THRESHOLDS[2]}\n- **Critical Priority:** gap_score &gt; {THRESHOLDS[2]}\n\n## Results Summary\n- Total census tracts: {len(census)}\n- Total EV stations: {len(stations)}\n- Population coverage: {coverage_percent:.1f}%\n- Critical priority areas: {gap_stats.get('Critical Priority', {}).get('tract_count', 0)} tracts\n\n## Output Files\n- **gap_analysis_fixed.gpkg**: GeoPackage with spatial layers\n- **analysis_results.json**: Complete nested analysis results\n- **summary_statistics.json**: Key population statistics by category\n- **equity_analysis.csv**: Detailed equity analysis results\n- **coverage_statistics.csv**: Coverage by demographic group\n- **visualizations/**: Charts, maps and summary tables\n\"\"\"\n    with open('data/README.md','w') as f: f.write(readme)\n\n    # Save complete analysis results (nested structure)\n    complete_results = {\n        'summary': {\n            'total_census_tracts': len(census),\n            'total_stations': len(stations),\n            'total_population': int(census['total_pop'].sum()),\n            'study_area_sq_mi': float(census.geometry.area.sum() / (5280 * 5280))\n        },\n        'coverage': {\n            'served_tracts': int(census['is_served'].sum()),\n            'served_population': int(census[census['is_served']]['total_pop'].sum()),\n            'service_coverage_pct': coverage_percent\n        },\n        'gap_scores': {\n            'mean': float(census['gap_score'].mean()),\n            'median': float(census['gap_score'].median()),\n            'min': float(census['gap_score'].min()),\n            'max': float(census['gap_score'].max()),\n            'std': float(census['gap_score'].std())\n        },\n        'priority_areas': gap_stats,\n        'weights': dyn_wts,\n        'demographic_coverage': demo_cov,\n        'validation': census['validation'] if 'validation' in census.columns else {}\n    }\n    \n    with open('data/analysis_results.json', 'w') as f:\n        json.dump(complete_results, f, indent=2, default=str)\n    \n    # Save summary statistics with population counts by category\n    summary_stats = {\n        'total_tracts': len(census),\n        'total_stations': len(stations),\n        'total_population': int(census['total_pop'].sum()),\n        'area_sq_mi': float(census.geometry.area.sum() / (5280 * 5280)),\n        'population_density': float(census['total_pop'].sum() / (census.geometry.area.sum() / (5280 * 5280))),\n        'coverage': {\n            'served_population': int(census[census['is_served']]['total_pop'].sum()),\n            'coverage_percent': coverage_percent\n        },\n        'gap_categories': {\n            level: {\n                'tract_count': gap_stats[level]['tract_count'],\n                'population': gap_stats[level]['population']\n            } for level in priority_levels\n        }\n    }\n    \n    with open('data/summary_statistics.json', 'w') as f:\n        json.dump(summary_stats, f, indent=2)\n\n\n# ----------------------------------------------------------\n# Generate visualizations: histogram, equity summary table, bar chart, and interactive map\ndef create_visualizations(census, equity_res, demo_cov):\n    \"\"\"Generate visualizations: histogram, equity summary table, bar chart, and interactive map.\"\"\"\n    # Create the directory if it doesn't exist\n    vis_dir = 'data/visualizations'\n    os.makedirs(vis_dir, exist_ok=True)\n    \n    # Make sure the directory is writable\n    if not os.access(vis_dir, os.W_OK):\n        print(f\"ERROR: Directory {vis_dir} is not writable!\")\n        return\n    \n    # 1) Histogram of gap scores with threshold lines and priority labels\n    try:\n        # Set up the figure\n        plt.figure(figsize=(14, 8))\n        \n        # Create the histogram with gap scores\n        n, bins, patches = plt.hist(census['gap_score'], \n                                   bins=30, \n                                   color='steelblue', \n                                   edgecolor='black', \n                                   alpha=0.8)\n        \n        # Add vertical lines for score thresholds\n        for i, threshold in enumerate(THRESHOLDS):\n            plt.axvline(x=threshold, color='red', linestyle='--', alpha=0.7, linewidth=2)\n            \n           \n            label_y = max(n) * 0.85  # Position at 85% of max height\n            \n            # Create a white background for the label\n            bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"red\", alpha=0.8)\n            \n            \n            plt.text(threshold + 0.01, label_y, f\"{threshold:.2f}\", \n                    color='red', fontweight='bold', ha='left', va='center',\n                    bbox=bbox_props)\n        \n        # Add priority area labels centered in each section\n        priorities = [\"Low Priority\", \"Medium Priority\", \"High Priority\", \"Critical Priority\"]\n        section_mids = [0.375, (THRESHOLDS[0]+THRESHOLDS[1])/2, \n                        (THRESHOLDS[1]+THRESHOLDS[2])/2, (THRESHOLDS[2] + 0.8)/2]\n        \n        for i, (priority, x_pos) in enumerate(zip(priorities, section_mids)):\n            plt.text(x_pos, max(n) * 0.95, priority, \n                    ha='center', va='center', fontsize=12, fontweight='bold')\n        \n        plt.title('Distribution of Gap Scores with Fixed Thresholds', fontsize=14)\n        plt.xlabel('Gap Score')\n        plt.ylabel('Number of Tracts')\n        plt.grid(True, alpha=0.3)\n        \n        # Save the histogram\n        hist_file = f'{vis_dir}/gap_score_distribution.png'\n        plt.savefig(hist_file, dpi=200)\n        plt.close()  # Make sure to close the plot to avoid displaying it\n        \n        if os.path.exists(hist_file) and os.path.getsize(hist_file) &gt; 0:\n            print(f\"Successfully created histogram: {hist_file}\")\n        else:\n            print(f\"ERROR: Failed to create histogram or file is empty: {hist_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating histogram: {str(e)}\")\n    \n    # 2) Demographic equity summary CSV\n    try:\n        if DEBUG_MODE:\n            print(\"Equity results structure:\", equity_res.keys())\n            print(\"First buffer sample:\", list(equity_res.values())[0] if equity_res else \"Empty\")\n\n        rows = []\n        for buffer_name, metrics in equity_res.items():\n            for variable, stats in metrics.items():\n                row = {'buffer': buffer_name, 'variable': variable}\n                for stat_name, stat_value in stats.items():\n                    row[stat_name] = stat_value\n                rows.append(row)\n        \n        if not rows:\n            print(\"WARNING: No equity data available for CSV export!\")\n            \n        eq_df = pd.DataFrame(rows)\n\n        if DEBUG_MODE:\n            print(f\"Equity DataFrame shape: {eq_df.shape}\")\n            if not eq_df.empty:\n                print(eq_df.head(2))\n\n        # Save to visualizations subfolder\n        csv_file = f'{vis_dir}/demographic_equity_summary.csv'\n        eq_df.to_csv(csv_file, index=False)\n        \n        # Check that file was created successfully\n        if os.path.exists(csv_file) and os.path.getsize(csv_file) &gt; 0:\n            print(f\"Successfully created equity CSV: {csv_file}\")\n        else:\n            print(f\"ERROR: Failed to create equity CSV or file is empty: {csv_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating equity CSV: {str(e)}\")\n    \n    # 3) Bar chart of coverage by demographic group\n    try:\n        # Debug the input data\n        if DEBUG_MODE:\n            print(\"\\nDemographic coverage data:\")\n            print(f\"Keys: {demo_cov.keys()}\")\n            for col, cats in demo_cov.items():\n                print(f\"  {col}: {len(cats)} categories\")\n                for cat, stats in cats.items():\n                    print(f\"    {cat}: {stats}\")\n        \n        # Prepare data for plotting\n        categories = ['density', 'education', 'income']\n        levels = ['low', 'medium', 'high']\n        \n        # Create dataframe with simplified structure\n        plot_data = []\n        \n        for category in categories:\n            cat_key = f\"{category}_category\"\n            if cat_key in demo_cov:\n                for level in levels:\n                    level_key = f\"{level}_{category}\"\n                    if level_key in demo_cov[cat_key]:\n                        coverage = demo_cov[cat_key][level_key]['coverage_percent']\n                        plot_data.append({\n                            'category': category.title(),\n                            'level': level.title(),\n                            'coverage': coverage\n                        })\n        \n        # Convert to DataFrame\n        plot_df = pd.DataFrame(plot_data)\n        \n        if plot_df.empty:\n            print(\"WARNING: No demographic coverage data available for bar chart!\")\n            return\n            \n        if DEBUG_MODE:\n            print(f\"\\nPlot DataFrame shape: {plot_df.shape}\")\n            print(plot_df.head())\n        \n        # Create improved bar chart\n        plt.figure(figsize=(12, 7))\n        \n        # Calculate positions for bars\n        categories_n = len(categories)\n        levels_n = len(levels)\n        width = 0.8 / levels_n  # Bar width\n        \n        # Colors for bars\n        colors = ['#FF8C61', '#FFB56B', '#FDD57E']\n        \n        # Plot bars for each level within each category\n        for i, level in enumerate(levels):\n            level_data = plot_df[plot_df['level'] == level.title()]\n            x_positions = np.arange(categories_n) + (i - levels_n/2 + 0.5) * width\n            plt.bar(x_positions, \n                   level_data['coverage'], \n                   width=width, \n                   color=colors[i % len(colors)],\n                   label=level.title())\n        \n        # Set chart title and labels\n        plt.title('Service Coverage by Census Tract Characteristic', fontsize=14)\n        plt.ylabel('Coverage Percentage')\n        plt.ylim(0, 55)  # Set y limit to give space for labels\n        \n        # Set x-axis ticks - use an empty string to remove the labels\n        plt.xticks(np.arange(categories_n), [''] * categories_n)\n        \n        # Add level labels beneath each category\n        for i, category in enumerate(categories):\n            for j, level in enumerate(levels):\n                x_pos = i + (j - levels_n/2 + 0.5) * width\n                plt.text(x_pos, -5, level.title(), \n                        ha='center', fontsize=10)\n        \n        # Add bracket annotations for categories\n        for i, category in enumerate(categories):\n            # Draw bracket from first to last bar in category\n            first_x = i - width * levels_n/2 + width/2\n            last_x = i + width * levels_n/2 - width/2\n            mid_x = i\n            \n            # Bracket height\n            y_bracket = -8\n            bracket_height = 2\n            \n            # Draw the horizontal lines\n            plt.plot([first_x, last_x], [y_bracket, y_bracket], 'k-', lw=1.5)\n            \n            # Add the category label\n            plt.text(mid_x, y_bracket - bracket_height - 2, category.title(), \n                    ha='center', fontweight='bold', fontsize=12)\n        \n        plt.grid(True, alpha=0.3, axis='y')\n        plt.ylim(bottom=-14)  # Extend bottom margin for labels and brackets\n        plt.tight_layout()\n        \n        # Save and verify file\n        bar_file = f'{vis_dir}/demographic_coverage.png'\n        plt.savefig(bar_file, dpi=200)\n        plt.close()\n        \n        # Check that file was created successfully\n        if os.path.exists(bar_file) and os.path.getsize(bar_file) &gt; 0:\n            print(f\"Successfully created bar chart: {bar_file}\")\n        else:\n            print(f\"ERROR: Failed to create bar chart or file is empty: {bar_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating bar chart: {str(e)}\")\n    \n    # 4) Interactive Folium map of priority areas and stations\n    try:\n        m = folium.Map(location=[39.9526, -75.1652], zoom_start=11, tiles='CartoDB positron')\n        # Priority colors\n        colors = {'Low Priority':'#fee5d9','Medium Priority':'#fcae91','High Priority':'#fb6a4a','Critical Priority':'#cb181d'}\n        \n        # Add tracts\n        for _, r in census.to_crs('EPSG:4326').iterrows():\n            fc = folium.GeoJson(\n                shapely_mapping(r['geometry']), \n                style_function=lambda f,cat=r['gap_category']: {\n                    'fillColor': colors.get(cat,'gray'),\n                    'color': 'black',\n                    'weight': 1,\n                    'fillOpacity': 0.7\n                },\n                # Tooltip with more tract info\n                tooltip=f\"Priority: {r['gap_category']}&lt;br&gt;Population: {int(r['total_pop']) if not pd.isna(r['total_pop']) else 'N/A'}&lt;br&gt;Income: ${int(r['median_income']):,} \" if not pd.isna(r['median_income']) else \"Priority: {r['gap_category']}&lt;br&gt;Population: {int(r['total_pop']) if not pd.isna(r['total_pop']) else 'N/A'}&lt;br&gt;Income: N/A\"\n            )\n            fc.add_to(m)\n            \n        # Add stations\n        for _, s in stations.to_crs('EPSG:4326').iterrows():\n            folium.CircleMarker(\n                location=[s.geometry.y, s.geometry.x],\n                radius=5,\n                color='black',\n                fill=True,\n                fill_color='white',\n                fill_opacity=1,\n                # Tooltip with station info\n                tooltip=f\"Station: {s['name']}&lt;br&gt;Points: {int(s['num_points']) if not pd.isna(s['num_points']) else 'N/A'}&lt;br&gt;Max Power: {s['max_power']:.1f} kW\" if not pd.isna(s['max_power']) else f\"Station: {s['name']}&lt;br&gt;Points: {int(s['num_points']) if not pd.isna(s['num_points']) else 'N/A'}&lt;br&gt;Max Power: N/A\"\n            ).add_to(m)\n\n        # Add a legend\n        legend_html = '''\n        &lt;div style=\"position: fixed; \n                   bottom: 50px; right: 50px; \n                   border:2px solid grey; z-index:9999; \n                   background-color:white;\n                   padding:10px;\n                   font-size:14px;\n                   \"&gt;\n        &lt;p&gt;&lt;b&gt;Priority Levels&lt;/b&gt;&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #fee5d9; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; Low Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #fcae91; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; Medium Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #fb6a4a; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; High Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #cb181d; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; Critical Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: white; border: 2px solid black; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; EV Station&lt;/p&gt;\n        &lt;/div&gt;\n        '''\n        m.get_root().html.add_child(folium.Element(legend_html))\n        \n        # Save and verify file\n        map_file = f'{vis_dir}/priority_areas_map.html'\n        m.save(map_file)\n        \n        # Check that file was created successfully\n        if os.path.exists(map_file) and os.path.getsize(map_file) &gt; 0:\n            print(f\"Successfully created interactive map: {map_file}\")\n        else:\n            print(f\"ERROR: Failed to create interactive map or file is empty: {map_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating interactive map: {str(e)}\")\n\n\n# Filter stations by charging speed and minimum points \ndef filter_stations(stations, charging_speed='All', min_points=1):\n    \"\"\"\n    Filter charging stations by charging speed category and minimum number of points.\n    \n    Parameters:\n    -----------\n    stations : GeoDataFrame\n        Station data with num_points and charging_speed columns\n    charging_speed : str\n        One of: 'All', 'Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)'\n    min_points : int\n        Minimum number of charging points (outlets) per station\n    \n    Returns:\n    --------\n    GeoDataFrame\n        Filtered station dataset\n    \"\"\"\n    filtered = stations.copy()\n    \n    if charging_speed != 'All':\n        filtered = filtered[filtered['charging_speed'] == charging_speed]\n    \n    filtered = filtered[filtered['num_points'] &gt;= min_points]\n    \n    print(f\"Filtered from {len(stations)} to {len(filtered)} stations \" +\n          f\"(speed: {charging_speed}, min points: {min_points})\")\n    \n    return filtered\n\n\ndef snap_points_to_network(points_gdf, network, cache_name=None):\n    \"\"\"\n    Snap points to the nearest edge on the network and return a new GeoDataFrame\n    with the same attributes but snapped geometries.\n    \n    This improves network analysis accuracy by ensuring that origins and destinations\n    are properly connected to the network.\n    \n    Parameters:\n    -----------\n    points_gdf : GeoDataFrame\n        Points to snap to the network\n    network : networkx.Graph\n        Road network \n    cache_name : str, optional\n        Name to use for caching the snapped points. If provided, will try to load\n        from cache first, and save to cache if not found.\n        \n    Returns:\n    --------\n    GeoDataFrame\n        New GeoDataFrame with original attributes but snapped geometries\n    \"\"\"\n    # Check if we should use cached snapped points\n    if cache_name:\n        cache_file = os.path.join(CACHE_DIR, f'snapped_{cache_name}.pkl')\n        \n        # Try to load from cache\n        if os.path.exists(cache_file):\n            try:\n                print(f\"Loading snapped points from cache: {cache_name}...\")\n                with open(cache_file, 'rb') as f:\n                    snapped_gdf = pickle.load(f)\n                print(f\"Loaded {len(snapped_gdf)} snapped points from cache.\")\n                return snapped_gdf\n            except Exception as e:\n                print(f\"Error loading snapped points cache: {e}\")\n                # Continue with snapping\n    \n    print(f\"Snapping {len(points_gdf)} points to network...\")\n    snapped_points = []\n    failed_snaps = 0\n    \n    for idx, row in tqdm(points_gdf.iterrows(), total=len(points_gdf), desc=\"Snapping points\"):\n        try:\n            # Find the nearest edge\n            nearest_edge = ox.distance.nearest_edges(\n                network, row.geometry.x, row.geometry.y, return_dist=False)\n            \n            # Get the edge geometry (might be a LineString or a geometry attribute)\n            u, v, _ = nearest_edge  # Unpack edge tuple\n            \n            if 'geometry' in network.edges[nearest_edge]:\n                # If edge has a geometry attribute, use it\n                edge_geom = network.edges[nearest_edge]['geometry']\n            else:\n                # Otherwise, create a line between nodes\n                u_x, u_y = network.nodes[u]['x'], network.nodes[u]['y']\n                v_x, v_y = network.nodes[v]['x'], network.nodes[v]['y']\n                edge_geom = LineString([(u_x, u_y), (v_x, v_y)])\n            \n            # Snap point to edge (project point onto line)\n            projected_point = edge_geom.interpolate(\n                edge_geom.project(row.geometry)\n            )\n            \n            # Create new row with snapped geometry\n            new_row = row.copy()\n            new_row.geometry = projected_point\n            snapped_points.append(new_row)\n            \n        except Exception as e:\n            # If snapping fails, keep the original point\n            if DEBUG_MODE:\n                print(f\"Failed to snap point {idx}: {str(e)[:100]}... Using original point.\")\n            failed_snaps += 1\n            snapped_points.append(row)\n    \n    if failed_snaps &gt; 0:\n        print(f\"Warning: {failed_snaps} points ({failed_snaps/len(points_gdf)*100:.1f}%) could not be snapped to the network.\")\n    \n    # Create a new GeoDataFrame with the same attributes\n    snapped_gdf = gpd.GeoDataFrame(snapped_points, crs=points_gdf.crs)\n    \n    # Cache the results if cache_name is provided\n    if cache_name:\n        try:\n            cache_file = os.path.join(CACHE_DIR, f'snapped_{cache_name}.pkl')\n            with open(cache_file, 'wb') as f:\n                pickle.dump(snapped_gdf, f)\n            print(f\"Saved snapped points to cache: {cache_name}\")\n        except Exception as e:\n            print(f\"Failed to save snapped points cache: {e}\")\n    \n    return snapped_gdf\n\n\n# Main entry point to run the complete pipeline\nif __name__ == '__main__':\n    # 1) Load data and networks\n    census, boundary, stations, validation = load_data()\n    \n    # Create NetworkX networks\n    G_walk, G_drive = create_networks()\n    \n    print(\"Using NetworkX for network analysis with point snapping\")\n    \n    # Optional: Filter stations (uncomment and adjust parameters as needed)\n    # Possible values for charging_speed: 'All', 'Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)'\n    # stations = filter_stations(stations, charging_speed='All', min_points=1)\n    \n    # Preprocess: Snap points to network for better connectivity\n    print(\"\\n=== PREPROCESSING POINTS FOR NETWORK ANALYSIS ===\")\n    # Snap stations to the road network\n    stations_snapped = snap_points_to_network(stations, G_drive, cache_name='stations')\n    print(f\"Using {len(stations_snapped)} snapped stations for network analysis\")\n    \n    # Create a GeoDataFrame of census tract centroids\n    census_centroids = gpd.GeoDataFrame(\n        geometry=[point for point in census.geometry.centroid],\n        index=census.index,\n        crs=census.crs\n    )\n    # Snap centroids to the road network\n    centroids_snapped = snap_points_to_network(census_centroids, G_drive, cache_name='centroids')\n    print(f\"Using {len(centroids_snapped)} snapped census tract centroids for network analysis\")\n    print(\"Preprocessing complete. Points are now properly connected to the network.\")\n    \n    # 2) Distances & initial coverage\n    # Use the snapped geometries for better network connectivity\n    origins = {i: row.geometry for i, row in centroids_snapped.iterrows()}\n    walk_d, drive_d = calculate_multimodal_distance_batch(\n        origins, \n        list(stations_snapped.geometry), \n        G_walk, \n        G_drive\n    )\n    cov = calculate_coverage_scores(census, walk_d, drive_d, stations)\n    census = census.join(cov)\n    \n    # Flag served tracts\n    census['is_served'] = census['coverage_score'] &gt; census['coverage_score'].mean()\n    \n    # 3) Service areas\n    # Use snapped stations for better service area generation\n    areas = build_service_areas(stations_snapped, G_walk, G_drive)\n    \n    # 4) Equity analysis\n    equity_res = perform_equity_analysis(census, areas)\n    \n    # 5) Compute dynamic weights and apply gap scoring\n    dyn_wts = calculate_dynamic_weights(equity_res)\n    census = apply_gap_scoring(census, dyn_wts)\n    \n    # 6) Categorize demographics by quantiles\n    census['income_category'] = pd.qcut(census['median_income'], 3, labels=['low_income','medium_income','high_income'])\n    census['education_category'] = pd.qcut(census['bach_degree_rate'], 3, labels=['low_education','medium_education','high_education'])\n    census['density_category'] = pd.qcut(census['pop_density'], 3, labels=['low_density','medium_density','high_density'])\n    \n    # 7) Coverage by demographic groups\n    demo_cov = compute_coverage_by_demographics(census)\n    \n    # 8) Save outputs and visualizations\n    save_pipeline_outputs(census, stations, areas, equity_res, demo_cov, dyn_wts)\n    create_visualizations(census, equity_res, demo_cov)\n\n    # Print equity analysis results if in debug mode\n    if DEBUG_MODE:\n        print(\"\\n=== EQUITY ANALYSIS SUMMARY ===\")\n        # Calculate average values for served vs unserved areas\n        served = census[census['is_served']]\n        unserved = census[~census['is_served']]\n        \n        # Print validation statistics first\n        print(\"\\nValidation Statistics:\")\n        print(f\"- Total stations from API: {validation['total_stations_from_api']}\")\n        print(f\"- Stations within boundary: {validation['stations_within_boundary']}\")\n        print(f\"- Excluded stations: {validation['excluded_stations']}\")\n        print(f\"  - Outside boundary: {validation['exclusion_reasons']['outside_boundary']}\")\n        print(f\"  - Missing coordinates: {validation['exclusion_reasons']['missing_coordinates']}\")\n        \n        # Print service area info if available\n        print(\"\\nService Areas:\")\n        if areas:\n            for area_name, area_geom in areas.items():\n                if not area_geom:\n                    print(f\"- {area_name}: Empty geometry\")\n                    continue\n                    \n                # Count intersecting tracts\n                intersecting = sum(1 for _, tract in census.iterrows() if tract.geometry.intersects(area_geom))\n                percent = (intersecting / len(census)) * 100\n                print(f\"- {area_name}: {intersecting} tracts ({percent:.1f}%)\")\n        else:\n            print(\"No service areas were created.\")\n        \n        # Print metric comparisons (original code)\n        print(\"\\nServed vs. Unserved Areas:\")\n        metrics = ['median_income', 'poverty_rate', 'pop_density', 'bach_degree_rate']\n        print(f\"{'Metric':&lt;20} {'Served':&gt;12} {'Unserved':&gt;12} {'Percent Diff':&gt;12} {'Significant':&gt;10}\")\n        print(\"-\" * 70)\n        \n        for var in metrics:\n            served_mean = served[var].mean()\n            unserved_mean = unserved[var].mean()\n            pct_diff = ((served_mean - unserved_mean) / unserved_mean) * 100 if unserved_mean != 0 else 0\n            t_stat, p_val = stats.ttest_ind(served[var].dropna(), unserved[var].dropna())\n            \n            print(f\"{var:&lt;20} {served_mean:&gt;12.1f} {unserved_mean:&gt;12.1f} {pct_diff:&gt;+12.1f}% {'Yes' if p_val &lt; 0.05 else 'No':&gt;10}\")\n\n    # Print dynamic weights if in debug mode\n    if DEBUG_MODE:\n        print(\"\\n=== DYNAMIC WEIGHTS ===\")\n        for k, v in dyn_wts.items():\n            print(f\"{k}: {v:.4f} ({v*100:.1f}%)\")\n\n    # Add validation to pipeline outputs\n    census['validation'] = validation\n    print('Complete gap analysis pipeline.') \n\nWorking directory: c:\\Users\\rache\\Documents\\musa 550 FINAL\\quarto_musa_550\\analysis\nOSMnx cache folder: data\\cache\\osmnx\nUsing NetworkX with point snapping for network analysis.\nUsing NetworkX for network analysis with point snapping\n\n=== PREPROCESSING POINTS FOR NETWORK ANALYSIS ===\nLoading snapped points from cache: stations...\nLoaded 130 snapped points from cache.\nUsing 130 snapped stations for network analysis\nLoading snapped points from cache: centroids...\nLoaded 385 snapped points from cache.\nUsing 385 snapped census tract centroids for network analysis\nPreprocessing complete. Points are now properly connected to the network.\nLoading distances from cache...\nLoaded 50050 walking and 50050 driving distances from cache.\nLoading service areas from cache...\nLoaded 5 service areas from cache\nLoading equity analysis from cache...\nLoaded equity analysis for 5 service areas from cache\nOriginal weights: {'coverage': 0.3, 'income': 0.2, 'poverty': 0.30000000000000004, 'density': 0.25, 'education': 0.2}\nEffect sizes: {'income': 0.12465946575488303, 'poverty': 0.6365812002143018, 'density': 0.7496382501362957, 'education': 0.30932633445014873}\nSignificant effects: {'income': False, 'poverty': True, 'density': True, 'education': True}\nNormalized weights: {'coverage': 0.24, 'income': 0.16, 'poverty': 0.24000000000000005, 'density': 0.2, 'education': 0.16}\nFixed thresholds: [0.45, 0.55, 0.65]\n\n=== GAP SCORE COMPONENTS ===\nCoverage component (weight: 0.24):\ncount    385.000000\nmean       0.188283\nstd        0.037668\nmin        0.000000\n25%        0.166852\n50%        0.203798\n75%        0.208540\nmax        0.240000\nName: coverage_score, dtype: float64\n\nIncome component (weight: 0.16):\ncount    380.000000\nmean       0.113021\nstd        0.030561\nmin        0.000000\n25%        0.094887\n50%        0.118142\n75%        0.136283\nmax        0.160000\nName: median_income, dtype: float64\n\nPoverty component (weight: 0.24):\ncount    385.000000\nmean       0.066670\nstd        0.044222\nmin        0.000000\n25%        0.032301\n50%        0.058785\n75%        0.094378\nmax        0.240000\nName: poverty_rate, dtype: float64\n\nDensity component (weight: 0.20):\ncount    385.000000\nmean       0.043050\nstd        0.026599\nmin        0.000000\n25%        0.023881\n50%        0.038673\n75%        0.057276\nmax        0.200000\nName: pop_density, dtype: float64\n\nEducation component (weight: 0.16):\ncount    385.000000\nmean       0.108519\nstd        0.037093\nmin        0.000000\n25%        0.085061\n50%        0.117489\n75%        0.137574\nmax        0.160000\nName: bach_degree_rate, dtype: float64\n\n=== GAP SCORE STATISTICS ===\ncount    380.000000\nmean       0.519702\nstd        0.108200\nmin        0.242863\n25%        0.431873\n50%        0.520160\n75%        0.604442\nmax        0.800245\nName: gap_score, dtype: float64\n\n=== PRIORITY CATEGORY DISTRIBUTION ===\nLow Priority: 117 tracts (30.4%)\nMedium Priority: 106 tracts (27.5%)\nHigh Priority: 111 tracts (28.8%)\nCritical Priority: 46 tracts (11.9%)\n\n\nC:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_51260\\409186455.py:1010: RuntimeWarning: invalid value encountered in scalar divide\n  'pop_density': float(tracts['total_pop'].sum() / (tracts.geometry.area.sum() / (5280 * 5280))),\n\n\nSuccessfully created histogram: data/visualizations/gap_score_distribution.png\nEquity results structure: dict_keys(['walk_400m', 'walk_800m', 'drive_1000m', 'drive_3000m', 'drive_8000m'])\nFirst buffer sample: {'median_income': {'effect_size': -0.12465946575488303, 'ci_lower': -6684.376767083484, 'ci_upper': 6684.127448151974, 'p_value': 0.24716139952343424}, 'poverty_rate': {'effect_size': 0.6365812002143018, 'ci_lower': -2.2308921974977856, 'ci_upper': 3.504054597926389, 'p_value': 6.057475773524907e-09}, 'pop_density': {'effect_size': 0.7496382501362957, 'ci_lower': -2404.4217684494174, 'ci_upper': 2405.92104494969, 'p_value': 1.105167556086776e-11}, 'bach_degree_rate': {'effect_size': 0.30932633445014873, 'ci_lower': -1.821517594859319, 'ci_upper': 2.4401702637596165, 'p_value': 0.00405668195382175}}\nEquity DataFrame shape: (20, 6)\n      buffer       variable  effect_size     ci_lower     ci_upper  \\\n0  walk_400m  median_income    -0.124659 -6684.376767  6684.127448   \n1  walk_400m   poverty_rate     0.636581    -2.230892     3.504055   \n\n        p_value  \n0  2.471614e-01  \n1  6.057476e-09  \nSuccessfully created equity CSV: data/visualizations/demographic_equity_summary.csv\n\nDemographic coverage data:\nKeys: dict_keys(['income_category', 'education_category', 'density_category'])\n  income_category: 4 categories\n    high_income: {'total_population': 455526, 'covered_population': 141652, 'coverage_percent': 31.096358934506483}\n    medium_income: {'total_population': 587637, 'covered_population': 288962, 'coverage_percent': 49.17355442220282}\n    low_income: {'total_population': 537747, 'covered_population': 90287, 'coverage_percent': 16.78986586629028}\n    nan: {'total_population': 0, 'covered_population': 0, 'coverage_percent': 0}\n  education_category: 3 categories\n    high_education: {'total_population': 443757, 'covered_population': 97725, 'coverage_percent': 22.02218781900905}\n    medium_education: {'total_population': 548675, 'covered_population': 234217, 'coverage_percent': 42.68774775595753}\n    low_education: {'total_population': 598128, 'covered_population': 193367, 'coverage_percent': 32.32869887381965}\n  density_category: 3 categories\n    low_density: {'total_population': 452288, 'covered_population': 191582, 'coverage_percent': 42.35840880147163}\n    medium_density: {'total_population': 566156, 'covered_population': 164581, 'coverage_percent': 29.06990299493426}\n    high_density: {'total_population': 572116, 'covered_population': 169146, 'coverage_percent': 29.564983325059952}\n\nPlot DataFrame shape: (9, 3)\n    category   level   coverage\n0    Density     Low  42.358409\n1    Density  Medium  29.069903\n2    Density    High  29.564983\n3  Education     Low  32.328699\n4  Education  Medium  42.687748\nSuccessfully created bar chart: data/visualizations/demographic_coverage.png\nSuccessfully created interactive map: data/visualizations/priority_areas_map.html\n\n=== EQUITY ANALYSIS SUMMARY ===\n\nValidation Statistics:\n- Total stations from API: 150\n- Stations within boundary: 130\n- Excluded stations: 20\n  - Outside boundary: 150\n  - Missing coordinates: 0\n\nService Areas:\n- walk_400m: 251 tracts (65.2%)\n- walk_800m: 251 tracts (65.2%)\n- drive_1000m: 251 tracts (65.2%)\n- drive_3000m: 251 tracts (65.2%)\n- drive_8000m: 251 tracts (65.2%)\n\nServed vs. Unserved Areas:\nMetric                     Served     Unserved Percent Diff Significant\n----------------------------------------------------------------------\nmedian_income             61172.9      64744.9         -5.5%         No\npoverty_rate                 20.4         22.9        -11.0%         No\npop_density               18940.5      21501.5        -11.9%         No\nbach_degree_rate             11.5         15.3        -25.0%        Yes\n\n=== DYNAMIC WEIGHTS ===\ncoverage: 0.2400 (24.0%)\nincome: 0.1600 (16.0%)\npoverty: 0.2400 (24.0%)\ndensity: 0.2000 (20.0%)\neducation: 0.1600 (16.0%)\nComplete gap analysis pipeline."
  },
  {
    "objectID": "analysis/4-Interactive-Dashboard.html",
    "href": "analysis/4-Interactive-Dashboard.html",
    "title": "Interactive EV Charging Dashboard",
    "section": "",
    "text": "This interactive dashboard allows exploration of Philadelphia’s EV charging infrastructure, service areas, and gap analysis results. Use the controls to filter stations, adjust service areas, and explore different layers of analysis."
  },
  {
    "objectID": "analysis/2-Service-Gap-Analysis.html",
    "href": "analysis/2-Service-Gap-Analysis.html",
    "title": "Network Service Gap Methods & Analysis",
    "section": "",
    "text": "Code\nimport os\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport osmnx as ox\nimport geopy.distance\nimport requests \nfrom scipy import stats\nfrom tqdm import tqdm\nfrom shapely.geometry import Point, LineString, MultiPoint\nfrom shapely.ops import unary_union\nfrom shapely.geometry import Polygon\nfrom dotenv import load_dotenv\nimport random\nfrom datetime import datetime\nimport folium\nfrom shapely.geometry import mapping as shapely_mapping\n\n# Print current working directory\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Set OSMnx cache location explicitly\nox.settings.cache_folder = os.path.join('data', 'cache', 'osmnx')\nprint(f\"OSMnx cache folder: {ox.settings.cache_folder}\")\n\n# For our cache\nCACHE_DIR = os.path.join('data', 'cache')\nos.makedirs(CACHE_DIR, exist_ok=True)\n\n# Set global debug mode (set to True for detailed component outputs)\nDEBUG_MODE = True\n\n# Define global thresholds for gap scoring - used in multiple functions\nTHRESHOLDS = [0.45, 0.55, 0.65]  # Adjusted thresholds for better distribution\n\n# Using NetworkX with point snapping\n\nprint(\"Using NetworkX with point snapping for network analysis.\")\n\ndef load_data():\n    \"\"\"Load census, boundary, and EV station data.\"\"\"\n    load_dotenv()\n    api_key = os.getenv('OPENCHARGE_API_KEY')\n    if not api_key:\n        raise EnvironmentError('OPENCHARGE_API_KEY is not set.')\n\n    # Census data\n    census = gpd.read_file('data/phila_census1.gpkg')\n    census = census[census['total_pop'] &gt; 0]\n    census = census[census['pop_density'] &gt;= 1000]\n    census = census.to_crs('EPSG:4326')\n\n    # City boundary\n    boundary = gpd.read_file('data/City_Limits.geojson').to_crs('EPSG:4326')\n\n    # EV stations\n    params = {\n        'key': api_key,\n        'countrycode': 'US',\n        'maxresults': 1000,\n        'latitude': 39.9526,\n        'longitude': -75.1652,\n        'distance': 10,\n        'distanceunit': 'km',\n        'compact': True,\n        'verbose': False,\n        'output': 'json'\n    }\n    resp = requests.get('https://api.openchargemap.io/v3/poi', params=params, timeout=10)\n    resp.raise_for_status()\n    data = resp.json()\n\n    records = []\n    for s in data:\n        lat = s.get('AddressInfo',{}).get('Latitude')\n        lon = s.get('AddressInfo',{}).get('Longitude')\n        if lat is None or lon is None:\n            continue\n        pt = Point(lon, lat)\n        if not pt.within(boundary.geometry.iloc[0]):\n            continue\n        conns = s.get('Connections', [])\n        max_kw = max((c.get('PowerKW',0) for c in conns), default=0)\n        records.append({\n            'id': s.get('ID'),\n            'name': s.get('AddressInfo',{}).get('Title',''),\n            'num_points': len(conns),\n            'max_power': max_kw,\n            'geometry': pt\n        })\n    stations = gpd.GeoDataFrame(records, crs='EPSG:4326')\n\n    # Project to state plane for spatial operations\n    census = census.to_crs('EPSG:2272')\n    boundary = boundary.to_crs('EPSG:2272')\n    stations = stations.to_crs('EPSG:2272')\n\n    # Add charging speed categories\n    stations['charging_speed'] = pd.cut(\n        stations['max_power'],\n        bins=[-np.inf, 7, 50, np.inf],\n        labels=['Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)']\n    )\n\n    # Validation tracking\n    validation = {\n        'total_stations_from_api': len(data),\n        'stations_within_boundary': len(records),\n        'excluded_stations': len(data) - len(records),\n        'exclusion_reasons': {\n            'outside_boundary': sum(1 for s in data if s.get('AddressInfo',{}).get('Latitude') and\n                                   s.get('AddressInfo',{}).get('Longitude') and\n                                   not Point(s.get('AddressInfo',{}).get('Longitude'), \n                                            s.get('AddressInfo',{}).get('Latitude')).within(boundary.geometry.iloc[0])),\n            'missing_coordinates': sum(1 for s in data if not s.get('AddressInfo',{}).get('Latitude') or\n                                      not s.get('AddressInfo',{}).get('Longitude'))\n        }\n    }\n\n    return census, boundary, stations, validation\n\n\ndef create_networks():\n    \"\"\"Load or build walk and drive networks and cache them.\"\"\"\n    cache = 'data/network_cache.pkl'\n    if os.path.exists(cache):\n        with open(cache,'rb') as f:\n            return pickle.load(f)\n\n    G_walk = ox.graph_from_place('Philadelphia, Pennsylvania', network_type='walk', simplify=False)\n    G_walk = ox.project_graph(G_walk, to_crs='EPSG:2272')\n    G_walk = ox.distance.add_edge_lengths(G_walk)\n\n    G_drive = ox.graph_from_place('Philadelphia, Pennsylvania', network_type='drive', simplify=False)\n    G_drive = ox.project_graph(G_drive, to_crs='EPSG:2272')\n    G_drive = ox.distance.add_edge_lengths(G_drive)\n\n    os.makedirs('data', exist_ok=True)\n    with open(cache,'wb') as f:\n        pickle.dump((G_walk, G_drive),f)\n\n    def check_network_connectivity(network, network_type):\n        \"\"\"Check and report on network connectivity issues.\"\"\"\n        # Find connected components\n        connected_components = list(nx.connected_components(network.to_undirected()))\n        print(f\"{network_type} network has {len(connected_components)} connected components\")\n        \n        # Report on largest component\n        largest_component = max(connected_components, key=len)\n        largest_pct = len(largest_component) / network.number_of_nodes() * 100\n        print(f\"Largest component contains {largest_pct:.1f}% of all nodes\")\n        \n        if len(connected_components) &gt; 1:\n            print(\"WARNING: Network has disconnected components, which may cause routing failures\")\n        \n        return len(connected_components)\n\n    # Add after creating networks:\n    print(\"Checking network connectivity...\")\n    walk_components = check_network_connectivity(G_walk, \"Walking\")\n    drive_components = check_network_connectivity(G_drive, \"Driving\")\n\n    return G_walk, G_drive\n\n\ndef find_nearest_node(network, point_x, point_y, fallback_tolerance=5000):\n    \"\"\"\n    Find the nearest node to a point using robust search methods.\n    Tries OSMnx nearest_nodes first, then falls back to manual distance calculation\n    with an increasing search radius if needed.\n    \n    Parameters:\n    -----------\n    network : networkx.Graph\n        Road network from OSMnx\n    point_x, point_y : float\n        Coordinates of the target point\n    fallback_tolerance : float\n        Maximum distance to search for nodes if nearest_nodes fails\n        \n    Returns:\n    --------\n    int or None\n        Node ID of nearest node, or None if no node found within tolerance\n    \"\"\"\n    # First try with OSMnx's built-in function\n    try:\n        return ox.nearest_nodes(network, point_x, point_y)\n    except Exception as e:\n        if DEBUG_MODE:\n            print(f\"Standard nearest_nodes failed: {e}, trying manual search...\")\n    \n    # Manual fallback with increasing radius\n    try:\n        # Get all nodes and their coordinates\n        nodes = list(network.nodes)\n        if not nodes:\n            if DEBUG_MODE:\n                print(\"No nodes in network!\")\n            return None\n            \n        # Use vectorized operations for efficiency\n        node_coords = np.array([\n            [network.nodes[n].get('x', 0), network.nodes[n].get('y', 0)] \n            for n in nodes\n        ])\n        \n        point_coords = np.array([point_x, point_y])\n        \n        # Calculate squared distances to all nodes\n        squared_distances = np.sum((node_coords - point_coords)**2, axis=1)\n        \n        # Get the node with minimum distance\n        min_idx = np.argmin(squared_distances)\n        min_distance = np.sqrt(squared_distances[min_idx])\n        \n        if min_distance &lt;= fallback_tolerance:\n            if DEBUG_MODE and min_distance &gt; 100:\n                print(f\"Found node at distance {min_distance:.1f} units\")\n            return nodes[min_idx]\n        else:\n            if DEBUG_MODE:\n                print(f\"Nearest node is too far: {min_distance:.1f} units &gt; {fallback_tolerance}\")\n            return None\n            \n    except Exception as e:\n        if DEBUG_MODE:\n            print(f\"Manual nearest node search failed: {e}\")\n        return None\n\n\ndef map_match_point(network, point_x, point_y, tolerance=5000):\n    \"\"\"\n    Map match a point to the nearest edge in the network and return\n    the nearest node along that edge.\n    \n    This provides better connectivity than just finding the nearest node,\n    especially when points are far from actual network nodes.\n    \n    Parameters:\n    -----------\n    network : networkx.Graph\n        Road network from OSMnx\n    point_x, point_y : float\n        Coordinates of the target point\n    tolerance : float\n        Maximum distance to search\n        \n    Returns:\n    --------\n    int or None\n        Node ID of matched node, or None if no match found\n    \"\"\"\n    try:\n        # Create a Point object for the input coordinates\n        point = Point(point_x, point_y)\n        \n        # Find nearest edges, not just nodes\n        best_edge = None\n        best_dist = float('inf')\n        \n        # Check a sample of edges for efficiency (every 10th edge)\n        edges_sample = list(network.edges(data=True))[::10]\n        \n        for u, v, data in edges_sample:\n            if 'geometry' in data:\n                # If edge has a geometry attribute, use it\n                edge_geom = data['geometry']\n                dist = edge_geom.distance(point)\n            else:\n                # Otherwise, create a line between nodes\n                try:\n                    u_x, u_y = network.nodes[u]['x'], network.nodes[u]['y']\n                    v_x, v_y = network.nodes[v]['x'], network.nodes[v]['y']\n                    line = [(u_x, u_y), (v_x, v_y)]\n                    edge_geom = LineString(line)\n                    dist = edge_geom.distance(point)\n                except KeyError:\n                    # Skip if nodes don't have coordinates\n                    continue\n            \n            # Update best match\n            if dist &lt; best_dist:\n                best_dist = dist\n                best_edge = (u, v)\n        \n        # Return the closer node from the best edge\n        if best_edge and best_dist &lt;= tolerance:\n            u, v = best_edge\n            u_dist = ((network.nodes[u]['x'] - point_x)**2 + \n                      (network.nodes[u]['y'] - point_y)**2)**0.5\n            v_dist = ((network.nodes[v]['x'] - point_x)**2 + \n                      (network.nodes[v]['y'] - point_y)**2)**0.5\n            return u if u_dist &lt; v_dist else v\n        \n        # Fallback to regular nearest node if no good edge found\n        return find_nearest_node(network, point_x, point_y, fallback_tolerance=tolerance)\n    \n    except Exception as e:\n        if DEBUG_MODE:\n            print(f\"Map matching failed: {e}\")\n        # Fallback to regular nearest node\n        return find_nearest_node(network, point_x, point_y, fallback_tolerance=tolerance)\n\n\ndef enhance_network_connectivity(G):\n    \"\"\"\n    Enhance network connectivity by ensuring key locations are connected.\n    \n    Parameters:\n    -----------\n    G : networkx.Graph\n        Road network to enhance\n    \n    Returns:\n    --------\n    networkx.Graph\n        Enhanced network\n    \"\"\"\n    # Add self-loops to all nodes to ensure they can be reached from themselves\n    # This helps with isolated nodes\n    for node in G.nodes():\n        if not G.has_edge(node, node):\n            G.add_edge(node, node, length=0)\n    \n    # Check the number of connected components\n    connected_components = list(nx.connected_components(G.to_undirected()))\n    largest_component = max(connected_components, key=len)\n    \n    # If we have multiple components, add edges to connect them to largest component\n    if len(connected_components) &gt; 1:\n        largest_component_nodes = list(largest_component)\n        for component in connected_components:\n            if component != largest_component:\n                # Find the closest node pair between components\n                min_dist = float('inf')\n                best_pair = None\n                \n                # Sample nodes from each component for efficiency\n                component_nodes = list(component)\n                sample_size = min(10, len(component_nodes))\n                sampled_component = random.sample(component_nodes, sample_size)\n                \n                sample_size_largest = min(20, len(largest_component_nodes))\n                sampled_largest = random.sample(largest_component_nodes, sample_size_largest)\n                \n                # Find the closest pair\n                for n1 in sampled_component:\n                    for n2 in sampled_largest:\n                        try:\n                            x1, y1 = G.nodes[n1]['x'], G.nodes[n1]['y']\n                            x2, y2 = G.nodes[n2]['x'], G.nodes[n2]['y']\n                            dist = ((x1-x2)**2 + (y1-y2)**2)**0.5\n                            if dist &lt; min_dist:\n                                min_dist = dist\n                                best_pair = (n1, n2)\n                        except KeyError:\n                            continue\n                \n                # Add an edge between the closest nodes\n                if best_pair:\n                    G.add_edge(best_pair[0], best_pair[1], length=min_dist)\n    \n    return G\n\n\ndef calculate_multimodal_distance_batch(origins, destinations, walk_net, drive_net):\n    \"\"\"\n    Calculate minimum walking and driving distances from origins to destinations using NetworkX.\n    \n    Parameters\n    ----------\n    origins : dict\n        Dictionary mapping IDs to Point geometries for origins\n    destinations : list\n        List of Point geometries for destinations\n    walk_net : networkx.Graph\n        Walking network \n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    tuple\n        (walk_distances, drive_distances) dictionaries mapping (origin_id, destination_idx)\n        to distances in meters\n    \"\"\"\n    # Check if we should use cached distances\n    cache_file = os.path.join(CACHE_DIR, 'distance_cache.pkl')\n    \n    # Create cache dir if needed\n    os.makedirs(CACHE_DIR, exist_ok=True)\n    \n    # Try to load from cache\n    if os.path.exists(cache_file):\n        try:\n            print(\"Loading distances from cache...\")\n            with open(cache_file, 'rb') as f:\n                distances = pickle.load(f)\n            walk_d, drive_d = distances\n            print(f\"Loaded {len(walk_d)} walking and {len(drive_d)} driving distances from cache.\")\n            return walk_d, drive_d\n        except Exception as e:\n            print(f\"Error loading distance cache: {e}\")\n            # Continue with calculation\n    \n    # Calculate distances with NetworkX\n    print(\"Calculating distances with NetworkX...\")\n    walk_distances, drive_distances = calculate_distances_with_networkx(\n        origins, destinations, walk_net, drive_net\n    )\n    \n    # Cache the results\n    try:\n        with open(cache_file, 'wb') as f:\n            pickle.dump((walk_distances, drive_distances), f)\n        print(f\"Saved distance calculations to {cache_file}\")\n    except Exception as e:\n        print(f\"Failed to save distance cache: {e}\")\n    \n    return walk_distances, drive_distances\n\n\ndef calculate_distances_with_networkx(origins, destinations, walk_net, drive_net):\n    \"\"\"\n    Calculate distances between origins and destinations using NetworkX.\n    \n    Parameters\n    ----------\n    origins : dict\n        Dictionary mapping IDs to Point geometries for origins\n    destinations : list\n        List of Point geometries for destinations\n    walk_net : networkx.Graph\n        Walking network \n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    tuple\n        (walk_distances, drive_distances) dictionaries mapping (origin_id, destination_idx)\n        to distances in meters\n    \"\"\"\n    # Prepare outputs\n    walk_distances = {}\n    drive_distances = {}\n    \n    # Track failures for reporting\n    walk_failures = 0\n    drive_failures = 0\n    total_pairs = len(origins) * len(destinations)\n    \n    # Define distance filter threshold in meters\n    # This is based on the maximum distance band used in the methodology (15,840 ft ≈ 4.8 km)\n    # Adding a small buffer to ensure we don't miss any relevant connections\n    MAX_DISTANCE_FILTER = 6000  # 6 km\n    \n    # Track stats for filtered pairs\n    skipped_pairs = 0\n    processed_pairs = 0\n    \n    print(f\"Using distance filter of {MAX_DISTANCE_FILTER/1000:.1f} km\")\n    \n    # Process each origin-destination pair\n    for origin_id, origin_geom in tqdm(origins.items(), desc=\"Calculating distances\"):\n        origin_x, origin_y = origin_geom.x, origin_geom.y\n        \n        # Find nearest nodes once per origin to speed things up\n        try:\n            origin_node_walk = ox.distance.nearest_nodes(walk_net, origin_x, origin_y)\n            origin_node_drive = ox.distance.nearest_nodes(drive_net, origin_x, origin_y)\n        except Exception as e:\n            if DEBUG_MODE:\n                print(f\"Failed to find nearest nodes for origin {origin_id}: {str(e)[:100]}...\")\n            # Skip this origin if we can't find nodes\n            continue\n        \n        # Pre-filter destinations based on Euclidean distance\n        filtered_destinations = []\n        for dest_idx, dest_geom in enumerate(destinations):\n            # Calculate straight-line distance\n            dest_x, dest_y = dest_geom.x, dest_geom.y\n            straight_dist = ((origin_x - dest_x)**2 + (origin_y - dest_y)**2)**0.5\n            \n            # Only process destinations within the threshold distance\n            if straight_dist &lt;= MAX_DISTANCE_FILTER:\n                filtered_destinations.append((dest_idx, dest_geom))\n            else:\n                # For points beyond our filter, set to infinity (they'll get the maximum score anyway)\n                walk_distances[(origin_id, dest_idx)] = float('inf')\n                drive_distances[(origin_id, dest_idx)] = float('inf')\n                skipped_pairs += 1\n        \n        # Process each filtered destination for this origin\n        for dest_idx, dest_geom in filtered_destinations:\n            processed_pairs += 1\n            dest_x, dest_y = dest_geom.x, dest_geom.y\n            \n            # Try walking distance calculation\n            try:\n                dest_node = ox.distance.nearest_nodes(walk_net, dest_x, dest_y)\n                path_length = nx.shortest_path_length(\n                    walk_net, origin_node_walk, dest_node, weight='length'\n                )\n                walk_distances[(origin_id, dest_idx)] = path_length\n            except Exception as e:\n                # Fall back to straight-line distance with 1.3 detour factor for walking\n                # This is a reasonable approximation for urban areas\n                if DEBUG_MODE:\n                    print(f\"Walking path failed for ({origin_id}, {dest_idx}): {str(e)[:100]}...\")\n                \n                straight_dist = geopy.distance.geodesic(\n                    (origin_y, origin_x), (dest_y, dest_x)\n                ).meters\n                \n                # Apply detour factor - walking routes tend to be ~1.3x straight line distance\n                walk_distances[(origin_id, dest_idx)] = straight_dist * 1.3\n                walk_failures += 1\n            \n            # Try driving distance calculation\n            try:\n                dest_node = ox.distance.nearest_nodes(drive_net, dest_x, dest_y)\n                path_length = nx.shortest_path_length(\n                    drive_net, origin_node_drive, dest_node, weight='length'\n                )\n                drive_distances[(origin_id, dest_idx)] = path_length\n            except Exception as e:\n                # Fall back to straight-line distance with 1.5 detour factor for driving\n                # This is a reasonable approximation for urban areas\n                if DEBUG_MODE:\n                    print(f\"Driving path failed for ({origin_id}, {dest_idx}): {str(e)[:100]}...\")\n                \n                straight_dist = geopy.distance.geodesic(\n                    (origin_y, origin_x), (dest_y, dest_x)\n                ).meters\n                \n                # Apply detour factor - driving routes tend to be ~1.5x straight line distance\n                drive_distances[(origin_id, dest_idx)] = straight_dist * 1.5\n                drive_failures += 1\n    \n    # Report on statistics\n    filter_percent = skipped_pairs / total_pairs * 100 if total_pairs &gt; 0 else 0\n    print(f\"Distance filter: Processed {processed_pairs} pairs, skipped {skipped_pairs} pairs ({filter_percent:.1f}% reduction)\")\n    \n    # Report on failures\n    if walk_failures &gt; 0:\n        print(f\"Warning: {walk_failures}/{processed_pairs} walking distance calculations failed ({walk_failures/processed_pairs*100:.1f}%)\")\n        print(\"Used straight-line distance with detour factor as fallback.\")\n    if drive_failures &gt; 0:\n        print(f\"Warning: {drive_failures}/{processed_pairs} driving distance calculations failed ({drive_failures/processed_pairs*100:.1f}%)\")\n        print(\"Used straight-line distance with detour factor as fallback.\")\n    \n    return walk_distances, drive_distances\n\n\ndef calculate_coverage_scores(census, walk_dist, drive_dist, stations):\n    \"\"\"Calculate coverage scores with station capacity and power factored in.\"\"\"\n    # Create station quality index (combine points count and power)\n    stations['quality_index'] = stations['num_points'] * np.sqrt(stations['max_power'] / 50)\n    # Normalize quality to 0.5-1.5 range (0.5=worst, 1.0=average, 1.5=best)\n    min_q = stations['quality_index'].min()\n    max_q = stations['quality_index'].max()\n    stations['quality_factor'] = 0.5 + ((stations['quality_index'] - min_q) / (max_q - min_q)) if max_q &gt; min_q else 1.0\n    \n    # Build station lookup by geometry for quick access\n    station_lookup = {geom.wkt: factor for geom, factor in zip(stations.geometry, stations['quality_factor'])}\n    \n    results = []\n    for idx, row in census.iterrows():\n        w = walk_dist.get(idx, np.inf)\n        d = drive_dist.get(idx, np.inf)\n        \n        # Find nearest station and its quality factor\n        nearest_dist = min(w, d)\n        nearest_station_geom = min(\n            [pt for pt in stations.geometry],\n            key=lambda pt: ((pt.x - row.geometry.centroid.x)**2 + (pt.y - row.geometry.centroid.y)**2)**0.5\n        )\n        quality_factor = station_lookup.get(nearest_station_geom.wkt, 1.0)\n        \n        # Calculate base coverage as before\n        ws = 0.7 if w&lt;=1320 else 0.5 if w&lt;=2640 else 0.3 if w&lt;=3960 else 1.0\n        ds = 0.7 if d&lt;=5280 else 0.5 if d&lt;=10560 else 0.3 if d&lt;=15840 else 1.0\n        combined = 0.4*ws + 0.6*ds\n        \n        # Adjust coverage by station quality\n        adjusted_combined = combined * quality_factor\n        \n        # Apply density adjustment\n        dens = row['pop_density']/census['pop_density'].max()\n        score = adjusted_combined * (1 - 0.3*dens)\n        \n        results.append((score, w, d))\n        \n    df = pd.DataFrame(results, index=census.index, columns=['score_raw','walk_ft','drive_ft'])\n    minv, maxv = df['score_raw'].min(), df['score_raw'].max()\n    df['coverage_score'] = (df['score_raw'] - minv)/(maxv-minv)\n    return df\n\n\ndef build_service_areas(stations, walk_net, drive_net):\n    \"\"\"\n    Build service area polygons for the stations using NetworkX.\n    \n    Parameters\n    ----------\n    stations : GeoDataFrame\n        GeoDataFrame of EV charging stations\n    walk_net : networkx.Graph\n        Walking network\n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    dict\n        Dictionary of service area polygons by type and distance\n    \"\"\"\n    # Check if we should use cached service areas\n    cache_file = os.path.join(CACHE_DIR, 'service_areas_cache.pkl')\n    \n    # Try to load from cache\n    if os.path.exists(cache_file):\n        try:\n            print(\"Loading service areas from cache...\")\n            with open(cache_file, 'rb') as f:\n                service_areas = pickle.load(f)\n            print(f\"Loaded {len(service_areas)} service areas from cache\")\n            return service_areas\n        except Exception as e:\n            print(f\"Error loading service areas cache: {e}\")\n            # Continue with calculation\n    \n    # Calculate service areas if not cached\n    print(\"Building service areas using NetworkX...\")\n    service_areas = build_service_areas_with_networkx(stations, walk_net, drive_net)\n    \n    # Cache the results\n    try:\n        with open(cache_file, 'wb') as f:\n            pickle.dump(service_areas, f)\n        print(f\"Saved service areas to {cache_file}\")\n    except Exception as e:\n        print(f\"Failed to save service areas cache: {e}\")\n    \n    return service_areas\n\n\ndef build_service_areas_with_networkx(stations, walk_net, drive_net):\n    \"\"\"\n    Legacy method to build service areas using NetworkX.\n    \n    Parameters\n    ----------\n    stations : GeoDataFrame\n        GeoDataFrame of EV charging stations\n    walk_net : networkx.Graph\n        Walking network\n    drive_net : networkx.Graph\n        Driving network\n        \n    Returns\n    -------\n    dict\n        Dictionary of service area polygons by type and distance\n    \"\"\"\n    # Initialize result dictionary\n    service_areas = {}\n    \n    # Define service area distances\n    walk_distances = [400, 800]  # meters (approx. 5 and 10 min walk)\n    drive_distances = [1000, 3000, 8000]  # meters\n    \n    # Track successes and failures\n    success_count = 0\n    error_count = 0\n    \n    # Create walking service areas\n    for dist in walk_distances:\n        print(f\"Creating walking service area for {dist}m...\")\n        area_key = f'walk_{dist}m'\n        reachable_nodes = []\n        \n        # For each station, find reachable nodes\n        for idx, station in stations.iterrows():\n            try:\n                # Find nearest node to station\n                start_node = ox.distance.nearest_nodes(walk_net, station.geometry.x, station.geometry.y)\n                \n                # Get reachable nodes within distance\n                reachable = nx.single_source_dijkstra_path_length(\n                    walk_net, start_node, cutoff=dist, weight='length'\n                )\n                \n                # Get coordinates of reachable nodes\n                for node_id in reachable:\n                    x, y = walk_net.nodes[node_id]['x'], walk_net.nodes[node_id]['y']\n                    reachable_nodes.append(Point(x, y))\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error calculating walking service area for station {idx}: {str(e)[:100]}...\")\n                continue\n        \n        # Create service area from reachable nodes\n        if len(reachable_nodes) &gt;= 3:\n            try:\n                # Create convex hull from reachable nodes\n                multi_point = MultiPoint(reachable_nodes)\n                service_areas[area_key] = multi_point.convex_hull\n                success_count += 1\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error creating convex hull for {area_key}: {str(e)[:100]}...\")\n                # Fallback to buffer\n                service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n                error_count += 1\n        else:\n            # If too few reachable nodes, use buffer\n            print(f\"Too few reachable nodes ({len(reachable_nodes)}) for {area_key}. Using buffer.\")\n            service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n            error_count += 1\n    \n    # Create driving service areas\n    for dist in drive_distances:\n        print(f\"Creating driving service area for {dist}m...\")\n        area_key = f'drive_{dist}m'\n        reachable_nodes = []\n        \n        # For each station, find reachable nodes\n        for idx, station in stations.iterrows():\n            try:\n                # Find nearest node to station\n                start_node = ox.distance.nearest_nodes(drive_net, station.geometry.x, station.geometry.y)\n                \n                # Get reachable nodes within distance\n                reachable = nx.single_source_dijkstra_path_length(\n                    drive_net, start_node, cutoff=dist, weight='length'\n                )\n                \n                # Get coordinates of reachable nodes\n                for node_id in reachable:\n                    x, y = drive_net.nodes[node_id]['x'], drive_net.nodes[node_id]['y']\n                    reachable_nodes.append(Point(x, y))\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error calculating driving service area for station {idx}: {str(e)[:100]}...\")\n                continue\n        \n        # Create service area from reachable nodes\n        if len(reachable_nodes) &gt;= 3:\n            try:\n                # Create convex hull from reachable nodes\n                multi_point = MultiPoint(reachable_nodes)\n                service_areas[area_key] = multi_point.convex_hull\n                success_count += 1\n            except Exception as e:\n                if DEBUG_MODE:\n                    print(f\"Error creating convex hull for {area_key}: {str(e)[:100]}...\")\n                # Fallback to buffer\n                service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n                error_count += 1\n        else:\n            # If too few reachable nodes, use buffer\n            print(f\"Too few reachable nodes ({len(reachable_nodes)}) for {area_key}. Using buffer.\")\n            service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n            error_count += 1\n    \n    print(f\"Service area generation: {success_count} successes, {error_count} fallbacks to buffers\")\n    return service_areas\n\n\ndef perform_equity_analysis(census, areas):\n    \"\"\"Compute effect size and confidence intervals for equity metrics.\"\"\"\n    # Check if we should use cached equity results\n    cache_file = os.path.join(CACHE_DIR, 'equity_analysis_cache.pkl')\n    \n    # Try to load from cache\n    if os.path.exists(cache_file):\n        try:\n            print(\"Loading equity analysis from cache...\")\n            with open(cache_file, 'rb') as f:\n                equity = pickle.load(f)\n            print(f\"Loaded equity analysis for {len(equity)} service areas from cache\")\n            return equity\n        except Exception as e:\n            print(f\"Error loading equity analysis cache: {e}\")\n            # Continue with calculation\n    \n    # Perform equity analysis if not cached\n    print(\"Computing equity analysis...\")\n    equity = {}\n    for label, geom in areas.items():\n        covered = census[census.geometry.intersects(geom)]\n        not_covered = census[~census.index.isin(covered.index)]\n        metrics = {}\n        for var in ['median_income','poverty_rate','pop_density','bach_degree_rate']:\n            a = covered[var].dropna()\n            b = not_covered[var].dropna()\n            if len(a)&gt;1 and len(b)&gt;1:\n                pooled = np.sqrt(((len(a)-1)*a.var()+(len(b)-1)*b.var())/(len(a)+len(b)-2))\n                d = (a.mean()-b.mean())/pooled\n                se = pooled*np.sqrt(1/len(a)+1/len(b))\n                ci = (d-1.96*se, d+1.96*se)\n                pval = stats.ttest_ind(a,b).pvalue\n                metrics[var] = {'effect_size':d,'ci_lower':ci[0],'ci_upper':ci[1],'p_value':pval}\n        equity[label] = metrics\n    \n    # Cache the results\n    try:\n        with open(cache_file, 'wb') as f:\n            pickle.dump(equity, f)\n        print(f\"Saved equity analysis to {cache_file}\")\n    except Exception as e:\n        print(f\"Failed to save equity analysis cache: {e}\")\n    \n    return equity\n\n\n# ----------------------------------------------------------\n# Compute dynamic weights based on equity disparities\ndef calculate_dynamic_weights(equity_results):\n    # Base weights for each component - starting point\n    weights = {'coverage':0.30, 'income':0.20, 'poverty':0.20, 'density':0.15, 'education':0.15}\n    \n    # Track effect sizes and significance by variable across all buffer types\n    effect_sizes = {'income': 0, 'poverty': 0, 'density': 0, 'education': 0}\n    significant_effects = {'income': False, 'poverty': False, 'density': False, 'education': False}\n    \n    # Map variable names to weight keys\n    var_to_weight = {\n        'median_income': 'income',\n        'poverty_rate': 'poverty',\n        'pop_density': 'density',\n        'bach_degree_rate': 'education'\n    }\n    \n    # First pass: collect maximum effect sizes and check significance\n    for buf, metrics in equity_results.items():\n        for var, stats in metrics.items():\n            if var in var_to_weight:\n                weight_key = var_to_weight[var]\n                # Track the highest absolute effect size found\n                effect_sizes[weight_key] = max(effect_sizes[weight_key], abs(stats['effect_size']))\n                # Mark as significant if p-value &lt; 0.05\n                if stats['p_value'] &lt; 0.05:\n                    significant_effects[weight_key] = True\n    \n    # Adjust weights based on effect sizes and significance\n    for weight_key, effect_size in effect_sizes.items():\n        is_significant = significant_effects[weight_key]\n        \n        if is_significant:\n            # For significant effects, apply progressive scaling:\n            # Small effect (0.2-0.5): +5%\n            # Medium effect (0.5-0.8): +10%\n            # Large effect (&gt;0.8): +15%\n            if effect_size &gt; 0.8:\n                weights[weight_key] += 0.15\n            elif effect_size &gt; 0.5:\n                weights[weight_key] += 0.10\n            elif effect_size &gt; 0.2:\n                weights[weight_key] += 0.05\n    \n    # Normalize weights to sum to 1\n    total = sum(weights.values())\n    normalized_weights = {k: v/total for k, v in weights.items()}\n    \n    if DEBUG_MODE:\n        print(\"Original weights:\", weights)\n        print(\"Effect sizes:\", effect_sizes)\n        print(\"Significant effects:\", significant_effects)\n        print(\"Normalized weights:\", normalized_weights)\n    \n    return normalized_weights\n\n\n# ----------------------------------------------------------\n# Apply final gap scoring by combining coverage, income, poverty, density, and education\ndef apply_gap_scoring(census, dyn_wts):\n    \"\"\"\n    Apply a simpler gap scoring methodology using:\n    1. Linear combinations of normalized components\n    2. Quantile-based thresholds for natural data distribution\n    \"\"\"\n    # Step 1: Create simple normalized components (0-1 scale)\n    \n    # Coverage component (higher coverage =&gt; lower gap)\n    cov_norm = 1 - census['coverage_score']  # Invert so higher = worse coverage\n    cov_comp = cov_norm * dyn_wts['coverage']\n    \n    # Income component (lower income =&gt; higher gap)\n    inc_norm = (census['median_income'].max() - census['median_income']) / (census['median_income'].max() - census['median_income'].min())\n    inc_comp = inc_norm * dyn_wts['income']\n    \n    # Poverty component (higher poverty =&gt; higher gap)\n    pov_norm = (census['poverty_rate'] - census['poverty_rate'].min()) / (census['poverty_rate'].max() - census['poverty_rate'].min())\n    pov_comp = pov_norm * dyn_wts['poverty']\n    \n    # Density component (higher pop density =&gt; higher gap)\n    den_norm = (census['pop_density'] - census['pop_density'].min()) / (census['pop_density'].max() - census['pop_density'].min())\n    den_comp = den_norm * dyn_wts['density']\n    \n    # Education component (lower education =&gt; higher gap)\n    edu_norm = (census['bach_degree_rate'].max() - census['bach_degree_rate']) / (census['bach_degree_rate'].max() - census['bach_degree_rate'].min())\n    edu_comp = edu_norm * dyn_wts['education']\n    \n    # Step 2: Linear combination of components (simple weighted sum)\n    census['gap_score'] = cov_comp + inc_comp + pov_comp + den_comp + edu_comp\n    \n    # Step 3: Use fixed thresholds based on meaningful categories rather than quartiles\n    # Low, Medium, High, Critical priority with more tracts in lower categories\n    # These fixed thresholds create a right-skewed distribution\n    # Using global THRESHOLDS instead of defining locally\n    \n    if DEBUG_MODE:\n        print(f\"Fixed thresholds: {THRESHOLDS}\")\n    \n    labels = ['Low Priority','Medium Priority','High Priority','Critical Priority']\n    census['gap_category'] = pd.cut(census['gap_score'], \n                                   bins=[-np.inf] + THRESHOLDS + [np.inf], \n                                   labels=labels, \n                                   include_lowest=True)\n\n    # Print component statistics for debugging\n    if DEBUG_MODE:\n        print(\"\\n=== GAP SCORE COMPONENTS ===\")\n        print(f\"Coverage component (weight: {dyn_wts['coverage']:.2f}):\")\n        print(cov_comp.describe())\n        print(f\"\\nIncome component (weight: {dyn_wts['income']:.2f}):\")\n        print(inc_comp.describe())\n        print(f\"\\nPoverty component (weight: {dyn_wts['poverty']:.2f}):\")\n        print(pov_comp.describe())\n        print(f\"\\nDensity component (weight: {dyn_wts['density']:.2f}):\")\n        print(den_comp.describe())\n        print(f\"\\nEducation component (weight: {dyn_wts['education']:.2f}):\")\n        print(edu_comp.describe())\n\n    # Print final gap score statistics\n    if DEBUG_MODE:\n        print(\"\\n=== GAP SCORE STATISTICS ===\")\n        print(census['gap_score'].describe())\n        \n        # Print distribution of priority categories\n        print(\"\\n=== PRIORITY CATEGORY DISTRIBUTION ===\")\n        cat_counts = census['gap_category'].value_counts().sort_index()\n        for category, count in cat_counts.items():\n            percentage = (count / len(census)) * 100\n            print(f\"{category}: {count} tracts ({percentage:.1f}%)\")\n\n    return census\n\n\n# ----------------------------------------------------------\n# Compute coverage statistics by demographic category\ndef compute_coverage_by_demographics(census):\n    demo_cov = {}\n    # Loop through each demographic dimension\n    for col in ['income_category','education_category','density_category']:\n        demo_cov[col] = {}\n        for cat in census[col].unique():\n            sub = census[census[col] == cat]\n            total_pop = int(sub['total_pop'].sum())\n            covered_pop = int(sub[sub['is_served']]['total_pop'].sum())\n            pct = covered_pop / total_pop * 100 if total_pop&gt;0 else 0\n            demo_cov[col][cat] = {'total_population': total_pop, 'covered_population': covered_pop, 'coverage_percent': pct}\n    return demo_cov\n\n\n# ----------------------------------------------------------\n# Save all pipeline outputs: GeoPackage, JSON/CSV stats, and README\ndef save_pipeline_outputs(census, stations, areas, equity_res, demo_cov, dyn_wts):\n    os.makedirs('data', exist_ok=True)\n    # Save census tracts and stations\n    census.to_file('data/gap_analysis_fixed.gpkg', layer='census', driver='GPKG')\n    stations.to_file('data/gap_analysis_fixed.gpkg', layer='stations', driver='GPKG')\n    # Save each service area as its own layer\n    for label, geom in areas.items():\n        gdf = gpd.GeoDataFrame(geometry=[geom], crs=census.crs)\n        gdf.to_file('data/gap_analysis_fixed.gpkg', layer=f'service_area_{label}', driver='GPKG')\n    # Equity analysis JSON & CSV\n    with open('data/equity_analysis.json','w') as f: json.dump(equity_res, f, indent=2)\n    eq_list = []\n    for buf, mets in equity_res.items():\n        for var, st in mets.items(): eq_list.append({'buffer':buf,'variable':var,**st})\n    pd.DataFrame(eq_list).to_csv('data/equity_analysis.csv', index=False)\n    # Demographic coverage JSON & CSV\n    with open('data/coverage_by_demographics.json','w') as f: json.dump(demo_cov, f, indent=2)\n    cv_list = []\n    for col, cats in demo_cov.items():\n        for cat, st in cats.items(): cv_list.append({'demographic_group':col,'category':cat,**st})\n    pd.DataFrame(cv_list).to_csv('data/coverage_statistics.csv', index=False)\n    # Summary of dynamic weights and overall coverage\n    summary = {'dynamic_weights':dyn_wts,'total_tracts':len(census),'total_stations':len(stations),'coverage_percent':float(census['is_served'].mean()*100)}\n    with open('data/analysis_summary.json','w') as f: json.dump(summary, f, indent=2)\n    \n    # Calculate statistics by gap category\n    gap_stats = {}\n    priority_levels = census['gap_category'].unique()\n    for level in priority_levels:\n        tracts = census[census['gap_category'] == level]\n        gap_stats[level] = {\n            'tract_count': int(len(tracts)),\n            'population': int(tracts['total_pop'].sum()),\n            'area_sq_mi': float(tracts.geometry.area.sum() / (5280 * 5280)),  # Convert sq ft to sq mi\n            'pop_density': float(tracts['total_pop'].sum() / (tracts.geometry.area.sum() / (5280 * 5280))),\n            'median_income_avg': float(tracts['median_income'].mean()),\n            'poverty_rate_avg': float(tracts['poverty_rate'].mean())\n        }\n    \n    # Calculate summary stats for README\n    coverage_percent = float(census[census['is_served']]['total_pop'].sum() / census['total_pop'].sum() * 100)\n\n    # Create more detailed README with buffer distances\n    readme = f\"\"\"\n# EV Charging Gap Analysis Results\n\n**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## Overview\nThis analysis identifies areas in Philadelphia with the greatest need for additional EV charging infrastructure, \nbased on physical access to existing chargers and socioeconomic factors.\n\n## Methodology\nThe gap score combines:\n- **Coverage score** ({dyn_wts['coverage']*100:.1f}%): Physical access to existing chargers\n- **Socioeconomic scores** ({(1-dyn_wts['coverage'])*100:.1f}%):\n  - Income ({dyn_wts['income']*100:.1f}%)\n  - Poverty ({dyn_wts['poverty']*100:.1f}%)\n  - Population density ({dyn_wts['density']*100:.1f}%)\n  - Education ({dyn_wts['education']*100:.1f}%)\n\n## Service Area Buffer Distances\n| Mode | Distance | Time Equivalent |\n|------|----------|-----------------|\n| Walking | 1,320 ft | ~5 min walk |\n| Walking | 2,640 ft | ~10 min walk |\n| Walking | 3,960 ft | ~15 min walk |\n| Driving | 5,280 ft | ~1 mile |\n| Driving | 10,560 ft | ~2 miles |\n| Driving | 15,840 ft | ~3 miles |\n\n## Priority Categories\n- **Low Priority:** gap_score ≤ {THRESHOLDS[0]}\n- **Medium Priority:** {THRESHOLDS[0]} &lt; gap_score ≤ {THRESHOLDS[1]}\n- **High Priority:** {THRESHOLDS[1]} &lt; gap_score ≤ {THRESHOLDS[2]}\n- **Critical Priority:** gap_score &gt; {THRESHOLDS[2]}\n\n## Results Summary\n- Total census tracts: {len(census)}\n- Total EV stations: {len(stations)}\n- Population coverage: {coverage_percent:.1f}%\n- Critical priority areas: {gap_stats.get('Critical Priority', {}).get('tract_count', 0)} tracts\n\n## Output Files\n- **gap_analysis_fixed.gpkg**: GeoPackage with spatial layers\n- **analysis_results.json**: Complete nested analysis results\n- **summary_statistics.json**: Key population statistics by category\n- **equity_analysis.csv**: Detailed equity analysis results\n- **coverage_statistics.csv**: Coverage by demographic group\n- **visualizations/**: Charts, maps and summary tables\n\"\"\"\n    with open('data/README.md','w') as f: f.write(readme)\n\n    # Save complete analysis results (nested structure)\n    complete_results = {\n        'summary': {\n            'total_census_tracts': len(census),\n            'total_stations': len(stations),\n            'total_population': int(census['total_pop'].sum()),\n            'study_area_sq_mi': float(census.geometry.area.sum() / (5280 * 5280))\n        },\n        'coverage': {\n            'served_tracts': int(census['is_served'].sum()),\n            'served_population': int(census[census['is_served']]['total_pop'].sum()),\n            'service_coverage_pct': coverage_percent\n        },\n        'gap_scores': {\n            'mean': float(census['gap_score'].mean()),\n            'median': float(census['gap_score'].median()),\n            'min': float(census['gap_score'].min()),\n            'max': float(census['gap_score'].max()),\n            'std': float(census['gap_score'].std())\n        },\n        'priority_areas': gap_stats,\n        'weights': dyn_wts,\n        'demographic_coverage': demo_cov,\n        'validation': census['validation'] if 'validation' in census.columns else {}\n    }\n    \n    with open('data/analysis_results.json', 'w') as f:\n        json.dump(complete_results, f, indent=2, default=str)\n    \n    # Save summary statistics with population counts by category\n    summary_stats = {\n        'total_tracts': len(census),\n        'total_stations': len(stations),\n        'total_population': int(census['total_pop'].sum()),\n        'area_sq_mi': float(census.geometry.area.sum() / (5280 * 5280)),\n        'population_density': float(census['total_pop'].sum() / (census.geometry.area.sum() / (5280 * 5280))),\n        'coverage': {\n            'served_population': int(census[census['is_served']]['total_pop'].sum()),\n            'coverage_percent': coverage_percent\n        },\n        'gap_categories': {\n            level: {\n                'tract_count': gap_stats[level]['tract_count'],\n                'population': gap_stats[level]['population']\n            } for level in priority_levels\n        }\n    }\n    \n    with open('data/summary_statistics.json', 'w') as f:\n        json.dump(summary_stats, f, indent=2)\n\n\n# ----------------------------------------------------------\n# Generate visualizations: histogram, equity summary table, bar chart, and interactive map\ndef create_visualizations(census, equity_res, demo_cov):\n    \"\"\"Generate visualizations: histogram, equity summary table, bar chart, and interactive map.\"\"\"\n    # Create the directory if it doesn't exist\n    vis_dir = 'data/visualizations'\n    os.makedirs(vis_dir, exist_ok=True)\n    \n    # Make sure the directory is writable\n    if not os.access(vis_dir, os.W_OK):\n        print(f\"ERROR: Directory {vis_dir} is not writable!\")\n        return\n    \n    # 1) Histogram of gap scores with threshold lines and priority labels\n    try:\n        # Set up the figure\n        plt.figure(figsize=(14, 8))\n        \n        # Create the histogram with gap scores\n        n, bins, patches = plt.hist(census['gap_score'], \n                                   bins=30, \n                                   color='steelblue', \n                                   edgecolor='black', \n                                   alpha=0.8)\n        \n        # Add vertical lines for score thresholds\n        for i, threshold in enumerate(THRESHOLDS):\n            plt.axvline(x=threshold, color='red', linestyle='--', alpha=0.7, linewidth=2)\n            \n            # Add label above the line with a white background for better readability\n            label_y = max(n) * 0.85  # Position at 85% of max height\n            \n            # Create a white background for the label\n            bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"red\", alpha=0.8)\n            \n            # Position the label slightly to the right of the line to avoid overlap\n            plt.text(threshold + 0.01, label_y, f\"{threshold:.2f}\", \n                    color='red', fontweight='bold', ha='left', va='center',\n                    bbox=bbox_props)\n        \n        # Add priority area labels centered in each section\n        priorities = [\"Low Priority\", \"Medium Priority\", \"High Priority\", \"Critical Priority\"]\n        section_mids = [0.375, (THRESHOLDS[0]+THRESHOLDS[1])/2, \n                        (THRESHOLDS[1]+THRESHOLDS[2])/2, (THRESHOLDS[2] + 0.8)/2]\n        \n        for i, (priority, x_pos) in enumerate(zip(priorities, section_mids)):\n            plt.text(x_pos, max(n) * 0.95, priority, \n                    ha='center', va='center', fontsize=12, fontweight='bold')\n        \n        plt.title('Distribution of Gap Scores with Fixed Thresholds', fontsize=14)\n        plt.xlabel('Gap Score')\n        plt.ylabel('Number of Tracts')\n        plt.grid(True, alpha=0.3)\n        \n        # Save the histogram\n        hist_file = f'{vis_dir}/gap_score_distribution.png'\n        plt.savefig(hist_file, dpi=200)\n        plt.close()  # Make sure to close the plot to avoid displaying it\n        \n        if os.path.exists(hist_file) and os.path.getsize(hist_file) &gt; 0:\n            print(f\"Successfully created histogram: {hist_file}\")\n        else:\n            print(f\"ERROR: Failed to create histogram or file is empty: {hist_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating histogram: {str(e)}\")\n    \n    # 2) Demographic equity summary CSV\n    try:\n        if DEBUG_MODE:\n            print(\"Equity results structure:\", equity_res.keys())\n            print(\"First buffer sample:\", list(equity_res.values())[0] if equity_res else \"Empty\")\n\n        rows = []\n        for buffer_name, metrics in equity_res.items():\n            for variable, stats in metrics.items():\n                row = {'buffer': buffer_name, 'variable': variable}\n                for stat_name, stat_value in stats.items():\n                    row[stat_name] = stat_value\n                rows.append(row)\n        \n        if not rows:\n            print(\"WARNING: No equity data available for CSV export!\")\n            \n        eq_df = pd.DataFrame(rows)\n\n        if DEBUG_MODE:\n            print(f\"Equity DataFrame shape: {eq_df.shape}\")\n            if not eq_df.empty:\n                print(eq_df.head(2))\n\n        # Save to visualizations subfolder\n        csv_file = f'{vis_dir}/demographic_equity_summary.csv'\n        eq_df.to_csv(csv_file, index=False)\n        \n        # Check that file was created successfully\n        if os.path.exists(csv_file) and os.path.getsize(csv_file) &gt; 0:\n            print(f\"Successfully created equity CSV: {csv_file}\")\n        else:\n            print(f\"ERROR: Failed to create equity CSV or file is empty: {csv_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating equity CSV: {str(e)}\")\n    \n    # 3) Bar chart of coverage by demographic group\n    try:\n        # Debug the input data\n        if DEBUG_MODE:\n            print(\"\\nDemographic coverage data:\")\n            print(f\"Keys: {demo_cov.keys()}\")\n            for col, cats in demo_cov.items():\n                print(f\"  {col}: {len(cats)} categories\")\n                for cat, stats in cats.items():\n                    print(f\"    {cat}: {stats}\")\n        \n        # Prepare data for plotting\n        categories = ['density', 'education', 'income']\n        levels = ['low', 'medium', 'high']\n        \n        # Create dataframe with simplified structure\n        plot_data = []\n        \n        for category in categories:\n            cat_key = f\"{category}_category\"\n            if cat_key in demo_cov:\n                for level in levels:\n                    level_key = f\"{level}_{category}\"\n                    if level_key in demo_cov[cat_key]:\n                        coverage = demo_cov[cat_key][level_key]['coverage_percent']\n                        plot_data.append({\n                            'category': category.title(),\n                            'level': level.title(),\n                            'coverage': coverage\n                        })\n        \n        # Convert to DataFrame\n        plot_df = pd.DataFrame(plot_data)\n        \n        if plot_df.empty:\n            print(\"WARNING: No demographic coverage data available for bar chart!\")\n            return\n            \n        if DEBUG_MODE:\n            print(f\"\\nPlot DataFrame shape: {plot_df.shape}\")\n            print(plot_df.head())\n        \n        # Create improved bar chart\n        plt.figure(figsize=(12, 7))\n        \n        # Calculate positions for bars\n        categories_n = len(categories)\n        levels_n = len(levels)\n        width = 0.8 / levels_n  # Bar width\n        \n        # Colors for bars\n        colors = ['#FF8C61', '#FFB56B', '#FDD57E']\n        \n        # Plot bars for each level within each category\n        for i, level in enumerate(levels):\n            level_data = plot_df[plot_df['level'] == level.title()]\n            x_positions = np.arange(categories_n) + (i - levels_n/2 + 0.5) * width\n            plt.bar(x_positions, \n                   level_data['coverage'], \n                   width=width, \n                   color=colors[i % len(colors)],\n                   label=level.title())\n        \n        # Set chart title and labels\n        plt.title('Service Coverage by Census Tract Characteristic', fontsize=14)\n        plt.ylabel('Coverage Percentage')\n        plt.ylim(0, 55)  # Set y limit to give space for labels\n        \n        # Set x-axis ticks - use an empty string to remove the labels\n        plt.xticks(np.arange(categories_n), [''] * categories_n)\n        \n        # Add level labels beneath each category\n        for i, category in enumerate(categories):\n            for j, level in enumerate(levels):\n                x_pos = i + (j - levels_n/2 + 0.5) * width\n                plt.text(x_pos, -5, level.title(), \n                        ha='center', fontsize=10)\n        \n        # Add bracket annotations for categories\n        for i, category in enumerate(categories):\n            # Draw bracket from first to last bar in category\n            first_x = i - width * levels_n/2 + width/2\n            last_x = i + width * levels_n/2 - width/2\n            mid_x = i\n            \n            # Bracket height\n            y_bracket = -8\n            bracket_height = 2\n            \n            # Draw the horizontal lines\n            plt.plot([first_x, last_x], [y_bracket, y_bracket], 'k-', lw=1.5)\n            \n            # Add the category label\n            plt.text(mid_x, y_bracket - bracket_height - 2, category.title(), \n                    ha='center', fontweight='bold', fontsize=12)\n        \n        plt.grid(True, alpha=0.3, axis='y')\n        plt.ylim(bottom=-14)  # Extend bottom margin for labels and brackets\n        plt.tight_layout()\n        \n        # Save and verify file\n        bar_file = f'{vis_dir}/demographic_coverage.png'\n        plt.savefig(bar_file, dpi=200)\n        plt.close()\n        \n        # Check that file was created successfully\n        if os.path.exists(bar_file) and os.path.getsize(bar_file) &gt; 0:\n            print(f\"Successfully created bar chart: {bar_file}\")\n        else:\n            print(f\"ERROR: Failed to create bar chart or file is empty: {bar_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating bar chart: {str(e)}\")\n    \n    # 4) Interactive Folium map of priority areas and stations\n    try:\n        m = folium.Map(location=[39.9526, -75.1652], zoom_start=11, tiles='CartoDB positron')\n        # Priority colors\n        colors = {'Low Priority':'#fee5d9','Medium Priority':'#fcae91','High Priority':'#fb6a4a','Critical Priority':'#cb181d'}\n        \n        # Add tracts\n        for _, r in census.to_crs('EPSG:4326').iterrows():\n            fc = folium.GeoJson(\n                shapely_mapping(r['geometry']), \n                style_function=lambda f,cat=r['gap_category']: {\n                    'fillColor': colors.get(cat,'gray'),\n                    'color': 'black',\n                    'weight': 1,\n                    'fillOpacity': 0.7\n                },\n                # Enhanced tooltip with more tract info\n                tooltip=f\"Priority: {r['gap_category']}&lt;br&gt;Population: {int(r['total_pop']) if not pd.isna(r['total_pop']) else 'N/A'}&lt;br&gt;Income: ${int(r['median_income']):,} \" if not pd.isna(r['median_income']) else \"Priority: {r['gap_category']}&lt;br&gt;Population: {int(r['total_pop']) if not pd.isna(r['total_pop']) else 'N/A'}&lt;br&gt;Income: N/A\"\n            )\n            fc.add_to(m)\n            \n        # Add stations\n        for _, s in stations.to_crs('EPSG:4326').iterrows():\n            folium.CircleMarker(\n                location=[s.geometry.y, s.geometry.x],\n                radius=5,\n                color='black',\n                fill=True,\n                fill_color='white',\n                fill_opacity=1,\n                # Enhanced tooltip with station info\n                tooltip=f\"Station: {s['name']}&lt;br&gt;Points: {int(s['num_points']) if not pd.isna(s['num_points']) else 'N/A'}&lt;br&gt;Max Power: {s['max_power']:.1f} kW\" if not pd.isna(s['max_power']) else f\"Station: {s['name']}&lt;br&gt;Points: {int(s['num_points']) if not pd.isna(s['num_points']) else 'N/A'}&lt;br&gt;Max Power: N/A\"\n            ).add_to(m)\n\n        # Add a legend\n        legend_html = '''\n        &lt;div style=\"position: fixed; \n                   bottom: 50px; right: 50px; \n                   border:2px solid grey; z-index:9999; \n                   background-color:white;\n                   padding:10px;\n                   font-size:14px;\n                   \"&gt;\n        &lt;p&gt;&lt;b&gt;Priority Levels&lt;/b&gt;&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #fee5d9; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; Low Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #fcae91; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; Medium Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #fb6a4a; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; High Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: #cb181d; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; Critical Priority&lt;/p&gt;\n        &lt;p&gt;&lt;i style=\"background: white; border: 2px solid black; padding:5px;\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/i&gt; EV Station&lt;/p&gt;\n        &lt;/div&gt;\n        '''\n        m.get_root().html.add_child(folium.Element(legend_html))\n        \n        # Save and verify file\n        map_file = f'{vis_dir}/priority_areas_map.html'\n        m.save(map_file)\n        \n        # Check that file was created successfully\n        if os.path.exists(map_file) and os.path.getsize(map_file) &gt; 0:\n            print(f\"Successfully created interactive map: {map_file}\")\n        else:\n            print(f\"ERROR: Failed to create interactive map or file is empty: {map_file}\")\n            \n    except Exception as e:\n        print(f\"ERROR creating interactive map: {str(e)}\")\n\n\n# Filter stations by charging speed and minimum points \ndef filter_stations(stations, charging_speed='All', min_points=1):\n    \"\"\"\n    Filter charging stations by charging speed category and minimum number of points.\n    \n    Parameters:\n    -----------\n    stations : GeoDataFrame\n        Station data with num_points and charging_speed columns\n    charging_speed : str\n        One of: 'All', 'Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)'\n    min_points : int\n        Minimum number of charging points (outlets) per station\n    \n    Returns:\n    --------\n    GeoDataFrame\n        Filtered station dataset\n    \"\"\"\n    filtered = stations.copy()\n    \n    if charging_speed != 'All':\n        filtered = filtered[filtered['charging_speed'] == charging_speed]\n    \n    filtered = filtered[filtered['num_points'] &gt;= min_points]\n    \n    print(f\"Filtered from {len(stations)} to {len(filtered)} stations \" +\n          f\"(speed: {charging_speed}, min points: {min_points})\")\n    \n    return filtered\n\n\ndef snap_points_to_network(points_gdf, network, cache_name=None):\n    \"\"\"\n    Snap points to the nearest edge on the network and return a new GeoDataFrame\n    with the same attributes but snapped geometries.\n    \n    This improves network analysis accuracy by ensuring that origins and destinations\n    are properly connected to the network.\n    \n    Parameters:\n    -----------\n    points_gdf : GeoDataFrame\n        Points to snap to the network\n    network : networkx.Graph\n        Road network \n    cache_name : str, optional\n        Name to use for caching the snapped points. If provided, will try to load\n        from cache first, and save to cache if not found.\n        \n    Returns:\n    --------\n    GeoDataFrame\n        New GeoDataFrame with original attributes but snapped geometries\n    \"\"\"\n    # Check if we should use cached snapped points\n    if cache_name:\n        cache_file = os.path.join(CACHE_DIR, f'snapped_{cache_name}.pkl')\n        \n        # Try to load from cache\n        if os.path.exists(cache_file):\n            try:\n                print(f\"Loading snapped points from cache: {cache_name}...\")\n                with open(cache_file, 'rb') as f:\n                    snapped_gdf = pickle.load(f)\n                print(f\"Loaded {len(snapped_gdf)} snapped points from cache.\")\n                return snapped_gdf\n            except Exception as e:\n                print(f\"Error loading snapped points cache: {e}\")\n                # Continue with snapping\n    \n    print(f\"Snapping {len(points_gdf)} points to network...\")\n    snapped_points = []\n    failed_snaps = 0\n    \n    for idx, row in tqdm(points_gdf.iterrows(), total=len(points_gdf), desc=\"Snapping points\"):\n        try:\n            # Find the nearest edge\n            nearest_edge = ox.distance.nearest_edges(\n                network, row.geometry.x, row.geometry.y, return_dist=False)\n            \n            # Get the edge geometry (might be a LineString or a geometry attribute)\n            u, v, _ = nearest_edge  # Unpack edge tuple\n            \n            if 'geometry' in network.edges[nearest_edge]:\n                # If edge has a geometry attribute, use it\n                edge_geom = network.edges[nearest_edge]['geometry']\n            else:\n                # Otherwise, create a line between nodes\n                u_x, u_y = network.nodes[u]['x'], network.nodes[u]['y']\n                v_x, v_y = network.nodes[v]['x'], network.nodes[v]['y']\n                edge_geom = LineString([(u_x, u_y), (v_x, v_y)])\n            \n            # Snap point to edge (project point onto line)\n            projected_point = edge_geom.interpolate(\n                edge_geom.project(row.geometry)\n            )\n            \n            # Create new row with snapped geometry\n            new_row = row.copy()\n            new_row.geometry = projected_point\n            snapped_points.append(new_row)\n            \n        except Exception as e:\n            # If snapping fails, keep the original point\n            if DEBUG_MODE:\n                print(f\"Failed to snap point {idx}: {str(e)[:100]}... Using original point.\")\n            failed_snaps += 1\n            snapped_points.append(row)\n    \n    if failed_snaps &gt; 0:\n        print(f\"Warning: {failed_snaps} points ({failed_snaps/len(points_gdf)*100:.1f}%) could not be snapped to the network.\")\n    \n    # Create a new GeoDataFrame with the same attributes\n    snapped_gdf = gpd.GeoDataFrame(snapped_points, crs=points_gdf.crs)\n    \n    # Cache the results if cache_name is provided\n    if cache_name:\n        try:\n            cache_file = os.path.join(CACHE_DIR, f'snapped_{cache_name}.pkl')\n            with open(cache_file, 'wb') as f:\n                pickle.dump(snapped_gdf, f)\n            print(f\"Saved snapped points to cache: {cache_name}\")\n        except Exception as e:\n            print(f\"Failed to save snapped points cache: {e}\")\n    \n    return snapped_gdf\n\n\n# Main entry point to run the complete pipeline\nif __name__ == '__main__':\n    # 1) Load data and networks\n    census, boundary, stations, validation = load_data()\n    \n    # Create NetworkX networks\n    G_walk, G_drive = create_networks()\n    \n    print(\"Using NetworkX for network analysis with point snapping\")\n    \n    # Optional: Filter stations (uncomment and adjust parameters as needed)\n    # Possible values for charging_speed: 'All', 'Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)'\n    # stations = filter_stations(stations, charging_speed='All', min_points=1)\n    \n    # Preprocess: Snap points to network for better connectivity\n    print(\"\\n=== PREPROCESSING POINTS FOR NETWORK ANALYSIS ===\")\n    # Snap stations to the road network\n    stations_snapped = snap_points_to_network(stations, G_drive, cache_name='stations')\n    print(f\"Using {len(stations_snapped)} snapped stations for network analysis\")\n    \n    # Create a GeoDataFrame of census tract centroids\n    census_centroids = gpd.GeoDataFrame(\n        geometry=[point for point in census.geometry.centroid],\n        index=census.index,\n        crs=census.crs\n    )\n    # Snap centroids to the road network\n    centroids_snapped = snap_points_to_network(census_centroids, G_drive, cache_name='centroids')\n    print(f\"Using {len(centroids_snapped)} snapped census tract centroids for network analysis\")\n    print(\"Preprocessing complete. Points are now properly connected to the network.\")\n    \n    # 2) Distances & initial coverage\n    # Use the snapped geometries for better network connectivity\n    origins = {i: row.geometry for i, row in centroids_snapped.iterrows()}\n    walk_d, drive_d = calculate_multimodal_distance_batch(\n        origins, \n        list(stations_snapped.geometry), \n        G_walk, \n        G_drive\n    )\n    cov = calculate_coverage_scores(census, walk_d, drive_d, stations)\n    census = census.join(cov)\n    \n    # Flag served tracts\n    census['is_served'] = census['coverage_score'] &gt; census['coverage_score'].mean()\n    \n    # 3) Service areas\n    # Use snapped stations for better service area generation\n    areas = build_service_areas(stations_snapped, G_walk, G_drive)\n    \n    # 4) Equity analysis\n    equity_res = perform_equity_analysis(census, areas)\n    \n    # 5) Compute dynamic weights and apply gap scoring\n    dyn_wts = calculate_dynamic_weights(equity_res)\n    census = apply_gap_scoring(census, dyn_wts)\n    \n    # 6) Categorize demographics by quantiles\n    census['income_category'] = pd.qcut(census['median_income'], 3, labels=['low_income','medium_income','high_income'])\n    census['education_category'] = pd.qcut(census['bach_degree_rate'], 3, labels=['low_education','medium_education','high_education'])\n    census['density_category'] = pd.qcut(census['pop_density'], 3, labels=['low_density','medium_density','high_density'])\n    \n    # 7) Coverage by demographic groups\n    demo_cov = compute_coverage_by_demographics(census)\n    \n    # 8) Save outputs and visualizations\n    save_pipeline_outputs(census, stations, areas, equity_res, demo_cov, dyn_wts)\n    create_visualizations(census, equity_res, demo_cov)\n\n    # Print equity analysis results if in debug mode\n    if DEBUG_MODE:\n        print(\"\\n=== EQUITY ANALYSIS SUMMARY ===\")\n        # Calculate average values for served vs unserved areas\n        served = census[census['is_served']]\n        unserved = census[~census['is_served']]\n        \n        # Print validation statistics first\n        print(\"\\nValidation Statistics:\")\n        print(f\"- Total stations from API: {validation['total_stations_from_api']}\")\n        print(f\"- Stations within boundary: {validation['stations_within_boundary']}\")\n        print(f\"- Excluded stations: {validation['excluded_stations']}\")\n        print(f\"  - Outside boundary: {validation['exclusion_reasons']['outside_boundary']}\")\n        print(f\"  - Missing coordinates: {validation['exclusion_reasons']['missing_coordinates']}\")\n        \n        # Print service area info if available\n        print(\"\\nService Areas:\")\n        if areas:\n            for area_name, area_geom in areas.items():\n                if not area_geom:\n                    print(f\"- {area_name}: Empty geometry\")\n                    continue\n                    \n                # Count intersecting tracts\n                intersecting = sum(1 for _, tract in census.iterrows() if tract.geometry.intersects(area_geom))\n                percent = (intersecting / len(census)) * 100\n                print(f\"- {area_name}: {intersecting} tracts ({percent:.1f}%)\")\n        else:\n            print(\"No service areas were created.\")\n        \n        # Print metric comparisons (original code)\n        print(\"\\nServed vs. Unserved Areas:\")\n        metrics = ['median_income', 'poverty_rate', 'pop_density', 'bach_degree_rate']\n        print(f\"{'Metric':&lt;20} {'Served':&gt;12} {'Unserved':&gt;12} {'Percent Diff':&gt;12} {'Significant':&gt;10}\")\n        print(\"-\" * 70)\n        \n        for var in metrics:\n            served_mean = served[var].mean()\n            unserved_mean = unserved[var].mean()\n            pct_diff = ((served_mean - unserved_mean) / unserved_mean) * 100 if unserved_mean != 0 else 0\n            t_stat, p_val = stats.ttest_ind(served[var].dropna(), unserved[var].dropna())\n            \n            print(f\"{var:&lt;20} {served_mean:&gt;12.1f} {unserved_mean:&gt;12.1f} {pct_diff:&gt;+12.1f}% {'Yes' if p_val &lt; 0.05 else 'No':&gt;10}\")\n\n    # Print dynamic weights if in debug mode\n    if DEBUG_MODE:\n        print(\"\\n=== DYNAMIC WEIGHTS ===\")\n        for k, v in dyn_wts.items():\n            print(f\"{k}: {v:.4f} ({v*100:.1f}%)\")\n\n    # Add validation to pipeline outputs\n    census['validation'] = validation\n    print('Complete gap analysis pipeline.')"
  },
  {
    "objectID": "analysis/2-Service-Gap-Analysis.html#data-preprocessing",
    "href": "analysis/2-Service-Gap-Analysis.html#data-preprocessing",
    "title": "Network Service Gap Methods & Analysis",
    "section": "Data & Pre‑processing",
    "text": "Data & Pre‑processing\n\nCensus Data: ACS data loaded from GeoPackage\nFilters: Tracts with population &gt; 0 and density ≥ 1000 people/mi²\nFiltered Tracts: 385 census tracts remained after filtering, with 23 tracts excluded from the original 408 Philadelphia tracts\nBoundary: Philadelphia city limits from GeoJSON\nProjection: WGS84 (4326) initially, then State Plane (2272) for analysis"
  },
  {
    "objectID": "analysis/2-Service-Gap-Analysis.html#ev-station-retrieval",
    "href": "analysis/2-Service-Gap-Analysis.html#ev-station-retrieval",
    "title": "Network Service Gap Methods & Analysis",
    "section": "EV Station Retrieval",
    "text": "EV Station Retrieval\n\nAPI: OpenChargeMap v3 with 10km search radius\nStation Data: Extracts coordinates, connection points, max power (kW)\nStation Filtering: Of 150 stations retrieved from the API, 20 were excluded for being outside the city boundary, leaving 130 stations for analysis\nStation Classification:\n\nLevel 1 (Slow): &lt; 7 kW\nLevel 2 (Medium): 7-50 kW\nDC Fast (Rapid): &gt; 50 kW\n\nValidation: Tracks excluded stations and reasons (outside boundary, missing coordinates)"
  },
  {
    "objectID": "analysis/2-Service-Gap-Analysis.html#network-analysis",
    "href": "analysis/2-Service-Gap-Analysis.html#network-analysis",
    "title": "Network Service Gap Methods & Analysis",
    "section": "Network Analysis",
    "text": "Network Analysis\n\nNetworks: OSMnx-generated walking and driving networks\nPoint Snapping: Stations and census tract centroids are snapped to the nearest edge on the network for accurate routing\nDistance Calculation: NetworkX-based shortest path routing with straight-line distance fallback\nDistance Filter: 6km threshold to eliminate unnecessary calculations for distant pairs\nCaching: Extensive caching of networks, snapped points, distances, service areas, and equity analysis for performance\nService Areas:\n\nWalking: 1,320ft (5min), 2,640ft (10min), 3,960ft (15min)\nDriving: 5,280ft (1mi), 10,560ft (2mi), 15,840ft (3mi)"
  },
  {
    "objectID": "analysis/2-Service-Gap-Analysis.html#methodology-computing-the-ev-charging-gap-score",
    "href": "analysis/2-Service-Gap-Analysis.html#methodology-computing-the-ev-charging-gap-score",
    "title": "Network Service Gap Methods & Analysis",
    "section": "Methodology: Computing the EV Charging “Gap Score”",
    "text": "Methodology: Computing the EV Charging “Gap Score”\nThe gap score is a single composite metric that ranks each Philadelphia census tract by its need for additional EV charging infrastructure. It blends:\n\nA coverage metric (physical access to existing chargers)\n\nFour socioeconomic vulnerability metrics (income, poverty, density, education)\n\nTogether, these capture both where chargers are missing and who is most harmed by those gaps.\n\n1. Coverage Score (24%)\n\nWhat it measures\nPer-tract service level based on walking and driving proximity, station quality, and population density.\n\n\nCalculation steps\n\nCompute network distances\n\nShortest-path walking (NetworkX with point snapping) from each tract centroid to all chargers.\nShortest-path driving likewise.\nStraight-line distance with detour factor (1.3× for walking, 1.5× for driving) as fallback when routing fails.\n\nApply proximity bands\n\nWalking:\n\nWithin 1,320 ft (~5 min): 0.7 score\nWithin 2,640 ft (~10 min): 0.5 score\nWithin 3,960 ft (~15 min): 0.3 score\nBeyond: 1.0 score (worst)\n\nDriving:\n\nWithin 5,280 ft (~1 mile): 0.7 score\nWithin 10,560 ft (~2 miles): 0.5 score\nWithin 15,840 ft (~3 miles): 0.3 score\nBeyond: 1.0 score (worst)\n\n\nStation quality adjustment\n\nCalculate quality index for each station: \\[\\text{quality\\_index} = \\text{num\\_points} \\times \\sqrt{\\frac{\\text{max\\_power}}{50}}\\]\nNormalize to 0.5-1.5 range: \\[\\text{quality\\_factor} = 0.5 + \\frac{\\text{quality\\_index} - \\min(\\text{quality\\_index})}{\\max(\\text{quality\\_index}) - \\min(\\text{quality\\_index})}\\]\nMultiply coverage by quality factor\n\nCombine modes\n\\[\\text{coverage\\_score}_i = 0.4 \\times \\text{walk\\_weight}_i + 0.6 \\times \\text{drive\\_weight}_i\\]\nDensity adjustment\n\nScale down by up to 30% for high-density tracts: \\[\\text{coverage\\_score}_i \\times= (1 - 0.3 \\times \\frac{\\text{pop\\_density}_i}{\\max(\\text{pop\\_density})})\\]\n\nLabel “served” vs. “unserved”\n\nServed if coverage_score &gt; mean(coverage_score)\n\nUnserved otherwise\n\n\n\n\n\n2. Socioeconomic Component Scores (Combined 76%)\nWe compute four additional 0–1 scores that capture relative vulnerability:\n\n\n\n\n\n\n\n\nComponent\nWeight\nRationale\n\n\n\n\nIncome\n16%\nLower-income areas may have fewer EV adopters and fewer resources to retrofit homes.\n\n\nPoverty\n24%\nHigh poverty correlates with lower EV uptake and higher energy burden.\n\n\nDensity\n20%\nMore people in a tract → higher absolute charging demand.\n\n\nEducation\n16%\nEducation level often correlates with technology adoption rates.\n\n\n\n\nHow we compute each score\n\nPull ACS census data for each tract.\n\nFor each metric (e.g. median income), scale the raw value so that the most vulnerable tract gets 1.0 and the least vulnerable gets 0.0.\n\ne.g. income_score_i = 1 - (income_i − min)/ (max − min)\n\n\nClip to [0, 1] range.\n\n\n\n\n3. Combining into the Gap Score\nFor each tract \\(i\\), we use a simple linear combination of the normalized components:\n\\[\\begin{aligned}\n\\text{gap\\_score}_i\n&= w_{\\text{cov}}(1 - \\text{coverage}_i) \\\\\n&\\quad + w_{\\text{inc}}\\text{income\\_score}_i\n+ w_{\\text{pov}}\\text{poverty\\_score}_i \\\\\n&\\quad + w_{\\text{den}}\\text{density\\_score}_i\n+ w_{\\text{edu}}\\text{education\\_score}_i\n\\end{aligned}\\]\nThis linear model ensures transparency and interpretability in how each factor contributes to the final score.\n\nComponent directions:\n\nCoverage: Inverted (1 - score) so higher values indicate worse coverage\nIncome: Inverted so lower incomes result in higher scores\nPoverty: Higher poverty rates result in higher scores\nDensity: Higher population density results in higher scores\nEducation: Inverted so lower education levels result in higher scores\n\n\n\nDynamic weights\nWeights are calculated based on observed disparities:\n\nWe start with base weights that sum to 1.0:\n\n\\(w_{\\text{cov}} = 0.30\\)\n\n\\(w_{\\text{inc}} = 0.20\\)\n\n\\(w_{\\text{pov}} = 0.20\\)\n\n\\(w_{\\text{den}} = 0.15\\)\n\n\\(w_{\\text{edu}} = 0.15\\)\n\nWe calculate effect sizes (Cohen’s d) for each socioeconomic variable between served and unserved areas.\nFor metrics with statistically significant disparities (p &lt; 0.05), we adjust weights:\n\nSmall effect (0.2-0.5): +5% weight\nMedium effect (0.5-0.8): +10% weight\nLarge effect (&gt;0.8): +15% weight\n\nWe then re-normalize all weights to sum to 1.0.\nIn our implementation, this resulted in the following weights:\n\n\\(w_{\\text{cov}} = 0.24\\)\n\n\\(w_{\\text{inc}} = 0.16\\)\n\n\\(w_{\\text{pov}} = 0.24\\)\n\n\\(w_{\\text{den}} = 0.20\\)\n\n\\(w_{\\text{edu}} = 0.16\\)\n\n\n\n\n\n4. Thresholding & Priority Tiers\nTo turn a continuous score into actionable categories, we use fixed thresholds:\n\nLow Priority: gap_score ≤ 0.45\nMedium Priority: 0.45 &lt; gap_score ≤ 0.55\nHigh Priority: 0.55 &lt; gap_score ≤ 0.65\nCritical Priority: gap_score &gt; 0.65\n\nThese thresholds create a balanced distribution of priority levels, with approximately 30% Low, 28% Medium, 29% High, and 13% Critical priority tracts.\n\n\n5. Validation & Outputs\nThe analysis pipeline tracks: - Excluded stations and reasons for exclusion - Demographic coverage across income, education, and density levels - Statistical significance of socioeconomic disparities - Component contributions to the final scores - Distribution of priority categories with tract counts and percentages\n\nFootnote:\nTracts with large group‑quarters populations (e.g. university dorms) may show artificially high poverty. We document this caveat but retain them in the analysis to avoid geographic bias."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Master of Urban Spatial AnalyticsUniversity of Pennsylvania\n\nGitHub Repository",
    "crumbs": [
      "Main",
      "About"
    ]
  },
  {
    "objectID": "about.html#project-background",
    "href": "about.html#project-background",
    "title": "About",
    "section": "Project Background",
    "text": "Project Background\nThis project analyzes equity in Philadelphia’s EV charging infrastructure by mapping access disparities across neighborhoods and demographic groups. Using geospatial network analysis and socioeconomic indicators, the study creates a comprehensive gap score system that identifies priority areas for future infrastructure investment. The research combines coverage metrics, income levels, population density, and education data to develop evidence-based recommendations for creating a more inclusive and accessible charging network.\nThe work was completed as part of the Fall 2024 MUSA 550 course at the University of Pennsylvania, applying advanced geospatial analytics and interactive visualization techniques to urban transportation planning challenges.",
    "crumbs": [
      "Main",
      "About"
    ]
  },
  {
    "objectID": "about.html#methodology-tools",
    "href": "about.html#methodology-tools",
    "title": "About",
    "section": "Methodology & Tools",
    "text": "Methodology & Tools\n\nSpatial Analysis\n\nGeoPandas\nNetworkX\nOSMnx for network analysis\n\n\n\nData Processing\n\nPandas\nNumPy\nDynamic weighting system\n\n\n\nVisualization\n\nFolium for interactive maps\nMatplotlib for static charts\nPanel dashboards",
    "crumbs": [
      "Main",
      "About"
    ]
  },
  {
    "objectID": "about.html#data-sources",
    "href": "about.html#data-sources",
    "title": "About",
    "section": "Data Sources",
    "text": "Data Sources\n\nEV Charging Stations: Alternative Fuels Data Center (AFDC)\nDemographics: American Community Survey (ACS) 2020\nStreet Network: OpenStreetMap (OSM)\nCity Boundaries: City of Philadelphia Open Data",
    "crumbs": [
      "Main",
      "About"
    ]
  },
  {
    "objectID": "about.html#acknowledgments",
    "href": "about.html#acknowledgments",
    "title": "About",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nSpecial thanks to the instructors and teaching assistants of MUSA 550 for their guidance and support throughout this project.\n\nHome Methodology Results Dashboard Conclusions",
    "crumbs": [
      "Main",
      "About"
    ]
  },
  {
    "objectID": "analysis/1-Census-Tract-Analysis.html",
    "href": "analysis/1-Census-Tract-Analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The analysis examines Philadelphia’s EV charging infrastructure, which consists of 130 charging stations. The city has 408 total census tracts, but when focusing on residential areas (excluding tracts with zero population and those with population density below 1,000 people per square mile), we analyze 385 tracts. The distribution of charging stations is highly concentrated - only 50 tracts (13.0% of residential tracts) contain any charging stations, with an average of 0.26 stations per tract. This spatial concentration means that more than 87% of Philadelphia’s residential census tracts lack any charging infrastructure, highlighting significant gaps in coverage and accessibility.\nTo ensure our analysis focuses on residential areas where EV charging infrastructure would be most impactful, we applied two filters to the census tract data. First, we excluded tracts with zero population to focus on inhabited areas. Second, we excluded tracts with population density below 1000 people per square mile, as these very low-density areas (which may include industrial zones, large parks, or areas with significant non-residential land use) have different infrastructure needs and characteristics. This filtering helps ensure our analysis reflects the distribution of EV charging infrastructure in areas where residents would most benefit from access to charging stations.\n\n\n\n\n\n\nSeveral limitations should be noted in this analysis.\n\n\n\nFirst, while we focus on residential tracts within Philadelphia city limits, some EV charging stations located near tract boundaries or in adjacent municipalities may serve Philadelphia residents but are not included in our analysis. This is particularly relevant for stations near the city’s borders, where residents might cross municipal boundaries to access charging infrastructure.\nSecond, our analysis excludes tracts with population density below 1,000 people per square mile, which may exclude some legitimate residential areas or mixed-use neighborhoods. These exclusions could potentially underrepresent the true accessibility of EV charging infrastructure for Philadelphia residents, particularly those living near the city’s borders, in lower-density neighborhoods, or near non-residential use areas."
  },
  {
    "objectID": "analysis/1-Census-Tract-Analysis.html#ev-charging-infrastructure-within-city-boundaries",
    "href": "analysis/1-Census-Tract-Analysis.html#ev-charging-infrastructure-within-city-boundaries",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The analysis examines Philadelphia’s EV charging infrastructure, which consists of 130 charging stations. The city has 408 total census tracts, but when focusing on residential areas (excluding tracts with zero population and those with population density below 1,000 people per square mile), we analyze 385 tracts. The distribution of charging stations is highly concentrated - only 50 tracts (13.0% of residential tracts) contain any charging stations, with an average of 0.26 stations per tract. This spatial concentration means that more than 87% of Philadelphia’s residential census tracts lack any charging infrastructure, highlighting significant gaps in coverage and accessibility.\nTo ensure our analysis focuses on residential areas where EV charging infrastructure would be most impactful, we applied two filters to the census tract data. First, we excluded tracts with zero population to focus on inhabited areas. Second, we excluded tracts with population density below 1000 people per square mile, as these very low-density areas (which may include industrial zones, large parks, or areas with significant non-residential land use) have different infrastructure needs and characteristics. This filtering helps ensure our analysis reflects the distribution of EV charging infrastructure in areas where residents would most benefit from access to charging stations.\n\n\n\n\n\n\nSeveral limitations should be noted in this analysis.\n\n\n\nFirst, while we focus on residential tracts within Philadelphia city limits, some EV charging stations located near tract boundaries or in adjacent municipalities may serve Philadelphia residents but are not included in our analysis. This is particularly relevant for stations near the city’s borders, where residents might cross municipal boundaries to access charging infrastructure.\nSecond, our analysis excludes tracts with population density below 1,000 people per square mile, which may exclude some legitimate residential areas or mixed-use neighborhoods. These exclusions could potentially underrepresent the true accessibility of EV charging infrastructure for Philadelphia residents, particularly those living near the city’s borders, in lower-density neighborhoods, or near non-residential use areas."
  },
  {
    "objectID": "analysis/1-Census-Tract-Analysis.html#census-data-analysis",
    "href": "analysis/1-Census-Tract-Analysis.html#census-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Census Data Analysis",
    "text": "Census Data Analysis\n\nTracts with vs. without EV Charging Stations Comparison\nThe demographic analysis of areas with and without charging stations highlights concerning inequities.\nTracts with stations show: - Higher median incomes ($82,179 compared to $61,136). - Higher educational attainment rates (25% versus 13% bachelor’s degree). - Lower poverty rates (18.7% versus 22.8%).\nPopulation characteristics also differ between served and unserved areas. Areas with stations have higher population density (22,974 versus 20,093 people per square mile) but smaller total populations (3,128 versus 4,217 residents). The median age difference is modest, with areas without stations being slightly older (36.6 versus 34.4 years).\nThese patterns indicate that current EV charging infrastructure may be reinforcing existing socioeconomic inequities. Despite higher population density in areas with stations, they serve smaller total populations and are concentrated in wealthier, more educated areas, potentially limiting EV adoption opportunities in less advantaged communities. This analysis supports the need for more equitable distribution of future EV charging infrastructure to ensure broader access across all demographic groups.\n\n\nCode\nfrom PIL import Image\n   \n   # Path to original file\ninput_path = 'data/visualizations/demographic_coverage.png'\n      # Path for smaller file\noutput_path = 'data/visualizations/demographic_coverage_small.png'\n   \n   # Open and resize image\nimg = Image.open(input_path)\n   # Resize to 75% of original size\nnew_width = int(img.width * 0.75)\nnew_height = int(img.height * 0.75)\nimg_resized = img.resize((new_width, new_height))\n   \n   # Save with compression\nimg_resized.save(output_path, 'PNG', optimize=True, quality=85)\n\n\n\n\nCode\n# Philadelphia boundary\nurl = \"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\"\ndistricts = gpd.read_file(url).to_crs('EPSG:4326')\nphilly_boundary = districts['geometry'].union_all()\n\nif not api_key:\n    print(\"Error: No API key found. Please check .env file\")\n    raise Exception(\"API key not found\")\n\nbase_url = \"https://api.openchargemap.io/v3/poi\"\nparams = {\n    'key': api_key,\n    'countrycode': 'US',\n    'maxresults': 1000,\n    'latitude': 39.9526,  # Philadelphia center \n    'longitude': -75.1652,\n    'distance': 10,\n    'distanceunit': 'km',\n    'compact': True,\n    'verbose': False,\n    'output': 'json'\n}\n\ntry:\n    response = requests.get(base_url, params=params, timeout=10)\n    response.raise_for_status()\n    stations_data = response.json()\n    \n    stations_list = []\n    for station in stations_data:\n        try:\n            station_info = {\n                'id': station.get('ID'),\n                'name': station.get('AddressInfo', {}).get('Title'),\n                'latitude': station.get('AddressInfo', {}).get('Latitude'),\n                'longitude': station.get('AddressInfo', {}).get('Longitude'),\n                'address': station.get('AddressInfo', {}).get('AddressLine1'),\n                'status': station.get('StatusType', {}).get('Title'),\n                'number_of_points': len(station.get('Connections', [])),\n                'usage_type': station.get('UsageType', {}).get('Title')\n            }\n            stations_list.append(station_info)\n        except Exception as e:\n            print(f\"Error processing station: {e}\")\n            continue\n    \n    # Create GeoDataFrame\n    stations_df = pd.DataFrame(stations_list)\n    stations_df = stations_df.dropna(subset=['latitude', 'longitude'])\n    stations_gdf = gpd.GeoDataFrame(\n        stations_df, \n        geometry=gpd.points_from_xy(stations_df.longitude, stations_df.latitude),\n        crs=\"EPSG:4326\"\n    )\n    \n    # Filter to only Philadelphia\n    philly_stations = stations_gdf[stations_gdf.geometry.within(philly_boundary)]\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")\n    raise \n\n\n\n\nCode\n# Network and Station Map\n\n# Street network within Philadelphia\nG = ox.graph_from_polygon(philly_boundary, network_type=\"drive\")\n\n# Convert network to GeoDataFrame\nedges = ox.graph_to_gdfs(G, nodes=False)\n\n# Base map\nnetwork_station_map = folium.Map(location=[39.9526, -75.1652], zoom_start=12,\n                                tiles='CartoDB positron')\n\n# Add street network\nfolium.GeoJson(\n    edges,\n    style_function=lambda x: {'color': 'gray', 'weight': 1, 'opacity': 0.5},\n    name=\"Street Network\"\n).add_to(network_station_map)\n\n# Add station points\nfor idx, row in philly_stations.iterrows():\n    folium.CircleMarker(\n        location=[row.geometry.y, row.geometry.x],\n        radius=5,\n        color='blue',\n        fill=True,\n        popup=f\"\"\"\n            &lt;b&gt;{row['name']}&lt;/b&gt;&lt;br&gt;\n            Status: {row['status']}&lt;br&gt;\n            Number of Points: {row['number_of_points']}\n        \"\"\",\n        tooltip=row['name']\n    ).add_to(network_station_map)\n\n# Add layer control\nfolium.LayerControl().add_to(network_station_map)\n\ndisplay(network_station_map)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\n# Load and prepare Philadelphia census data\ncensus_gdf = gpd.read_file('data/phila_census1.gpkg')\n\n# Filter out tracts with zero population\ncensus_gdf = census_gdf[census_gdf['total_pop'] &gt; 0]\n\n# Filter out tracts with very low population density (&lt; 1000 people per square mile)\ncensus_gdf = census_gdf[census_gdf['pop_density'] &gt;= 1000]\n\n# Set to WGS84 \ncensus_gdf = census_gdf.to_crs('EPSG:4326')\n\nprint(f\"Loaded {len(census_gdf)} census tracts\")\n\n# Analyze demographics - Spatial join with stations\nstations_with_census = gpd.sjoin(\n    stations_gdf,\n    census_gdf,\n    how=\"left\",\n    predicate=\"within\"\n)\n\n# Clean up joined data\nstations_with_census = stations_with_census.drop(['index_right'], axis=1)\n\n# Select only needed columns\ncolumns_to_keep = [\n    'id', 'name', 'latitude', 'longitude', 'address', \n    'status', 'number_of_points', 'usage_type',\n    'GEOID', 'pct_white', 'pct_black', 'pct_hispanic', 'pct_asian', 'median_age',\n    'median_income', 'median_home_value', 'bach_degree_rate', 'pop_density', 'total_pop', 'poverty_rate',\n    'diversity_index', 'gentrification_risk', 'geometry', 'ALAND20'\n]\nstations_with_census = stations_with_census[columns_to_keep]\n\n# Analysis of stations per tract\nstations_per_tract = stations_with_census.groupby('GEOID').size().reset_index(name='station_count')\ncensus_with_stations = census_gdf.merge(stations_per_tract, on='GEOID', how='left')\ncensus_with_stations['station_count'] = census_with_stations['station_count'].fillna(0)\n\n# Create map showing EV stations distribution by census tract\nstations_per_tract_map = folium.Map(location=[39.9526, -75.1652], zoom_start=12,\n                                  tiles='CartoDB positron')\n\n# Add choropleth layer\nchoropleth = folium.Choropleth(\n    geo_data=census_with_stations,\n    name='Stations per Tract',\n    data=census_with_stations,\n    columns=['GEOID', 'station_count'],\n    key_on='feature.properties.GEOID',\n    fill_color='YlOrRd',\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name='Number of EV Stations'\n).add_to(stations_per_tract_map)\n\n# Create station layer\nstation_group = folium.FeatureGroup(name=\"EV Stations\")\n\n# Add station points\nfor idx, row in philly_stations.iterrows():\n    folium.CircleMarker(\n        location=[row.geometry.y, row.geometry.x],\n        radius=5,\n        color='blue',\n        fill=True,\n        popup=f\"\"\"\n            &lt;b&gt;{row['name']}&lt;/b&gt;&lt;br&gt;\n            Status: {row['status']}&lt;br&gt;\n            Number of Points: {row['number_of_points']}\n        \"\"\",\n        tooltip=row['name']\n    ).add_to(station_group)\n\n# Add layers to map\nstation_group.add_to(stations_per_tract_map)\nfolium.LayerControl().add_to(stations_per_tract_map)\n\n# map\ndisplay(stations_per_tract_map)\n\n# Analysis summary\nprint(\"\\nStations per tract Analysis Summary:\")\nprint(f\"Total populated census tracts: {len(census_with_stations)}\")\nprint(f\"Populated tracts with stations: {len(census_with_stations[census_with_stations['station_count'] &gt; 0])}\")\nprint(f\"Average stations per populated tract: {census_with_stations['station_count'].mean():.2f}\")\n\n\nLoaded 385 census tracts\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nStations per tract Analysis Summary:\nTotal populated census tracts: 385\nPopulated tracts with stations: 50\nAverage stations per populated tract: 0.26\n\n\n\n\nCode\nvariables = {\n    'median_income': 'Median Income ($)',\n    'poverty_rate': 'Poverty Rate (%)',\n    'pop_density': 'Population Density',\n    'total_pop': 'Total Population',\n    'bach_degree_rate': 'Bachelor\\'s Degree (%)',\n    'median_age': 'Median Age'\n}\n\n# Calculate stats for tracts with and without stations\nhas_stations = census_with_stations['station_count'] &gt; 0\nsummary_stats = pd.DataFrame({\n    'With Stations': [census_with_stations[has_stations][var].mean() for var in variables],\n    'Without Stations': [census_with_stations[~has_stations][var].mean() for var in variables]\n}, index=variables.values())\n\n# Round the results to 2 decimal places and set the table name\nsummary_stats = summary_stats.round(2)\nsummary_stats.name = \"Demographic Characteristics of Census Tracts With and Without Bike Share Stations\"\n\ndisplay(summary_stats)\n\n\n\n\n\n\n\n\n\nWith Stations\nWithout Stations\n\n\n\n\nMedian Income ($)\n82178.84\n60956.00\n\n\nPoverty Rate (%)\n18.74\n22.76\n\n\nPopulation Density\n23432.06\n20387.71\n\n\nTotal Population\n3190.46\n4271.75\n\n\nBachelor's Degree (%)\n24.84\n12.66\n\n\nMedian Age\n34.31\n36.53\n\n\n\n\n\n\n\n\n\nCharging Station Density & Demographic Correlation Analysis\nThe correlation analysis reveals important patterns in Philadelphia’s EV charging infrastructure distribution.\nStrong Positive Correlations (in order of strength): - Gentrification risk (0.389) - Bachelor’s degree rate (0.375) - Median home value (0.346)\nThese relationships suggest that EV charging stations are more concentrated in areas experiencing gentrification and with populations with higher educational attainment.\nModerate positive Correlations: - Median income (0.247) - Population density (0.204)\nThis may indicate that stations tend to be located in more affluent and densely populated areas, though these relationships are less pronounced.\nLess Notable Correlations: - The diversity index shows almost no correlation (0.047) - While poverty rate exhibits a weak negative correlation (-0.122), suggesting that station placement has not strongly considered these equity factors.\nThough the relationships aren’t extremely strong, the correlations are meaningful enough to suggest patterns about charging infrastructure in Philadelphia.\n\n\nCode\n# Calculate station density (stations per square mile)\ncensus_with_stations['station_density'] = census_with_stations['station_count'] / (census_with_stations['ALAND20'] / 2589988.11)\n\n# Define demographic variables\ndemographic_vars_display = {\n    'median_income': 'Median Income',\n    'poverty_rate': 'Poverty Rate',\n    'pop_density': 'Population Density',\n    'bach_degree_rate': 'Bachelor\\'s Degree Rate',\n    'diversity_index': 'Diversity Index',\n    'gentrification_risk': 'Gentrification Risk',\n    'median_home_value': 'Median Home Value',\n    'station_density': 'Station Density'\n}\n\n# Calculate correlations\ncorrelations = census_with_stations[demographic_vars_display.keys()].corr()['station_density'].sort_values(ascending=False)\n\n# Correlation table\ncorrelation_table = pd.DataFrame({\n    'Variable': [demographic_vars_display[var] for var in correlations.index],\n    'Correlation with Station Density': correlations.values\n})\n\ndisplay(correlation_table.style\n    .set_caption(\"Table 2: Correlation Analysis between Philadelphia EV Charging Station Density and Demographic Factors\")\n    .format(precision=3))\n\n# Summary statistics table\nsummary = census_with_stations[demographic_vars_display.keys()].describe()\nsummary.columns = [demographic_vars_display[col] for col in summary.columns]\n\ndisplay(summary.style\n    .set_caption(\"Table 3: Descriptive Statistics of Demographic and EV Charging Station Variables in Philadelphia Census Tracts\")\n    .format(precision=2))\n\n\n\n\n\n\n\nTable 1: Table 2: Correlation Analysis between Philadelphia EV Charging Station Density and Demographic Factors\n\n\n\n\n\n \nVariable\nCorrelation with Station Density\n\n\n\n\n0\nStation Density\n1.000\n\n\n1\nGentrification Risk\n0.389\n\n\n2\nBachelor's Degree Rate\n0.370\n\n\n3\nMedian Home Value\n0.350\n\n\n4\nMedian Income\n0.249\n\n\n5\nPopulation Density\n0.211\n\n\n6\nDiversity Index\n0.057\n\n\n7\nPoverty Rate\n-0.122\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Table 3: Descriptive Statistics of Demographic and EV Charging Station Variables in Philadelphia Census Tracts\n\n\n\n\n\n \nMedian Income\nPoverty Rate\nPopulation Density\nBachelor's Degree Rate\nDiversity Index\nGentrification Risk\nMedian Home Value\nStation Density\n\n\n\n\ncount\n380.00\n385.00\n385.00\n385.00\n385.00\n373.00\n373.00\n385.00\n\n\nmean\n63748.48\n22.24\n20783.08\n14.24\n0.42\n0.22\n261076.94\n2.12\n\n\nstd\n31723.12\n14.27\n12166.68\n10.26\n0.19\n0.08\n171054.63\n8.55\n\n\nmin\n14983.00\n0.72\n1091.34\n0.00\n0.02\n0.05\n46700.00\n0.00\n\n\n25%\n39601.75\n11.14\n12014.76\n6.20\n0.27\n0.16\n131500.00\n0.00\n\n\n50%\n58432.00\n19.69\n18781.08\n11.76\n0.43\n0.20\n222300.00\n0.00\n\n\n75%\n82571.00\n31.18\n27290.24\n20.72\n0.59\n0.26\n321000.00\n0.00\n\n\nmax\n181066.00\n78.18\n92575.12\n44.25\n0.77\n0.53\n1036700.00\n94.71\n\n\n\n\n\n\n\n\nThe descriptive statistics provide additional context about Philadelphia’s socioeconomic landscape.\n\nMedian income varies substantially across census tracts, ranging from $14,983 to $181,066 (mean: $63,748), highlighting significant economic disparities.\nPoverty rates average 22.24% but reach as high as 78.18% in some tracts, underscoring areas of concentrated poverty.\nPopulation density shows considerable variation (mean: 20,783 people per square mile, min: 1,091, max: 92,575), reflecting Philadelphia’s mix of dense urban cores and less populated areas\nThe bachelor’s degree rate averages 14.24% but ranges from 0% to 44.25%, indicating educational attainment disparities.\n\nNotably, EV charging station density exhibits highly skewed distribution patterns.\n\nWhile the mean density is 2.12 stations per tract, the median of 0 and maximum of 94.71 reveals that stations are heavily concentrated in select areas, with many tracts having no stations at all. This distribution, combined with the correlation patterns, suggests that current EV infrastructure deployment may be reinforcing existing socioeconomic disparities rather than addressing them."
  },
  {
    "objectID": "analysis/3-Results.html",
    "href": "analysis/3-Results.html",
    "title": "Results",
    "section": "",
    "text": "As defined in Methods, each tract’s Gap Score (0–1) is computed and then bucketed into four priority tiers using fixed thresholds.\n\n\n\n\n\n\n\nDistribution of Gap Scores with Fixed Thresholds\n\n\n\n\nFigure 1: Figure 1: Distribution of Gap Scores with Fixed Thresholds\n\n\n\n\nMean gap score: 0.520 (σ = 0.108)\n\n25th–75th percentiles: 0.432–0.604\n\nPriority distribution:\n\nLow Priority (≤ 0.45): 117 tracts (30.4%)\n\nMedium Priority (0.45–0.55): 106 tracts (27.5%)\n\nHigh Priority (0.55–0.65): 111 tracts (28.8%)\n\nCritical Priority (&gt; 0.65): 46 tracts (11.9%)"
  },
  {
    "objectID": "analysis/3-Results.html#gap-analysis-findings-philadelphia-ev-charging-infrastructure",
    "href": "analysis/3-Results.html#gap-analysis-findings-philadelphia-ev-charging-infrastructure",
    "title": "Results",
    "section": "",
    "text": "As defined in Methods, each tract’s Gap Score (0–1) is computed and then bucketed into four priority tiers using fixed thresholds.\n\n\n\n\n\n\n\nDistribution of Gap Scores with Fixed Thresholds\n\n\n\n\nFigure 1: Figure 1: Distribution of Gap Scores with Fixed Thresholds\n\n\n\n\nMean gap score: 0.520 (σ = 0.108)\n\n25th–75th percentiles: 0.432–0.604\n\nPriority distribution:\n\nLow Priority (≤ 0.45): 117 tracts (30.4%)\n\nMedium Priority (0.45–0.55): 106 tracts (27.5%)\n\nHigh Priority (0.55–0.65): 111 tracts (28.8%)\n\nCritical Priority (&gt; 0.65): 46 tracts (11.9%)"
  },
  {
    "objectID": "analysis/3-Results.html#equity-analysis",
    "href": "analysis/3-Results.html#equity-analysis",
    "title": "Results",
    "section": "Equity Analysis",
    "text": "Equity Analysis\n\n“Served” and “Unserved” tracts are defined per Methods (coverage_score &gt; mean_coverage).\n\n\n\n\n\n\n\n\n\n\n\nMetric\nServed Areas\nUnserved Areas\n% Difference\nSignificant?\n\n\n\n\nMedian Income\n$61,173\n$64,745\n– 5.5%\nNo\n\n\nPoverty Rate\n20.4%\n22.9%\n– 11.0%\nNo\n\n\nPopulation Density\n18,941 p/mi²\n21,502 p/mi²\n– 11.9%\nNo\n\n\nBachelor’s Degree %\n11.5%\n15.3%\n– 25.0%\nYes\n\n\n\n\nBachelor’s Degree % shows the largest and only statistically significant difference, with higher education levels in unserved areas.\nOther socioeconomic factors show moderate differences but are not statistically significant."
  },
  {
    "objectID": "analysis/3-Results.html#demographic-coverage",
    "href": "analysis/3-Results.html#demographic-coverage",
    "title": "Results",
    "section": "Demographic Coverage",
    "text": "Demographic Coverage\n\n\n\n\n\n\nService Coverage by Demographic Group\n\n\n\n\nFigure 2: Figure 2: Service Coverage by Demographic Group\n\n\n\n\nIncome Groups\n\nMedium-income: 49.2% covered\n\nHigh-income: 31.1% covered\n\nLow-income: 16.8% covered\n\nEducation Levels\n\nMedium-education: 42.7% covered\n\nLow-education: 32.3% covered\n\nHigh-education: 22.0% covered\n\nPopulation Density\n\nLow-density: 42.4% covered\n\nMedium-density: 29.1% covered\n\nHigh-density: 29.6% covered\n\n\nPolicy Insight: Low-income areas have significantly worse EV charging access (only 16.8% covered), suggesting a potential socioeconomic inequity that prioritization should address."
  },
  {
    "objectID": "analysis/3-Results.html#component-contributions",
    "href": "analysis/3-Results.html#component-contributions",
    "title": "Results",
    "section": "Component Contributions",
    "text": "Component Contributions\nThe gap scores were computed using dynamically adjusted weights based on observed disparities:\n\n\n\n\n\n\n\n\nComponent\nFinal Weight\nRationale\n\n\n\n\nCoverage\n24%\nPhysical access to existing chargers\n\n\nIncome\n16%\nEconomic vulnerability (no significant disparity)\n\n\nPoverty\n24%\nEconomic need (medium effect size, increased weight)\n\n\nDensity\n20%\nPopulation concentration (medium-large effect, increased)\n\n\nEducation\n16%\nTechnology adoption (small effect, increased weight)\n\n\n\nDynamic weight adjustments reflect the significant effect sizes observed for poverty (0.64), population density (0.75), and education (0.31), with all three showing statistically significant disparities (p &lt; 0.05).\n\n\n\n\n\n\n\nNote\n\n\n\nObservation: The adjusted thresholds create a balanced distribution of priority categories, with approximately equal proportions in Low (30%), Medium (28%), and High (29%) priority tiers, while maintaining a focused Critical Priority group (12%). This distribution allows for targeted interventions across different need levels.\n\n\n\n\n\nCategory\n# Tracts\nPercentage\n\n\n\n\nLow Priority\n117\n30.4%\n\n\nMedium Priority\n106\n27.5%\n\n\nHigh Priority\n111\n28.8%\n\n\nCritical Priority\n46\n11.9%"
  },
  {
    "objectID": "analysis/3-Results.html#why-some-tracts-with-stations-still-rank-high-or-critical",
    "href": "analysis/3-Results.html#why-some-tracts-with-stations-still-rank-high-or-critical",
    "title": "Results",
    "section": "Why Some Tracts with Stations Still Rank High or Critical",
    "text": "Why Some Tracts with Stations Still Rank High or Critical\nEven tracts that host chargers can remain in the High or Critical tiers due to:\n\nEquity Weights → Our model places significant weight on socioeconomic factors (76% combined) versus pure coverage (24%), meaning areas with high poverty rates (24%) or population density (20%) receive priority regardless of physical access.\nStation Clustering → The map reveals significant station clustering in Center City and University City, while many high-priority neighborhoods have only isolated stations that cannot adequately serve the entire tract.\nQuality Disparities → Existing stations in underserved areas often have fewer charging points or lower power capacity than those in affluent areas."
  },
  {
    "objectID": "analysis/3-Results.html#spatial-patterns-in-the-ev-charging-gap-map",
    "href": "analysis/3-Results.html#spatial-patterns-in-the-ev-charging-gap-map",
    "title": "Results",
    "section": "Spatial Patterns in the EV Charging Gap Map",
    "text": "Spatial Patterns in the EV Charging Gap Map\nThe interactive map reveals several important spatial patterns:\n\nDowntown Concentration → EV charging infrastructure is heavily concentrated in Center City, with dense clusters of stations serving primarily commercial and high-income residential areas.\nNorth-South Divide → North and West Philadelphia show extensive areas of High and Critical priority (red), while Northeast Philadelphia generally shows lower priority needs.\nTransit Corridors → Medium priority areas (orange) often follow major transit corridors like Broad Street and Market Street, where access may be better but still insufficient given population density.\nSuburban Edge Contrast → The city’s edges show a contrast between lower priority areas in the Northeast versus higher priority areas in the Northwest and Southwest, reflecting socioeconomic differences between these regions.\n\nPolicy Recommendation: New station investments should target the Critical priority areas in North and West Philadelphia where high population density combines with significant socioeconomic need."
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "This section presents a comprehensive analysis of Electric Vehicle (EV) charging infrastructure in Philadelphia, with a focus on identifying service gaps and prioritizing areas for future investment.\n\n\nThe work leverages geospatial techniques, network analysis, and equity-focused metrics to assess the current distribution of EV charging stations across Philadelphia. Through this analysis, the study:\n\nMaps current infrastructure - Visualizes the spatial distribution of 130 EV charging stations across Philadelphia\nAssesses accessibility - Calculates walking and driving distances to nearest chargers for all census tracts\nIdentifies service gaps - Develops a robust gap scoring methodology combining coverage metrics with socioeconomic factors\nPrioritizes interventions - Classifies census tracts into four priority tiers: Low, Medium, High, and Critical\n\n\n\n\nThe analysis employs multiple technical approaches: - Network-based distance calculations rather than simple buffers - Dynamic weighting system that responds to observed disparities - Consideration of station quality and capacity in service assessment - Integration of demographic factors including income, poverty, population density, and education\n\n\n\nThroughout these analyses, interactive visualizations allow for in-depth data exploration. Readers can examine specific census tracts, view detailed station information, and understand the relationships between different factors contributing to service gaps.\nThe following pages demonstrate the step-by-step analytical process, from data preparation to service gap identification. While specific policy recommendations are not the primary focus, the priority classification system provides a clear framework for decision-makers to target future EV charging infrastructure investments.",
    "crumbs": [
      "Analysis",
      "Overview"
    ]
  },
  {
    "objectID": "analysis/index.html#analysis-overview",
    "href": "analysis/index.html#analysis-overview",
    "title": "Analysis",
    "section": "",
    "text": "The work leverages geospatial techniques, network analysis, and equity-focused metrics to assess the current distribution of EV charging stations across Philadelphia. Through this analysis, the study:\n\nMaps current infrastructure - Visualizes the spatial distribution of 130 EV charging stations across Philadelphia\nAssesses accessibility - Calculates walking and driving distances to nearest chargers for all census tracts\nIdentifies service gaps - Develops a robust gap scoring methodology combining coverage metrics with socioeconomic factors\nPrioritizes interventions - Classifies census tracts into four priority tiers: Low, Medium, High, and Critical",
    "crumbs": [
      "Analysis",
      "Overview"
    ]
  },
  {
    "objectID": "analysis/index.html#methodology-highlights",
    "href": "analysis/index.html#methodology-highlights",
    "title": "Analysis",
    "section": "",
    "text": "The analysis employs multiple technical approaches: - Network-based distance calculations rather than simple buffers - Dynamic weighting system that responds to observed disparities - Consideration of station quality and capacity in service assessment - Integration of demographic factors including income, poverty, population density, and education",
    "crumbs": [
      "Analysis",
      "Overview"
    ]
  },
  {
    "objectID": "analysis/index.html#interactive-elements",
    "href": "analysis/index.html#interactive-elements",
    "title": "Analysis",
    "section": "",
    "text": "Throughout these analyses, interactive visualizations allow for in-depth data exploration. Readers can examine specific census tracts, view detailed station information, and understand the relationships between different factors contributing to service gaps.\nThe following pages demonstrate the step-by-step analytical process, from data preparation to service gap identification. While specific policy recommendations are not the primary focus, the priority classification system provides a clear framework for decision-makers to target future EV charging infrastructure investments.",
    "crumbs": [
      "Analysis",
      "Overview"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Insights and Recommendations",
    "section": "",
    "text": "In conclusion, my analysis has provided a comprehensive overview of the EV charging station landscape in Philadelphia, revealing significant disparities in infrastructure distribution and access.",
    "crumbs": [
      "Insights and Recommendations"
    ]
  },
  {
    "objectID": "conclusions.html#key-insights",
    "href": "conclusions.html#key-insights",
    "title": "Insights and Recommendations",
    "section": "Key Insights",
    "text": "Key Insights\n\nSpatial Distribution Inequity\n\nOnly 13.0% of residential census tracts (50 out of 385) contain charging stations\nStrong concentration in Center City and University City areas\nSignificant North-South divide in infrastructure access\n\nDemographic Patterns\n\nClear education disparity (strongest statistically significant effect)\nLower coverage in high-density neighborhoods (42.4% for low density vs 29.6% for high density)\nMedium income areas have highest coverage (49.2%) compared to high (31.1%) and low income areas (16.8%)\n\nGap Analysis Findings\n\nMean gap score of 0.52 with standard deviation of 0.11\nBalanced distribution across priority categories using the new thresholds [0.45, 0.55, 0.65]\n11.9% of tracts fall into critical priority category\nStrong clusters of high-need areas in North and West Philadelphia",
    "crumbs": [
      "Insights and Recommendations"
    ]
  },
  {
    "objectID": "conclusions.html#recommendations",
    "href": "conclusions.html#recommendations",
    "title": "Insights and Recommendations",
    "section": "Recommendations",
    "text": "Recommendations\n\nPriority Area Development\n\nTarget North and West Philadelphia critical priority tracts (gap scores &gt; 0.65)\nFocus on high-density areas which are particularly underserved\nPrioritize locations that improve equity across demographic groups\n\nEquity-Focused Implementation\n\nDevelop sliding scale pricing based on neighborhood demographics\nPartner with community organizations in high-priority areas\nCreate programs to ensure accessibility in lower-income areas\n\nStrategic Infrastructure Planning\n\nAddress station clustering by promoting more dispersed distribution\nConsider existing station quality and capacity in planning\nPlan for multi-modal accessibility (both walking and driving)",
    "crumbs": [
      "Insights and Recommendations"
    ]
  },
  {
    "objectID": "conclusions.html#implementation-challenges-and-limitations",
    "href": "conclusions.html#implementation-challenges-and-limitations",
    "title": "Insights and Recommendations",
    "section": "Implementation Challenges and Limitations",
    "text": "Implementation Challenges and Limitations\n\nData Limitations\n\nCensus data from 2020 may not reflect current demographics\nEV charging station data may not include recent installations\nLimited data on station capacity, usage patterns, and maintenance status\n\n\n\nMethodological Considerations\n\nNetwork analysis limited by connectivity challenges in some areas\nDynamic weighting system reflects current patterns but may need adjustment\nEdge effects at city boundaries may affect analysis accuracy\n\n\n\nModel Constraints\n\nGap score components weighted based on observed disparities (Coverage: 24%, Income: 16%, Poverty: 24%, Density: 20%, Education: 16%)\n23 tracts excluded from the original 408 Philadelphia tracts\nLand use and zoning restrictions not incorporated in analysis",
    "crumbs": [
      "Insights and Recommendations"
    ]
  },
  {
    "objectID": "conclusions.html#summary-statistics",
    "href": "conclusions.html#summary-statistics",
    "title": "Insights and Recommendations",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\n\nMetric\nValue\nContext\n\n\n\n\nTotal Stations\n130\nFrom 150 API results\n\n\nAnalyzed Tracts\n385\nOf 408 total tracts\n\n\nMean Gap Score\n0.52\nScale 0-1\n\n\nLow Priority Tracts\n30.4%\nScore ≤ 0.45\n\n\nMedium Priority Tracts\n27.5%\nScore 0.45-0.55\n\n\nHigh Priority Tracts\n28.8%\nScore 0.55-0.65\n\n\nCritical Priority Tracts\n11.9%\nScore &gt; 0.65\n\n\nPopulation Coverage\n31.2%\nWithin service areas",
    "crumbs": [
      "Insights and Recommendations"
    ]
  },
  {
    "objectID": "conclusions.html#addressing-the-research-questions",
    "href": "conclusions.html#addressing-the-research-questions",
    "title": "Insights and Recommendations",
    "section": "Addressing the Research Questions",
    "text": "Addressing the Research Questions\nThis analysis set out to answer several key questions about Philadelphia’s EV charging infrastructure:\n\n1. How accessible are EV charging stations based on street network analysis?\nThe network analysis reveals that only 31.2% of Philadelphia residents live within walking distance (0.75 miles) of an EV charging station. The accessibility is highly uneven, with Center City and University City enjoying excellent coverage while North and West Philadelphia face significant accessibility gaps.\n\n\n2. What patterns exist in the distribution of charging stations relative to population density and income levels?\nThe analysis identified clear patterns in charging station distribution: - Density relationship: Counterintuitively, high-density areas have the lowest coverage (29.6%) - Income relationship: Medium-income areas have the highest coverage (49.2%), while low-income areas have the poorest access (16.8%) - Education correlation: Areas with higher education levels show statistically significant better access to charging infrastructure\n\n\n3. Are there significant gaps in charging infrastructure coverage within urban areas?\nSignificant gaps exist throughout Philadelphia, with 11.9% of census tracts classified as Critical Priority areas requiring immediate infrastructure investment. These gaps are particularly pronounced in residential neighborhoods outside the city center and in areas with lower-than-average education levels.",
    "crumbs": [
      "Insights and Recommendations"
    ]
  }
]