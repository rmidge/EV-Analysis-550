{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b2a549-1b0b-405b-a536-36a45434eb9e",
   "metadata": {},
   "source": [
    "---\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    page-layout: full\n",
    "execute:\n",
    "  echo: true    # Shows code\n",
    "  eval: true    # Runs code\n",
    "  output: false # Should hide standard outputs\n",
    "  warning: false\n",
    "  error: false\n",
    "  message: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481530d3-4256-4271-b03a-97e86e37ea74",
   "metadata": {},
   "source": [
    "# Network Service Gap Methods & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fd1878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\rache\\Documents\\musa 550 FINAL\\quarto_musa_550\\analysis\n",
      "OSMnx cache folder: data\\cache\\osmnx\n",
      "Using NetworkX with point snapping for network analysis.\n",
      "Using NetworkX for network analysis with point snapping\n",
      "\n",
      "=== PREPROCESSING POINTS FOR NETWORK ANALYSIS ===\n",
      "Loading snapped points from cache: stations...\n",
      "Loaded 130 snapped points from cache.\n",
      "Using 130 snapped stations for network analysis\n",
      "Loading snapped points from cache: centroids...\n",
      "Loaded 385 snapped points from cache.\n",
      "Using 385 snapped census tract centroids for network analysis\n",
      "Preprocessing complete. Points are now properly connected to the network.\n",
      "Loading distances from cache...\n",
      "Loaded 50050 walking and 50050 driving distances from cache.\n",
      "Loading service areas from cache...\n",
      "Loaded 5 service areas from cache\n",
      "Loading equity analysis from cache...\n",
      "Loaded equity analysis for 5 service areas from cache\n",
      "Original weights: {'coverage': 0.3, 'income': 0.2, 'poverty': 0.30000000000000004, 'density': 0.25, 'education': 0.2}\n",
      "Effect sizes: {'income': 0.12465946575488303, 'poverty': 0.6365812002143018, 'density': 0.7496382501362957, 'education': 0.30932633445014873}\n",
      "Significant effects: {'income': False, 'poverty': True, 'density': True, 'education': True}\n",
      "Normalized weights: {'coverage': 0.24, 'income': 0.16, 'poverty': 0.24000000000000005, 'density': 0.2, 'education': 0.16}\n",
      "Fixed thresholds: [0.45, 0.55, 0.65]\n",
      "\n",
      "=== GAP SCORE COMPONENTS ===\n",
      "Coverage component (weight: 0.24):\n",
      "count    385.000000\n",
      "mean       0.188283\n",
      "std        0.037668\n",
      "min        0.000000\n",
      "25%        0.166852\n",
      "50%        0.203798\n",
      "75%        0.208540\n",
      "max        0.240000\n",
      "Name: coverage_score, dtype: float64\n",
      "\n",
      "Income component (weight: 0.16):\n",
      "count    380.000000\n",
      "mean       0.113021\n",
      "std        0.030561\n",
      "min        0.000000\n",
      "25%        0.094887\n",
      "50%        0.118142\n",
      "75%        0.136283\n",
      "max        0.160000\n",
      "Name: median_income, dtype: float64\n",
      "\n",
      "Poverty component (weight: 0.24):\n",
      "count    385.000000\n",
      "mean       0.066670\n",
      "std        0.044222\n",
      "min        0.000000\n",
      "25%        0.032301\n",
      "50%        0.058785\n",
      "75%        0.094378\n",
      "max        0.240000\n",
      "Name: poverty_rate, dtype: float64\n",
      "\n",
      "Density component (weight: 0.20):\n",
      "count    385.000000\n",
      "mean       0.043050\n",
      "std        0.026599\n",
      "min        0.000000\n",
      "25%        0.023881\n",
      "50%        0.038673\n",
      "75%        0.057276\n",
      "max        0.200000\n",
      "Name: pop_density, dtype: float64\n",
      "\n",
      "Education component (weight: 0.16):\n",
      "count    385.000000\n",
      "mean       0.108519\n",
      "std        0.037093\n",
      "min        0.000000\n",
      "25%        0.085061\n",
      "50%        0.117489\n",
      "75%        0.137574\n",
      "max        0.160000\n",
      "Name: bach_degree_rate, dtype: float64\n",
      "\n",
      "=== GAP SCORE STATISTICS ===\n",
      "count    380.000000\n",
      "mean       0.519702\n",
      "std        0.108200\n",
      "min        0.242863\n",
      "25%        0.431873\n",
      "50%        0.520160\n",
      "75%        0.604442\n",
      "max        0.800245\n",
      "Name: gap_score, dtype: float64\n",
      "\n",
      "=== PRIORITY CATEGORY DISTRIBUTION ===\n",
      "Low Priority: 117 tracts (30.4%)\n",
      "Medium Priority: 106 tracts (27.5%)\n",
      "High Priority: 111 tracts (28.8%)\n",
      "Critical Priority: 46 tracts (11.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_23112\\409186455.py:1010: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  'pop_density': float(tracts['total_pop'].sum() / (tracts.geometry.area.sum() / (5280 * 5280))),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created histogram: data/visualizations/gap_score_distribution.png\n",
      "Equity results structure: dict_keys(['walk_400m', 'walk_800m', 'drive_1000m', 'drive_3000m', 'drive_8000m'])\n",
      "First buffer sample: {'median_income': {'effect_size': -0.12465946575488303, 'ci_lower': -6684.376767083484, 'ci_upper': 6684.127448151974, 'p_value': 0.24716139952343424}, 'poverty_rate': {'effect_size': 0.6365812002143018, 'ci_lower': -2.2308921974977856, 'ci_upper': 3.504054597926389, 'p_value': 6.057475773524907e-09}, 'pop_density': {'effect_size': 0.7496382501362957, 'ci_lower': -2404.4217684494174, 'ci_upper': 2405.92104494969, 'p_value': 1.105167556086776e-11}, 'bach_degree_rate': {'effect_size': 0.30932633445014873, 'ci_lower': -1.821517594859319, 'ci_upper': 2.4401702637596165, 'p_value': 0.00405668195382175}}\n",
      "Equity DataFrame shape: (20, 6)\n",
      "      buffer       variable  effect_size     ci_lower     ci_upper  \\\n",
      "0  walk_400m  median_income    -0.124659 -6684.376767  6684.127448   \n",
      "1  walk_400m   poverty_rate     0.636581    -2.230892     3.504055   \n",
      "\n",
      "        p_value  \n",
      "0  2.471614e-01  \n",
      "1  6.057476e-09  \n",
      "Successfully created equity CSV: data/visualizations/demographic_equity_summary.csv\n",
      "\n",
      "Demographic coverage data:\n",
      "Keys: dict_keys(['income_category', 'education_category', 'density_category'])\n",
      "  income_category: 4 categories\n",
      "    high_income: {'total_population': 455526, 'covered_population': 141652, 'coverage_percent': 31.096358934506483}\n",
      "    medium_income: {'total_population': 587637, 'covered_population': 288962, 'coverage_percent': 49.17355442220282}\n",
      "    low_income: {'total_population': 537747, 'covered_population': 90287, 'coverage_percent': 16.78986586629028}\n",
      "    nan: {'total_population': 0, 'covered_population': 0, 'coverage_percent': 0}\n",
      "  education_category: 3 categories\n",
      "    high_education: {'total_population': 443757, 'covered_population': 97725, 'coverage_percent': 22.02218781900905}\n",
      "    medium_education: {'total_population': 548675, 'covered_population': 234217, 'coverage_percent': 42.68774775595753}\n",
      "    low_education: {'total_population': 598128, 'covered_population': 193367, 'coverage_percent': 32.32869887381965}\n",
      "  density_category: 3 categories\n",
      "    low_density: {'total_population': 452288, 'covered_population': 191582, 'coverage_percent': 42.35840880147163}\n",
      "    medium_density: {'total_population': 566156, 'covered_population': 164581, 'coverage_percent': 29.06990299493426}\n",
      "    high_density: {'total_population': 572116, 'covered_population': 169146, 'coverage_percent': 29.564983325059952}\n",
      "\n",
      "Plot DataFrame shape: (9, 3)\n",
      "    category   level   coverage\n",
      "0    Density     Low  42.358409\n",
      "1    Density  Medium  29.069903\n",
      "2    Density    High  29.564983\n",
      "3  Education     Low  32.328699\n",
      "4  Education  Medium  42.687748\n",
      "Successfully created bar chart: data/visualizations/demographic_coverage.png\n",
      "Successfully created interactive map: data/visualizations/priority_areas_map.html\n",
      "\n",
      "=== EQUITY ANALYSIS SUMMARY ===\n",
      "\n",
      "Validation Statistics:\n",
      "- Total stations from API: 150\n",
      "- Stations within boundary: 130\n",
      "- Excluded stations: 20\n",
      "  - Outside boundary: 150\n",
      "  - Missing coordinates: 0\n",
      "\n",
      "Service Areas:\n",
      "- walk_400m: 251 tracts (65.2%)\n",
      "- walk_800m: 251 tracts (65.2%)\n",
      "- drive_1000m: 251 tracts (65.2%)\n",
      "- drive_3000m: 251 tracts (65.2%)\n",
      "- drive_8000m: 251 tracts (65.2%)\n",
      "\n",
      "Served vs. Unserved Areas:\n",
      "Metric                     Served     Unserved Percent Diff Significant\n",
      "----------------------------------------------------------------------\n",
      "median_income             61172.9      64744.9         -5.5%         No\n",
      "poverty_rate                 20.4         22.9        -11.0%         No\n",
      "pop_density               18940.5      21501.5        -11.9%         No\n",
      "bach_degree_rate             11.5         15.3        -25.0%        Yes\n",
      "\n",
      "=== DYNAMIC WEIGHTS ===\n",
      "coverage: 0.2400 (24.0%)\n",
      "income: 0.1600 (16.0%)\n",
      "poverty: 0.2400 (24.0%)\n",
      "density: 0.2000 (20.0%)\n",
      "education: 0.1600 (16.0%)\n",
      "Complete gap analysis pipeline.\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "import geopy.distance\n",
    "import requests \n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point, LineString, MultiPoint\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from datetime import datetime\n",
    "import folium\n",
    "from shapely.geometry import mapping as shapely_mapping\n",
    "\n",
    "# Print current working directory\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Set OSMnx cache location explicitly\n",
    "ox.settings.cache_folder = os.path.join('data', 'cache', 'osmnx')\n",
    "print(f\"OSMnx cache folder: {ox.settings.cache_folder}\")\n",
    "\n",
    "# For our cache\n",
    "CACHE_DIR = os.path.join('data', 'cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Set global debug mode (set to True for detailed component outputs)\n",
    "DEBUG_MODE = True\n",
    "\n",
    "# Define global thresholds for gap scoring - used in multiple functions\n",
    "THRESHOLDS = [0.45, 0.55, 0.65]  # Adjusted thresholds for better distribution\n",
    "\n",
    "# Using NetworkX with point snapping\n",
    "\n",
    "print(\"Using NetworkX with point snapping for network analysis.\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load census, boundary, and EV station data.\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('OPENCHARGE_API_KEY')\n",
    "    if not api_key:\n",
    "        raise EnvironmentError('OPENCHARGE_API_KEY is not set.')\n",
    "\n",
    "    # Census data\n",
    "    census = gpd.read_file('data/phila_census1.gpkg')\n",
    "    census = census[census['total_pop'] > 0]\n",
    "    census = census[census['pop_density'] >= 1000]\n",
    "    census = census.to_crs('EPSG:4326')\n",
    "\n",
    "    # City boundary\n",
    "    boundary = gpd.read_file('data/City_Limits.geojson').to_crs('EPSG:4326')\n",
    "\n",
    "    # EV stations\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'countrycode': 'US',\n",
    "        'maxresults': 1000,\n",
    "        'latitude': 39.9526,\n",
    "        'longitude': -75.1652,\n",
    "        'distance': 10,\n",
    "        'distanceunit': 'km',\n",
    "        'compact': True,\n",
    "        'verbose': False,\n",
    "        'output': 'json'\n",
    "    }\n",
    "    resp = requests.get('https://api.openchargemap.io/v3/poi', params=params, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    records = []\n",
    "    for s in data:\n",
    "        lat = s.get('AddressInfo',{}).get('Latitude')\n",
    "        lon = s.get('AddressInfo',{}).get('Longitude')\n",
    "        if lat is None or lon is None:\n",
    "            continue\n",
    "        pt = Point(lon, lat)\n",
    "        if not pt.within(boundary.geometry.iloc[0]):\n",
    "            continue\n",
    "        conns = s.get('Connections', [])\n",
    "        max_kw = max((c.get('PowerKW',0) for c in conns), default=0)\n",
    "        records.append({\n",
    "            'id': s.get('ID'),\n",
    "            'name': s.get('AddressInfo',{}).get('Title',''),\n",
    "            'num_points': len(conns),\n",
    "            'max_power': max_kw,\n",
    "            'geometry': pt\n",
    "        })\n",
    "    stations = gpd.GeoDataFrame(records, crs='EPSG:4326')\n",
    "\n",
    "    # Project to state plane for spatial operations\n",
    "    census = census.to_crs('EPSG:2272')\n",
    "    boundary = boundary.to_crs('EPSG:2272')\n",
    "    stations = stations.to_crs('EPSG:2272')\n",
    "\n",
    "    # Add charging speed categories\n",
    "    stations['charging_speed'] = pd.cut(\n",
    "        stations['max_power'],\n",
    "        bins=[-np.inf, 7, 50, np.inf],\n",
    "        labels=['Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)']\n",
    "    )\n",
    "\n",
    "    # Validation tracking\n",
    "    validation = {\n",
    "        'total_stations_from_api': len(data),\n",
    "        'stations_within_boundary': len(records),\n",
    "        'excluded_stations': len(data) - len(records),\n",
    "        'exclusion_reasons': {\n",
    "            'outside_boundary': sum(1 for s in data if s.get('AddressInfo',{}).get('Latitude') and\n",
    "                                   s.get('AddressInfo',{}).get('Longitude') and\n",
    "                                   not Point(s.get('AddressInfo',{}).get('Longitude'), \n",
    "                                            s.get('AddressInfo',{}).get('Latitude')).within(boundary.geometry.iloc[0])),\n",
    "            'missing_coordinates': sum(1 for s in data if not s.get('AddressInfo',{}).get('Latitude') or\n",
    "                                      not s.get('AddressInfo',{}).get('Longitude'))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return census, boundary, stations, validation\n",
    "\n",
    "\n",
    "def create_networks():\n",
    "    \"\"\"Load or build walk and drive networks and cache them.\"\"\"\n",
    "    cache = 'data/network_cache.pkl'\n",
    "    if os.path.exists(cache):\n",
    "        with open(cache,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    G_walk = ox.graph_from_place('Philadelphia, Pennsylvania', network_type='walk', simplify=False)\n",
    "    G_walk = ox.project_graph(G_walk, to_crs='EPSG:2272')\n",
    "    G_walk = ox.distance.add_edge_lengths(G_walk)\n",
    "\n",
    "    G_drive = ox.graph_from_place('Philadelphia, Pennsylvania', network_type='drive', simplify=False)\n",
    "    G_drive = ox.project_graph(G_drive, to_crs='EPSG:2272')\n",
    "    G_drive = ox.distance.add_edge_lengths(G_drive)\n",
    "\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    with open(cache,'wb') as f:\n",
    "        pickle.dump((G_walk, G_drive),f)\n",
    "\n",
    "    def check_network_connectivity(network, network_type):\n",
    "        \"\"\"Check and report on network connectivity issues.\"\"\"\n",
    "        # Find connected components\n",
    "        connected_components = list(nx.connected_components(network.to_undirected()))\n",
    "        print(f\"{network_type} network has {len(connected_components)} connected components\")\n",
    "        \n",
    "        # Report on largest component\n",
    "        largest_component = max(connected_components, key=len)\n",
    "        largest_pct = len(largest_component) / network.number_of_nodes() * 100\n",
    "        print(f\"Largest component contains {largest_pct:.1f}% of all nodes\")\n",
    "        \n",
    "        if len(connected_components) > 1:\n",
    "            print(\"WARNING: Network has disconnected components, which may cause routing failures\")\n",
    "        \n",
    "        return len(connected_components)\n",
    "\n",
    "    # Add after creating networks:\n",
    "    print(\"Checking network connectivity...\")\n",
    "    walk_components = check_network_connectivity(G_walk, \"Walking\")\n",
    "    drive_components = check_network_connectivity(G_drive, \"Driving\")\n",
    "\n",
    "    return G_walk, G_drive\n",
    "\n",
    "\n",
    "def find_nearest_node(network, point_x, point_y, fallback_tolerance=5000):\n",
    "    \"\"\"\n",
    "    Find the nearest node to a point using robust search methods.\n",
    "    Tries OSMnx nearest_nodes first, then falls back to manual distance calculation\n",
    "    with an increasing search radius if needed.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    network : networkx.Graph\n",
    "        Road network from OSMnx\n",
    "    point_x, point_y : float\n",
    "        Coordinates of the target point\n",
    "    fallback_tolerance : float\n",
    "        Maximum distance to search for nodes if nearest_nodes fails\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int or None\n",
    "        Node ID of nearest node, or None if no node found within tolerance\n",
    "    \"\"\"\n",
    "    # First try with OSMnx's built-in function\n",
    "    try:\n",
    "        return ox.nearest_nodes(network, point_x, point_y)\n",
    "    except Exception as e:\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"Standard nearest_nodes failed: {e}, trying manual search...\")\n",
    "    \n",
    "    # Manual fallback with increasing radius\n",
    "    try:\n",
    "        # Get all nodes and their coordinates\n",
    "        nodes = list(network.nodes)\n",
    "        if not nodes:\n",
    "            if DEBUG_MODE:\n",
    "                print(\"No nodes in network!\")\n",
    "            return None\n",
    "            \n",
    "        # Use vectorized operations for efficiency\n",
    "        node_coords = np.array([\n",
    "            [network.nodes[n].get('x', 0), network.nodes[n].get('y', 0)] \n",
    "            for n in nodes\n",
    "        ])\n",
    "        \n",
    "        point_coords = np.array([point_x, point_y])\n",
    "        \n",
    "        # Calculate squared distances to all nodes\n",
    "        squared_distances = np.sum((node_coords - point_coords)**2, axis=1)\n",
    "        \n",
    "        # Get the node with minimum distance\n",
    "        min_idx = np.argmin(squared_distances)\n",
    "        min_distance = np.sqrt(squared_distances[min_idx])\n",
    "        \n",
    "        if min_distance <= fallback_tolerance:\n",
    "            if DEBUG_MODE and min_distance > 100:\n",
    "                print(f\"Found node at distance {min_distance:.1f} units\")\n",
    "            return nodes[min_idx]\n",
    "        else:\n",
    "            if DEBUG_MODE:\n",
    "                print(f\"Nearest node is too far: {min_distance:.1f} units > {fallback_tolerance}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"Manual nearest node search failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def map_match_point(network, point_x, point_y, tolerance=5000):\n",
    "    \"\"\"\n",
    "    Map match a point to the nearest edge in the network and return\n",
    "    the nearest node along that edge.\n",
    "    \n",
    "    This provides better connectivity than just finding the nearest node,\n",
    "    especially when points are far from actual network nodes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    network : networkx.Graph\n",
    "        Road network from OSMnx\n",
    "    point_x, point_y : float\n",
    "        Coordinates of the target point\n",
    "    tolerance : float\n",
    "        Maximum distance to search\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int or None\n",
    "        Node ID of matched node, or None if no match found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Point object for the input coordinates\n",
    "        point = Point(point_x, point_y)\n",
    "        \n",
    "        # Find nearest edges, not just nodes\n",
    "        best_edge = None\n",
    "        best_dist = float('inf')\n",
    "        \n",
    "        # Check a sample of edges for efficiency (every 10th edge)\n",
    "        edges_sample = list(network.edges(data=True))[::10]\n",
    "        \n",
    "        for u, v, data in edges_sample:\n",
    "            if 'geometry' in data:\n",
    "                # If edge has a geometry attribute, use it\n",
    "                edge_geom = data['geometry']\n",
    "                dist = edge_geom.distance(point)\n",
    "            else:\n",
    "                # Otherwise, create a line between nodes\n",
    "                try:\n",
    "                    u_x, u_y = network.nodes[u]['x'], network.nodes[u]['y']\n",
    "                    v_x, v_y = network.nodes[v]['x'], network.nodes[v]['y']\n",
    "                    line = [(u_x, u_y), (v_x, v_y)]\n",
    "                    edge_geom = LineString(line)\n",
    "                    dist = edge_geom.distance(point)\n",
    "                except KeyError:\n",
    "                    # Skip if nodes don't have coordinates\n",
    "                    continue\n",
    "            \n",
    "            # Update best match\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_edge = (u, v)\n",
    "        \n",
    "        # Return the closer node from the best edge\n",
    "        if best_edge and best_dist <= tolerance:\n",
    "            u, v = best_edge\n",
    "            u_dist = ((network.nodes[u]['x'] - point_x)**2 + \n",
    "                      (network.nodes[u]['y'] - point_y)**2)**0.5\n",
    "            v_dist = ((network.nodes[v]['x'] - point_x)**2 + \n",
    "                      (network.nodes[v]['y'] - point_y)**2)**0.5\n",
    "            return u if u_dist < v_dist else v\n",
    "        \n",
    "        # Fallback to regular nearest node if no good edge found\n",
    "        return find_nearest_node(network, point_x, point_y, fallback_tolerance=tolerance)\n",
    "    \n",
    "    except Exception as e:\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"Map matching failed: {e}\")\n",
    "        # Fallback to regular nearest node\n",
    "        return find_nearest_node(network, point_x, point_y, fallback_tolerance=tolerance)\n",
    "\n",
    "\n",
    "def enhance_network_connectivity(G):\n",
    "    \"\"\"\n",
    "    Enhance network connectivity by ensuring key locations are connected.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G : networkx.Graph\n",
    "        Road network to enhance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    networkx.Graph\n",
    "        Enhanced network\n",
    "    \"\"\"\n",
    "    # Add self-loops to all nodes to ensure they can be reached from themselves\n",
    "    # This helps with isolated nodes\n",
    "    for node in G.nodes():\n",
    "        if not G.has_edge(node, node):\n",
    "            G.add_edge(node, node, length=0)\n",
    "    \n",
    "    # Check the number of connected components\n",
    "    connected_components = list(nx.connected_components(G.to_undirected()))\n",
    "    largest_component = max(connected_components, key=len)\n",
    "    \n",
    "    # If we have multiple components, add edges to connect them to largest component\n",
    "    if len(connected_components) > 1:\n",
    "        largest_component_nodes = list(largest_component)\n",
    "        for component in connected_components:\n",
    "            if component != largest_component:\n",
    "                # Find the closest node pair between components\n",
    "                min_dist = float('inf')\n",
    "                best_pair = None\n",
    "                \n",
    "                # Sample nodes from each component for efficiency\n",
    "                component_nodes = list(component)\n",
    "                sample_size = min(10, len(component_nodes))\n",
    "                sampled_component = random.sample(component_nodes, sample_size)\n",
    "                \n",
    "                sample_size_largest = min(20, len(largest_component_nodes))\n",
    "                sampled_largest = random.sample(largest_component_nodes, sample_size_largest)\n",
    "                \n",
    "                # Find the closest pair\n",
    "                for n1 in sampled_component:\n",
    "                    for n2 in sampled_largest:\n",
    "                        try:\n",
    "                            x1, y1 = G.nodes[n1]['x'], G.nodes[n1]['y']\n",
    "                            x2, y2 = G.nodes[n2]['x'], G.nodes[n2]['y']\n",
    "                            dist = ((x1-x2)**2 + (y1-y2)**2)**0.5\n",
    "                            if dist < min_dist:\n",
    "                                min_dist = dist\n",
    "                                best_pair = (n1, n2)\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                \n",
    "                # Add an edge between the closest nodes\n",
    "                if best_pair:\n",
    "                    G.add_edge(best_pair[0], best_pair[1], length=min_dist)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def calculate_multimodal_distance_batch(origins, destinations, walk_net, drive_net):\n",
    "    \"\"\"\n",
    "    Calculate minimum walking and driving distances from origins to destinations using NetworkX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    origins : dict\n",
    "        Dictionary mapping IDs to Point geometries for origins\n",
    "    destinations : list\n",
    "        List of Point geometries for destinations\n",
    "    walk_net : networkx.Graph\n",
    "        Walking network \n",
    "    drive_net : networkx.Graph\n",
    "        Driving network\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (walk_distances, drive_distances) dictionaries mapping (origin_id, destination_idx)\n",
    "        to distances in meters\n",
    "    \"\"\"\n",
    "    # Check if we should use cached distances\n",
    "    cache_file = os.path.join(CACHE_DIR, 'distance_cache.pkl')\n",
    "    \n",
    "    # Create cache dir if needed\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Try to load from cache\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            print(\"Loading distances from cache...\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                distances = pickle.load(f)\n",
    "            walk_d, drive_d = distances\n",
    "            print(f\"Loaded {len(walk_d)} walking and {len(drive_d)} driving distances from cache.\")\n",
    "            return walk_d, drive_d\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading distance cache: {e}\")\n",
    "            # Continue with calculation\n",
    "    \n",
    "    # Calculate distances with NetworkX\n",
    "    print(\"Calculating distances with NetworkX...\")\n",
    "    walk_distances, drive_distances = calculate_distances_with_networkx(\n",
    "        origins, destinations, walk_net, drive_net\n",
    "    )\n",
    "    \n",
    "    # Cache the results\n",
    "    try:\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump((walk_distances, drive_distances), f)\n",
    "        print(f\"Saved distance calculations to {cache_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save distance cache: {e}\")\n",
    "    \n",
    "    return walk_distances, drive_distances\n",
    "\n",
    "\n",
    "def calculate_distances_with_networkx(origins, destinations, walk_net, drive_net):\n",
    "    \"\"\"\n",
    "    Calculate distances between origins and destinations using NetworkX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    origins : dict\n",
    "        Dictionary mapping IDs to Point geometries for origins\n",
    "    destinations : list\n",
    "        List of Point geometries for destinations\n",
    "    walk_net : networkx.Graph\n",
    "        Walking network \n",
    "    drive_net : networkx.Graph\n",
    "        Driving network\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (walk_distances, drive_distances) dictionaries mapping (origin_id, destination_idx)\n",
    "        to distances in meters\n",
    "    \"\"\"\n",
    "    # Prepare outputs\n",
    "    walk_distances = {}\n",
    "    drive_distances = {}\n",
    "    \n",
    "    # Track failures for reporting\n",
    "    walk_failures = 0\n",
    "    drive_failures = 0\n",
    "    total_pairs = len(origins) * len(destinations)\n",
    "    \n",
    "    # Define distance filter threshold in meters\n",
    "    # This is based on the maximum distance band used in the methodology (15,840 ft â‰ˆ 4.8 km)\n",
    "    # Adding a small buffer to ensure we don't miss any relevant connections\n",
    "    MAX_DISTANCE_FILTER = 6000  # 6 km\n",
    "    \n",
    "    # Track stats for filtered pairs\n",
    "    skipped_pairs = 0\n",
    "    processed_pairs = 0\n",
    "    \n",
    "    print(f\"Using distance filter of {MAX_DISTANCE_FILTER/1000:.1f} km\")\n",
    "    \n",
    "    # Process each origin-destination pair\n",
    "    for origin_id, origin_geom in tqdm(origins.items(), desc=\"Calculating distances\"):\n",
    "        origin_x, origin_y = origin_geom.x, origin_geom.y\n",
    "        \n",
    "        # Find nearest nodes once per origin to speed things up\n",
    "        try:\n",
    "            origin_node_walk = ox.distance.nearest_nodes(walk_net, origin_x, origin_y)\n",
    "            origin_node_drive = ox.distance.nearest_nodes(drive_net, origin_x, origin_y)\n",
    "        except Exception as e:\n",
    "            if DEBUG_MODE:\n",
    "                print(f\"Failed to find nearest nodes for origin {origin_id}: {str(e)[:100]}...\")\n",
    "            # Skip this origin if we can't find nodes\n",
    "            continue\n",
    "        \n",
    "        # Pre-filter destinations based on Euclidean distance\n",
    "        filtered_destinations = []\n",
    "        for dest_idx, dest_geom in enumerate(destinations):\n",
    "            # Calculate straight-line distance\n",
    "            dest_x, dest_y = dest_geom.x, dest_geom.y\n",
    "            straight_dist = ((origin_x - dest_x)**2 + (origin_y - dest_y)**2)**0.5\n",
    "            \n",
    "            # Only process destinations within the threshold distance\n",
    "            if straight_dist <= MAX_DISTANCE_FILTER:\n",
    "                filtered_destinations.append((dest_idx, dest_geom))\n",
    "            else:\n",
    "                # For points beyond our filter, set to infinity (they'll get the maximum score anyway)\n",
    "                walk_distances[(origin_id, dest_idx)] = float('inf')\n",
    "                drive_distances[(origin_id, dest_idx)] = float('inf')\n",
    "                skipped_pairs += 1\n",
    "        \n",
    "        # Process each filtered destination for this origin\n",
    "        for dest_idx, dest_geom in filtered_destinations:\n",
    "            processed_pairs += 1\n",
    "            dest_x, dest_y = dest_geom.x, dest_geom.y\n",
    "            \n",
    "            # Try walking distance calculation\n",
    "            try:\n",
    "                dest_node = ox.distance.nearest_nodes(walk_net, dest_x, dest_y)\n",
    "                path_length = nx.shortest_path_length(\n",
    "                    walk_net, origin_node_walk, dest_node, weight='length'\n",
    "                )\n",
    "                walk_distances[(origin_id, dest_idx)] = path_length\n",
    "            except Exception as e:\n",
    "                # Fall back to straight-line distance with 1.3 detour factor for walking\n",
    "                # This is a reasonable approximation for urban areas\n",
    "                if DEBUG_MODE:\n",
    "                    print(f\"Walking path failed for ({origin_id}, {dest_idx}): {str(e)[:100]}...\")\n",
    "                \n",
    "                straight_dist = geopy.distance.geodesic(\n",
    "                    (origin_y, origin_x), (dest_y, dest_x)\n",
    "                ).meters\n",
    "                \n",
    "                # Apply detour factor - walking routes tend to be ~1.3x straight line distance\n",
    "                walk_distances[(origin_id, dest_idx)] = straight_dist * 1.3\n",
    "                walk_failures += 1\n",
    "            \n",
    "            # Try driving distance calculation\n",
    "            try:\n",
    "                dest_node = ox.distance.nearest_nodes(drive_net, dest_x, dest_y)\n",
    "                path_length = nx.shortest_path_length(\n",
    "                    drive_net, origin_node_drive, dest_node, weight='length'\n",
    "                )\n",
    "                drive_distances[(origin_id, dest_idx)] = path_length\n",
    "            except Exception as e:\n",
    "                # Fall back to straight-line distance with 1.5 detour factor for driving\n",
    "                # This is a reasonable approximation for urban areas\n",
    "                if DEBUG_MODE:\n",
    "                    print(f\"Driving path failed for ({origin_id}, {dest_idx}): {str(e)[:100]}...\")\n",
    "                \n",
    "                straight_dist = geopy.distance.geodesic(\n",
    "                    (origin_y, origin_x), (dest_y, dest_x)\n",
    "                ).meters\n",
    "                \n",
    "                # Apply detour factor - driving routes tend to be ~1.5x straight line distance\n",
    "                drive_distances[(origin_id, dest_idx)] = straight_dist * 1.5\n",
    "                drive_failures += 1\n",
    "    \n",
    "    # Report on statistics\n",
    "    filter_percent = skipped_pairs / total_pairs * 100 if total_pairs > 0 else 0\n",
    "    print(f\"Distance filter: Processed {processed_pairs} pairs, skipped {skipped_pairs} pairs ({filter_percent:.1f}% reduction)\")\n",
    "    \n",
    "    # Report on failures\n",
    "    if walk_failures > 0:\n",
    "        print(f\"Warning: {walk_failures}/{processed_pairs} walking distance calculations failed ({walk_failures/processed_pairs*100:.1f}%)\")\n",
    "        print(\"Used straight-line distance with detour factor as fallback.\")\n",
    "    if drive_failures > 0:\n",
    "        print(f\"Warning: {drive_failures}/{processed_pairs} driving distance calculations failed ({drive_failures/processed_pairs*100:.1f}%)\")\n",
    "        print(\"Used straight-line distance with detour factor as fallback.\")\n",
    "    \n",
    "    return walk_distances, drive_distances\n",
    "\n",
    "\n",
    "def calculate_coverage_scores(census, walk_dist, drive_dist, stations):\n",
    "    \"\"\"Calculate coverage scores with station capacity and power factored in.\"\"\"\n",
    "    # Create station quality index (combine points count and power)\n",
    "    stations['quality_index'] = stations['num_points'] * np.sqrt(stations['max_power'] / 50)\n",
    "    # Normalize quality to 0.5-1.5 range (0.5=worst, 1.0=average, 1.5=best)\n",
    "    min_q = stations['quality_index'].min()\n",
    "    max_q = stations['quality_index'].max()\n",
    "    stations['quality_factor'] = 0.5 + ((stations['quality_index'] - min_q) / (max_q - min_q)) if max_q > min_q else 1.0\n",
    "    \n",
    "    # Build station lookup by geometry for quick access\n",
    "    station_lookup = {geom.wkt: factor for geom, factor in zip(stations.geometry, stations['quality_factor'])}\n",
    "    \n",
    "    results = []\n",
    "    for idx, row in census.iterrows():\n",
    "        w = walk_dist.get(idx, np.inf)\n",
    "        d = drive_dist.get(idx, np.inf)\n",
    "        \n",
    "        # Find nearest station and its quality factor\n",
    "        nearest_dist = min(w, d)\n",
    "        nearest_station_geom = min(\n",
    "            [pt for pt in stations.geometry],\n",
    "            key=lambda pt: ((pt.x - row.geometry.centroid.x)**2 + (pt.y - row.geometry.centroid.y)**2)**0.5\n",
    "        )\n",
    "        quality_factor = station_lookup.get(nearest_station_geom.wkt, 1.0)\n",
    "        \n",
    "        # Calculate base coverage as before\n",
    "        ws = 0.7 if w<=1320 else 0.5 if w<=2640 else 0.3 if w<=3960 else 1.0\n",
    "        ds = 0.7 if d<=5280 else 0.5 if d<=10560 else 0.3 if d<=15840 else 1.0\n",
    "        combined = 0.4*ws + 0.6*ds\n",
    "        \n",
    "        # Adjust coverage by station quality\n",
    "        adjusted_combined = combined * quality_factor\n",
    "        \n",
    "        # Apply density adjustment\n",
    "        dens = row['pop_density']/census['pop_density'].max()\n",
    "        score = adjusted_combined * (1 - 0.3*dens)\n",
    "        \n",
    "        results.append((score, w, d))\n",
    "        \n",
    "    df = pd.DataFrame(results, index=census.index, columns=['score_raw','walk_ft','drive_ft'])\n",
    "    minv, maxv = df['score_raw'].min(), df['score_raw'].max()\n",
    "    df['coverage_score'] = (df['score_raw'] - minv)/(maxv-minv)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_service_areas(stations, walk_net, drive_net):\n",
    "    \"\"\"\n",
    "    Build service area polygons for the stations using NetworkX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stations : GeoDataFrame\n",
    "        GeoDataFrame of EV charging stations\n",
    "    walk_net : networkx.Graph\n",
    "        Walking network\n",
    "    drive_net : networkx.Graph\n",
    "        Driving network\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of service area polygons by type and distance\n",
    "    \"\"\"\n",
    "    # Check if we should use cached service areas\n",
    "    cache_file = os.path.join(CACHE_DIR, 'service_areas_cache.pkl')\n",
    "    \n",
    "    # Try to load from cache\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            print(\"Loading service areas from cache...\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                service_areas = pickle.load(f)\n",
    "            print(f\"Loaded {len(service_areas)} service areas from cache\")\n",
    "            return service_areas\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading service areas cache: {e}\")\n",
    "            # Continue with calculation\n",
    "    \n",
    "    # Calculate service areas if not cached\n",
    "    print(\"Building service areas using NetworkX...\")\n",
    "    service_areas = build_service_areas_with_networkx(stations, walk_net, drive_net)\n",
    "    \n",
    "    # Cache the results\n",
    "    try:\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(service_areas, f)\n",
    "        print(f\"Saved service areas to {cache_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save service areas cache: {e}\")\n",
    "    \n",
    "    return service_areas\n",
    "\n",
    "\n",
    "def build_service_areas_with_networkx(stations, walk_net, drive_net):\n",
    "    \"\"\"\n",
    "    Legacy method to build service areas using NetworkX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stations : GeoDataFrame\n",
    "        GeoDataFrame of EV charging stations\n",
    "    walk_net : networkx.Graph\n",
    "        Walking network\n",
    "    drive_net : networkx.Graph\n",
    "        Driving network\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of service area polygons by type and distance\n",
    "    \"\"\"\n",
    "    # Initialize result dictionary\n",
    "    service_areas = {}\n",
    "    \n",
    "    # Define service area distances\n",
    "    walk_distances = [400, 800]  # meters (approx. 5 and 10 min walk)\n",
    "    drive_distances = [1000, 3000, 8000]  # meters\n",
    "    \n",
    "    # Track successes and failures\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Create walking service areas\n",
    "    for dist in walk_distances:\n",
    "        print(f\"Creating walking service area for {dist}m...\")\n",
    "        area_key = f'walk_{dist}m'\n",
    "        reachable_nodes = []\n",
    "        \n",
    "        # For each station, find reachable nodes\n",
    "        for idx, station in stations.iterrows():\n",
    "            try:\n",
    "                # Find nearest node to station\n",
    "                start_node = ox.distance.nearest_nodes(walk_net, station.geometry.x, station.geometry.y)\n",
    "                \n",
    "                # Get reachable nodes within distance\n",
    "                reachable = nx.single_source_dijkstra_path_length(\n",
    "                    walk_net, start_node, cutoff=dist, weight='length'\n",
    "                )\n",
    "                \n",
    "                # Get coordinates of reachable nodes\n",
    "                for node_id in reachable:\n",
    "                    x, y = walk_net.nodes[node_id]['x'], walk_net.nodes[node_id]['y']\n",
    "                    reachable_nodes.append(Point(x, y))\n",
    "            except Exception as e:\n",
    "                if DEBUG_MODE:\n",
    "                    print(f\"Error calculating walking service area for station {idx}: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Create service area from reachable nodes\n",
    "        if len(reachable_nodes) >= 3:\n",
    "            try:\n",
    "                # Create convex hull from reachable nodes\n",
    "                multi_point = MultiPoint(reachable_nodes)\n",
    "                service_areas[area_key] = multi_point.convex_hull\n",
    "                success_count += 1\n",
    "            except Exception as e:\n",
    "                if DEBUG_MODE:\n",
    "                    print(f\"Error creating convex hull for {area_key}: {str(e)[:100]}...\")\n",
    "                # Fallback to buffer\n",
    "                service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n",
    "                error_count += 1\n",
    "        else:\n",
    "            # If too few reachable nodes, use buffer\n",
    "            print(f\"Too few reachable nodes ({len(reachable_nodes)}) for {area_key}. Using buffer.\")\n",
    "            service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n",
    "            error_count += 1\n",
    "    \n",
    "    # Create driving service areas\n",
    "    for dist in drive_distances:\n",
    "        print(f\"Creating driving service area for {dist}m...\")\n",
    "        area_key = f'drive_{dist}m'\n",
    "        reachable_nodes = []\n",
    "        \n",
    "        # For each station, find reachable nodes\n",
    "        for idx, station in stations.iterrows():\n",
    "            try:\n",
    "                # Find nearest node to station\n",
    "                start_node = ox.distance.nearest_nodes(drive_net, station.geometry.x, station.geometry.y)\n",
    "                \n",
    "                # Get reachable nodes within distance\n",
    "                reachable = nx.single_source_dijkstra_path_length(\n",
    "                    drive_net, start_node, cutoff=dist, weight='length'\n",
    "                )\n",
    "                \n",
    "                # Get coordinates of reachable nodes\n",
    "                for node_id in reachable:\n",
    "                    x, y = drive_net.nodes[node_id]['x'], drive_net.nodes[node_id]['y']\n",
    "                    reachable_nodes.append(Point(x, y))\n",
    "            except Exception as e:\n",
    "                if DEBUG_MODE:\n",
    "                    print(f\"Error calculating driving service area for station {idx}: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Create service area from reachable nodes\n",
    "        if len(reachable_nodes) >= 3:\n",
    "            try:\n",
    "                # Create convex hull from reachable nodes\n",
    "                multi_point = MultiPoint(reachable_nodes)\n",
    "                service_areas[area_key] = multi_point.convex_hull\n",
    "                success_count += 1\n",
    "            except Exception as e:\n",
    "                if DEBUG_MODE:\n",
    "                    print(f\"Error creating convex hull for {area_key}: {str(e)[:100]}...\")\n",
    "                # Fallback to buffer\n",
    "                service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n",
    "                error_count += 1\n",
    "        else:\n",
    "            # If too few reachable nodes, use buffer\n",
    "            print(f\"Too few reachable nodes ({len(reachable_nodes)}) for {area_key}. Using buffer.\")\n",
    "            service_areas[area_key] = stations.geometry.unary_union.buffer(dist)\n",
    "            error_count += 1\n",
    "    \n",
    "    print(f\"Service area generation: {success_count} successes, {error_count} fallbacks to buffers\")\n",
    "    return service_areas\n",
    "\n",
    "\n",
    "def perform_equity_analysis(census, areas):\n",
    "    \"\"\"Compute effect size and confidence intervals for equity metrics.\"\"\"\n",
    "    # Check if we should use cached equity results\n",
    "    cache_file = os.path.join(CACHE_DIR, 'equity_analysis_cache.pkl')\n",
    "    \n",
    "    # Try to load from cache\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            print(\"Loading equity analysis from cache...\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                equity = pickle.load(f)\n",
    "            print(f\"Loaded equity analysis for {len(equity)} service areas from cache\")\n",
    "            return equity\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading equity analysis cache: {e}\")\n",
    "            # Continue with calculation\n",
    "    \n",
    "    # Perform equity analysis if not cached\n",
    "    print(\"Computing equity analysis...\")\n",
    "    equity = {}\n",
    "    for label, geom in areas.items():\n",
    "        covered = census[census.geometry.intersects(geom)]\n",
    "        not_covered = census[~census.index.isin(covered.index)]\n",
    "        metrics = {}\n",
    "        for var in ['median_income','poverty_rate','pop_density','bach_degree_rate']:\n",
    "            a = covered[var].dropna()\n",
    "            b = not_covered[var].dropna()\n",
    "            if len(a)>1 and len(b)>1:\n",
    "                pooled = np.sqrt(((len(a)-1)*a.var()+(len(b)-1)*b.var())/(len(a)+len(b)-2))\n",
    "                d = (a.mean()-b.mean())/pooled\n",
    "                se = pooled*np.sqrt(1/len(a)+1/len(b))\n",
    "                ci = (d-1.96*se, d+1.96*se)\n",
    "                pval = stats.ttest_ind(a,b).pvalue\n",
    "                metrics[var] = {'effect_size':d,'ci_lower':ci[0],'ci_upper':ci[1],'p_value':pval}\n",
    "        equity[label] = metrics\n",
    "    \n",
    "    # Cache the results\n",
    "    try:\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(equity, f)\n",
    "        print(f\"Saved equity analysis to {cache_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save equity analysis cache: {e}\")\n",
    "    \n",
    "    return equity\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Compute dynamic weights based on equity disparities\n",
    "def calculate_dynamic_weights(equity_results):\n",
    "    # Base weights for each component - starting point\n",
    "    weights = {'coverage':0.30, 'income':0.20, 'poverty':0.20, 'density':0.15, 'education':0.15}\n",
    "    \n",
    "    # Track effect sizes and significance by variable across all buffer types\n",
    "    effect_sizes = {'income': 0, 'poverty': 0, 'density': 0, 'education': 0}\n",
    "    significant_effects = {'income': False, 'poverty': False, 'density': False, 'education': False}\n",
    "    \n",
    "    # Map variable names to weight keys\n",
    "    var_to_weight = {\n",
    "        'median_income': 'income',\n",
    "        'poverty_rate': 'poverty',\n",
    "        'pop_density': 'density',\n",
    "        'bach_degree_rate': 'education'\n",
    "    }\n",
    "    \n",
    "    # First pass: collect maximum effect sizes and check significance\n",
    "    for buf, metrics in equity_results.items():\n",
    "        for var, stats in metrics.items():\n",
    "            if var in var_to_weight:\n",
    "                weight_key = var_to_weight[var]\n",
    "                # Track the highest absolute effect size found\n",
    "                effect_sizes[weight_key] = max(effect_sizes[weight_key], abs(stats['effect_size']))\n",
    "                # Mark as significant if p-value < 0.05\n",
    "                if stats['p_value'] < 0.05:\n",
    "                    significant_effects[weight_key] = True\n",
    "    \n",
    "    # Adjust weights based on effect sizes and significance\n",
    "    for weight_key, effect_size in effect_sizes.items():\n",
    "        is_significant = significant_effects[weight_key]\n",
    "        \n",
    "        if is_significant:\n",
    "            # For significant effects, apply progressive scaling:\n",
    "            # Small effect (0.2-0.5): +5%\n",
    "            # Medium effect (0.5-0.8): +10%\n",
    "            # Large effect (>0.8): +15%\n",
    "            if effect_size > 0.8:\n",
    "                weights[weight_key] += 0.15\n",
    "            elif effect_size > 0.5:\n",
    "                weights[weight_key] += 0.10\n",
    "            elif effect_size > 0.2:\n",
    "                weights[weight_key] += 0.05\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    total = sum(weights.values())\n",
    "    normalized_weights = {k: v/total for k, v in weights.items()}\n",
    "    \n",
    "    if DEBUG_MODE:\n",
    "        print(\"Original weights:\", weights)\n",
    "        print(\"Effect sizes:\", effect_sizes)\n",
    "        print(\"Significant effects:\", significant_effects)\n",
    "        print(\"Normalized weights:\", normalized_weights)\n",
    "    \n",
    "    return normalized_weights\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Apply final gap scoring by combining coverage, income, poverty, density, and education\n",
    "def apply_gap_scoring(census, dyn_wts):\n",
    "    \"\"\"\n",
    "    Apply a simpler gap scoring methodology using:\n",
    "    1. Linear combinations of normalized components\n",
    "    2. Quantile-based thresholds for natural data distribution\n",
    "    \"\"\"\n",
    "    # Step 1: Create simple normalized components (0-1 scale)\n",
    "    \n",
    "    # Coverage component (higher coverage => lower gap)\n",
    "    cov_norm = 1 - census['coverage_score']  # Invert so higher = worse coverage\n",
    "    cov_comp = cov_norm * dyn_wts['coverage']\n",
    "    \n",
    "    # Income component (lower income => higher gap)\n",
    "    inc_norm = (census['median_income'].max() - census['median_income']) / (census['median_income'].max() - census['median_income'].min())\n",
    "    inc_comp = inc_norm * dyn_wts['income']\n",
    "    \n",
    "    # Poverty component (higher poverty => higher gap)\n",
    "    pov_norm = (census['poverty_rate'] - census['poverty_rate'].min()) / (census['poverty_rate'].max() - census['poverty_rate'].min())\n",
    "    pov_comp = pov_norm * dyn_wts['poverty']\n",
    "    \n",
    "    # Density component (higher pop density => higher gap)\n",
    "    den_norm = (census['pop_density'] - census['pop_density'].min()) / (census['pop_density'].max() - census['pop_density'].min())\n",
    "    den_comp = den_norm * dyn_wts['density']\n",
    "    \n",
    "    # Education component (lower education => higher gap)\n",
    "    edu_norm = (census['bach_degree_rate'].max() - census['bach_degree_rate']) / (census['bach_degree_rate'].max() - census['bach_degree_rate'].min())\n",
    "    edu_comp = edu_norm * dyn_wts['education']\n",
    "    \n",
    "    # Step 2: Linear combination of components (simple weighted sum)\n",
    "    census['gap_score'] = cov_comp + inc_comp + pov_comp + den_comp + edu_comp\n",
    "    \n",
    "    # Step 3: Use fixed thresholds based on meaningful categories rather than quartiles\n",
    "    # Low, Medium, High, Critical priority with more tracts in lower categories\n",
    "    # These fixed thresholds create a right-skewed distribution\n",
    "    # Using global THRESHOLDS instead of defining locally\n",
    "    \n",
    "    if DEBUG_MODE:\n",
    "        print(f\"Fixed thresholds: {THRESHOLDS}\")\n",
    "    \n",
    "    labels = ['Low Priority','Medium Priority','High Priority','Critical Priority']\n",
    "    census['gap_category'] = pd.cut(census['gap_score'], \n",
    "                                   bins=[-np.inf] + THRESHOLDS + [np.inf], \n",
    "                                   labels=labels, \n",
    "                                   include_lowest=True)\n",
    "\n",
    "    # Print component statistics for debugging\n",
    "    if DEBUG_MODE:\n",
    "        print(\"\\n=== GAP SCORE COMPONENTS ===\")\n",
    "        print(f\"Coverage component (weight: {dyn_wts['coverage']:.2f}):\")\n",
    "        print(cov_comp.describe())\n",
    "        print(f\"\\nIncome component (weight: {dyn_wts['income']:.2f}):\")\n",
    "        print(inc_comp.describe())\n",
    "        print(f\"\\nPoverty component (weight: {dyn_wts['poverty']:.2f}):\")\n",
    "        print(pov_comp.describe())\n",
    "        print(f\"\\nDensity component (weight: {dyn_wts['density']:.2f}):\")\n",
    "        print(den_comp.describe())\n",
    "        print(f\"\\nEducation component (weight: {dyn_wts['education']:.2f}):\")\n",
    "        print(edu_comp.describe())\n",
    "\n",
    "    # Print final gap score statistics\n",
    "    if DEBUG_MODE:\n",
    "        print(\"\\n=== GAP SCORE STATISTICS ===\")\n",
    "        print(census['gap_score'].describe())\n",
    "        \n",
    "        # Print distribution of priority categories\n",
    "        print(\"\\n=== PRIORITY CATEGORY DISTRIBUTION ===\")\n",
    "        cat_counts = census['gap_category'].value_counts().sort_index()\n",
    "        for category, count in cat_counts.items():\n",
    "            percentage = (count / len(census)) * 100\n",
    "            print(f\"{category}: {count} tracts ({percentage:.1f}%)\")\n",
    "\n",
    "    return census\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Compute coverage statistics by demographic category\n",
    "def compute_coverage_by_demographics(census):\n",
    "    demo_cov = {}\n",
    "    # Loop through each demographic dimension\n",
    "    for col in ['income_category','education_category','density_category']:\n",
    "        demo_cov[col] = {}\n",
    "        for cat in census[col].unique():\n",
    "            sub = census[census[col] == cat]\n",
    "            total_pop = int(sub['total_pop'].sum())\n",
    "            covered_pop = int(sub[sub['is_served']]['total_pop'].sum())\n",
    "            pct = covered_pop / total_pop * 100 if total_pop>0 else 0\n",
    "            demo_cov[col][cat] = {'total_population': total_pop, 'covered_population': covered_pop, 'coverage_percent': pct}\n",
    "    return demo_cov\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Save all pipeline outputs: GeoPackage, JSON/CSV stats, and README\n",
    "def save_pipeline_outputs(census, stations, areas, equity_res, demo_cov, dyn_wts):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    # Save census tracts and stations\n",
    "    census.to_file('data/gap_analysis_fixed.gpkg', layer='census', driver='GPKG')\n",
    "    stations.to_file('data/gap_analysis_fixed.gpkg', layer='stations', driver='GPKG')\n",
    "    # Save each service area as its own layer\n",
    "    for label, geom in areas.items():\n",
    "        gdf = gpd.GeoDataFrame(geometry=[geom], crs=census.crs)\n",
    "        gdf.to_file('data/gap_analysis_fixed.gpkg', layer=f'service_area_{label}', driver='GPKG')\n",
    "    # Equity analysis JSON & CSV\n",
    "    with open('data/equity_analysis.json','w') as f: json.dump(equity_res, f, indent=2)\n",
    "    eq_list = []\n",
    "    for buf, mets in equity_res.items():\n",
    "        for var, st in mets.items(): eq_list.append({'buffer':buf,'variable':var,**st})\n",
    "    pd.DataFrame(eq_list).to_csv('data/equity_analysis.csv', index=False)\n",
    "    # Demographic coverage JSON & CSV\n",
    "    with open('data/coverage_by_demographics.json','w') as f: json.dump(demo_cov, f, indent=2)\n",
    "    cv_list = []\n",
    "    for col, cats in demo_cov.items():\n",
    "        for cat, st in cats.items(): cv_list.append({'demographic_group':col,'category':cat,**st})\n",
    "    pd.DataFrame(cv_list).to_csv('data/coverage_statistics.csv', index=False)\n",
    "    # Summary of dynamic weights and overall coverage\n",
    "    summary = {'dynamic_weights':dyn_wts,'total_tracts':len(census),'total_stations':len(stations),'coverage_percent':float(census['is_served'].mean()*100)}\n",
    "    with open('data/analysis_summary.json','w') as f: json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # Calculate statistics by gap category\n",
    "    gap_stats = {}\n",
    "    priority_levels = census['gap_category'].unique()\n",
    "    for level in priority_levels:\n",
    "        tracts = census[census['gap_category'] == level]\n",
    "        gap_stats[level] = {\n",
    "            'tract_count': int(len(tracts)),\n",
    "            'population': int(tracts['total_pop'].sum()),\n",
    "            'area_sq_mi': float(tracts.geometry.area.sum() / (5280 * 5280)),  # Convert sq ft to sq mi\n",
    "            'pop_density': float(tracts['total_pop'].sum() / (tracts.geometry.area.sum() / (5280 * 5280))),\n",
    "            'median_income_avg': float(tracts['median_income'].mean()),\n",
    "            'poverty_rate_avg': float(tracts['poverty_rate'].mean())\n",
    "        }\n",
    "    \n",
    "    # Calculate summary stats for README\n",
    "    coverage_percent = float(census[census['is_served']]['total_pop'].sum() / census['total_pop'].sum() * 100)\n",
    "\n",
    "    # Create more detailed README with buffer distances\n",
    "    readme = f\"\"\"\n",
    "# EV Charging Gap Analysis Results\n",
    "\n",
    "**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Overview\n",
    "This analysis identifies areas in Philadelphia with the greatest need for additional EV charging infrastructure, \n",
    "based on physical access to existing chargers and socioeconomic factors.\n",
    "\n",
    "## Methodology\n",
    "The gap score combines:\n",
    "- **Coverage score** ({dyn_wts['coverage']*100:.1f}%): Physical access to existing chargers\n",
    "- **Socioeconomic scores** ({(1-dyn_wts['coverage'])*100:.1f}%):\n",
    "  - Income ({dyn_wts['income']*100:.1f}%)\n",
    "  - Poverty ({dyn_wts['poverty']*100:.1f}%)\n",
    "  - Population density ({dyn_wts['density']*100:.1f}%)\n",
    "  - Education ({dyn_wts['education']*100:.1f}%)\n",
    "\n",
    "## Service Area Buffer Distances\n",
    "| Mode | Distance | Time Equivalent |\n",
    "|------|----------|-----------------|\n",
    "| Walking | 1,320 ft | ~5 min walk |\n",
    "| Walking | 2,640 ft | ~10 min walk |\n",
    "| Walking | 3,960 ft | ~15 min walk |\n",
    "| Driving | 5,280 ft | ~1 mile |\n",
    "| Driving | 10,560 ft | ~2 miles |\n",
    "| Driving | 15,840 ft | ~3 miles |\n",
    "\n",
    "## Priority Categories\n",
    "- **Low Priority:** gap_score â‰¤ {THRESHOLDS[0]}\n",
    "- **Medium Priority:** {THRESHOLDS[0]} < gap_score â‰¤ {THRESHOLDS[1]}\n",
    "- **High Priority:** {THRESHOLDS[1]} < gap_score â‰¤ {THRESHOLDS[2]}\n",
    "- **Critical Priority:** gap_score > {THRESHOLDS[2]}\n",
    "\n",
    "## Results Summary\n",
    "- Total census tracts: {len(census)}\n",
    "- Total EV stations: {len(stations)}\n",
    "- Population coverage: {coverage_percent:.1f}%\n",
    "- Critical priority areas: {gap_stats.get('Critical Priority', {}).get('tract_count', 0)} tracts\n",
    "\n",
    "## Output Files\n",
    "- **gap_analysis_fixed.gpkg**: GeoPackage with spatial layers\n",
    "- **analysis_results.json**: Complete nested analysis results\n",
    "- **summary_statistics.json**: Key population statistics by category\n",
    "- **equity_analysis.csv**: Detailed equity analysis results\n",
    "- **coverage_statistics.csv**: Coverage by demographic group\n",
    "- **visualizations/**: Charts, maps and summary tables\n",
    "\"\"\"\n",
    "    with open('data/README.md','w') as f: f.write(readme)\n",
    "\n",
    "    # Save complete analysis results (nested structure)\n",
    "    complete_results = {\n",
    "        'summary': {\n",
    "            'total_census_tracts': len(census),\n",
    "            'total_stations': len(stations),\n",
    "            'total_population': int(census['total_pop'].sum()),\n",
    "            'study_area_sq_mi': float(census.geometry.area.sum() / (5280 * 5280))\n",
    "        },\n",
    "        'coverage': {\n",
    "            'served_tracts': int(census['is_served'].sum()),\n",
    "            'served_population': int(census[census['is_served']]['total_pop'].sum()),\n",
    "            'service_coverage_pct': coverage_percent\n",
    "        },\n",
    "        'gap_scores': {\n",
    "            'mean': float(census['gap_score'].mean()),\n",
    "            'median': float(census['gap_score'].median()),\n",
    "            'min': float(census['gap_score'].min()),\n",
    "            'max': float(census['gap_score'].max()),\n",
    "            'std': float(census['gap_score'].std())\n",
    "        },\n",
    "        'priority_areas': gap_stats,\n",
    "        'weights': dyn_wts,\n",
    "        'demographic_coverage': demo_cov,\n",
    "        'validation': census['validation'] if 'validation' in census.columns else {}\n",
    "    }\n",
    "    \n",
    "    with open('data/analysis_results.json', 'w') as f:\n",
    "        json.dump(complete_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save summary statistics with population counts by category\n",
    "    summary_stats = {\n",
    "        'total_tracts': len(census),\n",
    "        'total_stations': len(stations),\n",
    "        'total_population': int(census['total_pop'].sum()),\n",
    "        'area_sq_mi': float(census.geometry.area.sum() / (5280 * 5280)),\n",
    "        'population_density': float(census['total_pop'].sum() / (census.geometry.area.sum() / (5280 * 5280))),\n",
    "        'coverage': {\n",
    "            'served_population': int(census[census['is_served']]['total_pop'].sum()),\n",
    "            'coverage_percent': coverage_percent\n",
    "        },\n",
    "        'gap_categories': {\n",
    "            level: {\n",
    "                'tract_count': gap_stats[level]['tract_count'],\n",
    "                'population': gap_stats[level]['population']\n",
    "            } for level in priority_levels\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('data/summary_statistics.json', 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Generate visualizations: histogram, equity summary table, bar chart, and interactive map\n",
    "def create_visualizations(census, equity_res, demo_cov):\n",
    "    \"\"\"Generate visualizations: histogram, equity summary table, bar chart, and interactive map.\"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    vis_dir = 'data/visualizations'\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    \n",
    "    # Make sure the directory is writable\n",
    "    if not os.access(vis_dir, os.W_OK):\n",
    "        print(f\"ERROR: Directory {vis_dir} is not writable!\")\n",
    "        return\n",
    "    \n",
    "    # 1) Histogram of gap scores with threshold lines and priority labels\n",
    "    try:\n",
    "        # Set up the figure\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Create the histogram with gap scores\n",
    "        n, bins, patches = plt.hist(census['gap_score'], \n",
    "                                   bins=30, \n",
    "                                   color='steelblue', \n",
    "                                   edgecolor='black', \n",
    "                                   alpha=0.8)\n",
    "        \n",
    "        # Add vertical lines for score thresholds\n",
    "        for i, threshold in enumerate(THRESHOLDS):\n",
    "            plt.axvline(x=threshold, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "            \n",
    "            # Add label above the line with a white background for better readability\n",
    "            label_y = max(n) * 0.85  # Position at 85% of max height\n",
    "            \n",
    "            # Create a white background for the label\n",
    "            bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"red\", alpha=0.8)\n",
    "            \n",
    "            # Position the label slightly to the right of the line to avoid overlap\n",
    "            plt.text(threshold + 0.01, label_y, f\"{threshold:.2f}\", \n",
    "                    color='red', fontweight='bold', ha='left', va='center',\n",
    "                    bbox=bbox_props)\n",
    "        \n",
    "        # Add priority area labels centered in each section\n",
    "        priorities = [\"Low Priority\", \"Medium Priority\", \"High Priority\", \"Critical Priority\"]\n",
    "        section_mids = [0.375, (THRESHOLDS[0]+THRESHOLDS[1])/2, \n",
    "                        (THRESHOLDS[1]+THRESHOLDS[2])/2, (THRESHOLDS[2] + 0.8)/2]\n",
    "        \n",
    "        for i, (priority, x_pos) in enumerate(zip(priorities, section_mids)):\n",
    "            plt.text(x_pos, max(n) * 0.95, priority, \n",
    "                    ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.title('Distribution of Gap Scores with Fixed Thresholds', fontsize=14)\n",
    "        plt.xlabel('Gap Score')\n",
    "        plt.ylabel('Number of Tracts')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Save the histogram\n",
    "        hist_file = f'{vis_dir}/gap_score_distribution.png'\n",
    "        plt.savefig(hist_file, dpi=200)\n",
    "        plt.close()  # Make sure to close the plot to avoid displaying it\n",
    "        \n",
    "        if os.path.exists(hist_file) and os.path.getsize(hist_file) > 0:\n",
    "            print(f\"Successfully created histogram: {hist_file}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create histogram or file is empty: {hist_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating histogram: {str(e)}\")\n",
    "    \n",
    "    # 2) Demographic equity summary CSV\n",
    "    try:\n",
    "        if DEBUG_MODE:\n",
    "            print(\"Equity results structure:\", equity_res.keys())\n",
    "            print(\"First buffer sample:\", list(equity_res.values())[0] if equity_res else \"Empty\")\n",
    "\n",
    "        rows = []\n",
    "        for buffer_name, metrics in equity_res.items():\n",
    "            for variable, stats in metrics.items():\n",
    "                row = {'buffer': buffer_name, 'variable': variable}\n",
    "                for stat_name, stat_value in stats.items():\n",
    "                    row[stat_name] = stat_value\n",
    "                rows.append(row)\n",
    "        \n",
    "        if not rows:\n",
    "            print(\"WARNING: No equity data available for CSV export!\")\n",
    "            \n",
    "        eq_df = pd.DataFrame(rows)\n",
    "\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"Equity DataFrame shape: {eq_df.shape}\")\n",
    "            if not eq_df.empty:\n",
    "                print(eq_df.head(2))\n",
    "\n",
    "        # Save to visualizations subfolder\n",
    "        csv_file = f'{vis_dir}/demographic_equity_summary.csv'\n",
    "        eq_df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        # Check that file was created successfully\n",
    "        if os.path.exists(csv_file) and os.path.getsize(csv_file) > 0:\n",
    "            print(f\"Successfully created equity CSV: {csv_file}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create equity CSV or file is empty: {csv_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating equity CSV: {str(e)}\")\n",
    "    \n",
    "    # 3) Bar chart of coverage by demographic group\n",
    "    try:\n",
    "        # Debug the input data\n",
    "        if DEBUG_MODE:\n",
    "            print(\"\\nDemographic coverage data:\")\n",
    "            print(f\"Keys: {demo_cov.keys()}\")\n",
    "            for col, cats in demo_cov.items():\n",
    "                print(f\"  {col}: {len(cats)} categories\")\n",
    "                for cat, stats in cats.items():\n",
    "                    print(f\"    {cat}: {stats}\")\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        categories = ['density', 'education', 'income']\n",
    "        levels = ['low', 'medium', 'high']\n",
    "        \n",
    "        # Create dataframe with simplified structure\n",
    "        plot_data = []\n",
    "        \n",
    "        for category in categories:\n",
    "            cat_key = f\"{category}_category\"\n",
    "            if cat_key in demo_cov:\n",
    "                for level in levels:\n",
    "                    level_key = f\"{level}_{category}\"\n",
    "                    if level_key in demo_cov[cat_key]:\n",
    "                        coverage = demo_cov[cat_key][level_key]['coverage_percent']\n",
    "                        plot_data.append({\n",
    "                            'category': category.title(),\n",
    "                            'level': level.title(),\n",
    "                            'coverage': coverage\n",
    "                        })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        if plot_df.empty:\n",
    "            print(\"WARNING: No demographic coverage data available for bar chart!\")\n",
    "            return\n",
    "            \n",
    "        if DEBUG_MODE:\n",
    "            print(f\"\\nPlot DataFrame shape: {plot_df.shape}\")\n",
    "            print(plot_df.head())\n",
    "        \n",
    "        # Create improved bar chart\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        \n",
    "        # Calculate positions for bars\n",
    "        categories_n = len(categories)\n",
    "        levels_n = len(levels)\n",
    "        width = 0.8 / levels_n  # Bar width\n",
    "        \n",
    "        # Colors for bars\n",
    "        colors = ['#FF8C61', '#FFB56B', '#FDD57E']\n",
    "        \n",
    "        # Plot bars for each level within each category\n",
    "        for i, level in enumerate(levels):\n",
    "            level_data = plot_df[plot_df['level'] == level.title()]\n",
    "            x_positions = np.arange(categories_n) + (i - levels_n/2 + 0.5) * width\n",
    "            plt.bar(x_positions, \n",
    "                   level_data['coverage'], \n",
    "                   width=width, \n",
    "                   color=colors[i % len(colors)],\n",
    "                   label=level.title())\n",
    "        \n",
    "        # Set chart title and labels\n",
    "        plt.title('Service Coverage by Census Tract Characteristic', fontsize=14)\n",
    "        plt.ylabel('Coverage Percentage')\n",
    "        plt.ylim(0, 55)  # Set y limit to give space for labels\n",
    "        \n",
    "        # Set x-axis ticks - use an empty string to remove the labels\n",
    "        plt.xticks(np.arange(categories_n), [''] * categories_n)\n",
    "        \n",
    "        # Add level labels beneath each category\n",
    "        for i, category in enumerate(categories):\n",
    "            for j, level in enumerate(levels):\n",
    "                x_pos = i + (j - levels_n/2 + 0.5) * width\n",
    "                plt.text(x_pos, -5, level.title(), \n",
    "                        ha='center', fontsize=10)\n",
    "        \n",
    "        # Add bracket annotations for categories\n",
    "        for i, category in enumerate(categories):\n",
    "            # Draw bracket from first to last bar in category\n",
    "            first_x = i - width * levels_n/2 + width/2\n",
    "            last_x = i + width * levels_n/2 - width/2\n",
    "            mid_x = i\n",
    "            \n",
    "            # Bracket height\n",
    "            y_bracket = -8\n",
    "            bracket_height = 2\n",
    "            \n",
    "            # Draw the horizontal lines\n",
    "            plt.plot([first_x, last_x], [y_bracket, y_bracket], 'k-', lw=1.5)\n",
    "            \n",
    "            # Add the category label\n",
    "            plt.text(mid_x, y_bracket - bracket_height - 2, category.title(), \n",
    "                    ha='center', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.ylim(bottom=-14)  # Extend bottom margin for labels and brackets\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save and verify file\n",
    "        bar_file = f'{vis_dir}/demographic_coverage.png'\n",
    "        plt.savefig(bar_file, dpi=200)\n",
    "        plt.close()\n",
    "        \n",
    "        # Check that file was created successfully\n",
    "        if os.path.exists(bar_file) and os.path.getsize(bar_file) > 0:\n",
    "            print(f\"Successfully created bar chart: {bar_file}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create bar chart or file is empty: {bar_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating bar chart: {str(e)}\")\n",
    "    \n",
    "    # 4) Interactive Folium map of priority areas and stations\n",
    "    try:\n",
    "        m = folium.Map(location=[39.9526, -75.1652], zoom_start=11, tiles='CartoDB positron')\n",
    "        # Priority colors\n",
    "        colors = {'Low Priority':'#fee5d9','Medium Priority':'#fcae91','High Priority':'#fb6a4a','Critical Priority':'#cb181d'}\n",
    "        \n",
    "        # Add tracts\n",
    "        for _, r in census.to_crs('EPSG:4326').iterrows():\n",
    "            fc = folium.GeoJson(\n",
    "                shapely_mapping(r['geometry']), \n",
    "                style_function=lambda f,cat=r['gap_category']: {\n",
    "                    'fillColor': colors.get(cat,'gray'),\n",
    "                    'color': 'black',\n",
    "                    'weight': 1,\n",
    "                    'fillOpacity': 0.7\n",
    "                },\n",
    "                # Enhanced tooltip with more tract info\n",
    "                tooltip=f\"Priority: {r['gap_category']}<br>Population: {int(r['total_pop']) if not pd.isna(r['total_pop']) else 'N/A'}<br>Income: ${int(r['median_income']):,} \" if not pd.isna(r['median_income']) else \"Priority: {r['gap_category']}<br>Population: {int(r['total_pop']) if not pd.isna(r['total_pop']) else 'N/A'}<br>Income: N/A\"\n",
    "            )\n",
    "            fc.add_to(m)\n",
    "            \n",
    "        # Add stations\n",
    "        for _, s in stations.to_crs('EPSG:4326').iterrows():\n",
    "            folium.CircleMarker(\n",
    "                location=[s.geometry.y, s.geometry.x],\n",
    "                radius=5,\n",
    "                color='black',\n",
    "                fill=True,\n",
    "                fill_color='white',\n",
    "                fill_opacity=1,\n",
    "                # Enhanced tooltip with station info\n",
    "                tooltip=f\"Station: {s['name']}<br>Points: {int(s['num_points']) if not pd.isna(s['num_points']) else 'N/A'}<br>Max Power: {s['max_power']:.1f} kW\" if not pd.isna(s['max_power']) else f\"Station: {s['name']}<br>Points: {int(s['num_points']) if not pd.isna(s['num_points']) else 'N/A'}<br>Max Power: N/A\"\n",
    "            ).add_to(m)\n",
    "\n",
    "        # Add a legend\n",
    "        legend_html = '''\n",
    "        <div style=\"position: fixed; \n",
    "                   bottom: 50px; right: 50px; \n",
    "                   border:2px solid grey; z-index:9999; \n",
    "                   background-color:white;\n",
    "                   padding:10px;\n",
    "                   font-size:14px;\n",
    "                   \">\n",
    "        <p><b>Priority Levels</b></p>\n",
    "        <p><i style=\"background: #fee5d9; padding:5px;\">&nbsp;&nbsp;&nbsp;&nbsp;</i> Low Priority</p>\n",
    "        <p><i style=\"background: #fcae91; padding:5px;\">&nbsp;&nbsp;&nbsp;&nbsp;</i> Medium Priority</p>\n",
    "        <p><i style=\"background: #fb6a4a; padding:5px;\">&nbsp;&nbsp;&nbsp;&nbsp;</i> High Priority</p>\n",
    "        <p><i style=\"background: #cb181d; padding:5px;\">&nbsp;&nbsp;&nbsp;&nbsp;</i> Critical Priority</p>\n",
    "        <p><i style=\"background: white; border: 2px solid black; padding:5px;\">&nbsp;&nbsp;&nbsp;&nbsp;</i> EV Station</p>\n",
    "        </div>\n",
    "        '''\n",
    "        m.get_root().html.add_child(folium.Element(legend_html))\n",
    "        \n",
    "        # Save and verify file\n",
    "        map_file = f'{vis_dir}/priority_areas_map.html'\n",
    "        m.save(map_file)\n",
    "        \n",
    "        # Check that file was created successfully\n",
    "        if os.path.exists(map_file) and os.path.getsize(map_file) > 0:\n",
    "            print(f\"Successfully created interactive map: {map_file}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Failed to create interactive map or file is empty: {map_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating interactive map: {str(e)}\")\n",
    "\n",
    "\n",
    "# Filter stations by charging speed and minimum points \n",
    "def filter_stations(stations, charging_speed='All', min_points=1):\n",
    "    \"\"\"\n",
    "    Filter charging stations by charging speed category and minimum number of points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stations : GeoDataFrame\n",
    "        Station data with num_points and charging_speed columns\n",
    "    charging_speed : str\n",
    "        One of: 'All', 'Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)'\n",
    "    min_points : int\n",
    "        Minimum number of charging points (outlets) per station\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        Filtered station dataset\n",
    "    \"\"\"\n",
    "    filtered = stations.copy()\n",
    "    \n",
    "    if charging_speed != 'All':\n",
    "        filtered = filtered[filtered['charging_speed'] == charging_speed]\n",
    "    \n",
    "    filtered = filtered[filtered['num_points'] >= min_points]\n",
    "    \n",
    "    print(f\"Filtered from {len(stations)} to {len(filtered)} stations \" +\n",
    "          f\"(speed: {charging_speed}, min points: {min_points})\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def snap_points_to_network(points_gdf, network, cache_name=None):\n",
    "    \"\"\"\n",
    "    Snap points to the nearest edge on the network and return a new GeoDataFrame\n",
    "    with the same attributes but snapped geometries.\n",
    "    \n",
    "    This improves network analysis accuracy by ensuring that origins and destinations\n",
    "    are properly connected to the network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    points_gdf : GeoDataFrame\n",
    "        Points to snap to the network\n",
    "    network : networkx.Graph\n",
    "        Road network \n",
    "    cache_name : str, optional\n",
    "        Name to use for caching the snapped points. If provided, will try to load\n",
    "        from cache first, and save to cache if not found.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        New GeoDataFrame with original attributes but snapped geometries\n",
    "    \"\"\"\n",
    "    # Check if we should use cached snapped points\n",
    "    if cache_name:\n",
    "        cache_file = os.path.join(CACHE_DIR, f'snapped_{cache_name}.pkl')\n",
    "        \n",
    "        # Try to load from cache\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                print(f\"Loading snapped points from cache: {cache_name}...\")\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    snapped_gdf = pickle.load(f)\n",
    "                print(f\"Loaded {len(snapped_gdf)} snapped points from cache.\")\n",
    "                return snapped_gdf\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading snapped points cache: {e}\")\n",
    "                # Continue with snapping\n",
    "    \n",
    "    print(f\"Snapping {len(points_gdf)} points to network...\")\n",
    "    snapped_points = []\n",
    "    failed_snaps = 0\n",
    "    \n",
    "    for idx, row in tqdm(points_gdf.iterrows(), total=len(points_gdf), desc=\"Snapping points\"):\n",
    "        try:\n",
    "            # Find the nearest edge\n",
    "            nearest_edge = ox.distance.nearest_edges(\n",
    "                network, row.geometry.x, row.geometry.y, return_dist=False)\n",
    "            \n",
    "            # Get the edge geometry (might be a LineString or a geometry attribute)\n",
    "            u, v, _ = nearest_edge  # Unpack edge tuple\n",
    "            \n",
    "            if 'geometry' in network.edges[nearest_edge]:\n",
    "                # If edge has a geometry attribute, use it\n",
    "                edge_geom = network.edges[nearest_edge]['geometry']\n",
    "            else:\n",
    "                # Otherwise, create a line between nodes\n",
    "                u_x, u_y = network.nodes[u]['x'], network.nodes[u]['y']\n",
    "                v_x, v_y = network.nodes[v]['x'], network.nodes[v]['y']\n",
    "                edge_geom = LineString([(u_x, u_y), (v_x, v_y)])\n",
    "            \n",
    "            # Snap point to edge (project point onto line)\n",
    "            projected_point = edge_geom.interpolate(\n",
    "                edge_geom.project(row.geometry)\n",
    "            )\n",
    "            \n",
    "            # Create new row with snapped geometry\n",
    "            new_row = row.copy()\n",
    "            new_row.geometry = projected_point\n",
    "            snapped_points.append(new_row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If snapping fails, keep the original point\n",
    "            if DEBUG_MODE:\n",
    "                print(f\"Failed to snap point {idx}: {str(e)[:100]}... Using original point.\")\n",
    "            failed_snaps += 1\n",
    "            snapped_points.append(row)\n",
    "    \n",
    "    if failed_snaps > 0:\n",
    "        print(f\"Warning: {failed_snaps} points ({failed_snaps/len(points_gdf)*100:.1f}%) could not be snapped to the network.\")\n",
    "    \n",
    "    # Create a new GeoDataFrame with the same attributes\n",
    "    snapped_gdf = gpd.GeoDataFrame(snapped_points, crs=points_gdf.crs)\n",
    "    \n",
    "    # Cache the results if cache_name is provided\n",
    "    if cache_name:\n",
    "        try:\n",
    "            cache_file = os.path.join(CACHE_DIR, f'snapped_{cache_name}.pkl')\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(snapped_gdf, f)\n",
    "            print(f\"Saved snapped points to cache: {cache_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save snapped points cache: {e}\")\n",
    "    \n",
    "    return snapped_gdf\n",
    "\n",
    "\n",
    "# Main entry point to run the complete pipeline\n",
    "if __name__ == '__main__':\n",
    "    # 1) Load data and networks\n",
    "    census, boundary, stations, validation = load_data()\n",
    "    \n",
    "    # Create NetworkX networks\n",
    "    G_walk, G_drive = create_networks()\n",
    "    \n",
    "    print(\"Using NetworkX for network analysis with point snapping\")\n",
    "    \n",
    "    # Optional: Filter stations (uncomment and adjust parameters as needed)\n",
    "    # Possible values for charging_speed: 'All', 'Level 1 (Slow)', 'Level 2 (Medium)', 'DC Fast (Rapid)'\n",
    "    # stations = filter_stations(stations, charging_speed='All', min_points=1)\n",
    "    \n",
    "    # Preprocess: Snap points to network for better connectivity\n",
    "    print(\"\\n=== PREPROCESSING POINTS FOR NETWORK ANALYSIS ===\")\n",
    "    # Snap stations to the road network\n",
    "    stations_snapped = snap_points_to_network(stations, G_drive, cache_name='stations')\n",
    "    print(f\"Using {len(stations_snapped)} snapped stations for network analysis\")\n",
    "    \n",
    "    # Create a GeoDataFrame of census tract centroids\n",
    "    census_centroids = gpd.GeoDataFrame(\n",
    "        geometry=[point for point in census.geometry.centroid],\n",
    "        index=census.index,\n",
    "        crs=census.crs\n",
    "    )\n",
    "    # Snap centroids to the road network\n",
    "    centroids_snapped = snap_points_to_network(census_centroids, G_drive, cache_name='centroids')\n",
    "    print(f\"Using {len(centroids_snapped)} snapped census tract centroids for network analysis\")\n",
    "    print(\"Preprocessing complete. Points are now properly connected to the network.\")\n",
    "    \n",
    "    # 2) Distances & initial coverage\n",
    "    # Use the snapped geometries for better network connectivity\n",
    "    origins = {i: row.geometry for i, row in centroids_snapped.iterrows()}\n",
    "    walk_d, drive_d = calculate_multimodal_distance_batch(\n",
    "        origins, \n",
    "        list(stations_snapped.geometry), \n",
    "        G_walk, \n",
    "        G_drive\n",
    "    )\n",
    "    cov = calculate_coverage_scores(census, walk_d, drive_d, stations)\n",
    "    census = census.join(cov)\n",
    "    \n",
    "    # Flag served tracts\n",
    "    census['is_served'] = census['coverage_score'] > census['coverage_score'].mean()\n",
    "    \n",
    "    # 3) Service areas\n",
    "    # Use snapped stations for better service area generation\n",
    "    areas = build_service_areas(stations_snapped, G_walk, G_drive)\n",
    "    \n",
    "    # 4) Equity analysis\n",
    "    equity_res = perform_equity_analysis(census, areas)\n",
    "    \n",
    "    # 5) Compute dynamic weights and apply gap scoring\n",
    "    dyn_wts = calculate_dynamic_weights(equity_res)\n",
    "    census = apply_gap_scoring(census, dyn_wts)\n",
    "    \n",
    "    # 6) Categorize demographics by quantiles\n",
    "    census['income_category'] = pd.qcut(census['median_income'], 3, labels=['low_income','medium_income','high_income'])\n",
    "    census['education_category'] = pd.qcut(census['bach_degree_rate'], 3, labels=['low_education','medium_education','high_education'])\n",
    "    census['density_category'] = pd.qcut(census['pop_density'], 3, labels=['low_density','medium_density','high_density'])\n",
    "    \n",
    "    # 7) Coverage by demographic groups\n",
    "    demo_cov = compute_coverage_by_demographics(census)\n",
    "    \n",
    "    # 8) Save outputs and visualizations\n",
    "    save_pipeline_outputs(census, stations, areas, equity_res, demo_cov, dyn_wts)\n",
    "    create_visualizations(census, equity_res, demo_cov)\n",
    "\n",
    "    # Print equity analysis results if in debug mode\n",
    "    if DEBUG_MODE:\n",
    "        print(\"\\n=== EQUITY ANALYSIS SUMMARY ===\")\n",
    "        # Calculate average values for served vs unserved areas\n",
    "        served = census[census['is_served']]\n",
    "        unserved = census[~census['is_served']]\n",
    "        \n",
    "        # Print validation statistics first\n",
    "        print(\"\\nValidation Statistics:\")\n",
    "        print(f\"- Total stations from API: {validation['total_stations_from_api']}\")\n",
    "        print(f\"- Stations within boundary: {validation['stations_within_boundary']}\")\n",
    "        print(f\"- Excluded stations: {validation['excluded_stations']}\")\n",
    "        print(f\"  - Outside boundary: {validation['exclusion_reasons']['outside_boundary']}\")\n",
    "        print(f\"  - Missing coordinates: {validation['exclusion_reasons']['missing_coordinates']}\")\n",
    "        \n",
    "        # Print service area info if available\n",
    "        print(\"\\nService Areas:\")\n",
    "        if areas:\n",
    "            for area_name, area_geom in areas.items():\n",
    "                if not area_geom:\n",
    "                    print(f\"- {area_name}: Empty geometry\")\n",
    "                    continue\n",
    "                    \n",
    "                # Count intersecting tracts\n",
    "                intersecting = sum(1 for _, tract in census.iterrows() if tract.geometry.intersects(area_geom))\n",
    "                percent = (intersecting / len(census)) * 100\n",
    "                print(f\"- {area_name}: {intersecting} tracts ({percent:.1f}%)\")\n",
    "        else:\n",
    "            print(\"No service areas were created.\")\n",
    "        \n",
    "        # Print metric comparisons (original code)\n",
    "        print(\"\\nServed vs. Unserved Areas:\")\n",
    "        metrics = ['median_income', 'poverty_rate', 'pop_density', 'bach_degree_rate']\n",
    "        print(f\"{'Metric':<20} {'Served':>12} {'Unserved':>12} {'Percent Diff':>12} {'Significant':>10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for var in metrics:\n",
    "            served_mean = served[var].mean()\n",
    "            unserved_mean = unserved[var].mean()\n",
    "            pct_diff = ((served_mean - unserved_mean) / unserved_mean) * 100 if unserved_mean != 0 else 0\n",
    "            t_stat, p_val = stats.ttest_ind(served[var].dropna(), unserved[var].dropna())\n",
    "            \n",
    "            print(f\"{var:<20} {served_mean:>12.1f} {unserved_mean:>12.1f} {pct_diff:>+12.1f}% {'Yes' if p_val < 0.05 else 'No':>10}\")\n",
    "\n",
    "    # Print dynamic weights if in debug mode\n",
    "    if DEBUG_MODE:\n",
    "        print(\"\\n=== DYNAMIC WEIGHTS ===\")\n",
    "        for k, v in dyn_wts.items():\n",
    "            print(f\"{k}: {v:.4f} ({v*100:.1f}%)\")\n",
    "\n",
    "    # Add validation to pipeline outputs\n",
    "    census['validation'] = validation\n",
    "    print('Complete gap analysis pipeline.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f79c3f",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## Data & Preâ€‘processing\n",
    "- **Census Data**: ACS data loaded from GeoPackage\n",
    "- **Filters**: Tracts with population > 0 and density â‰¥ 1000 people/miÂ²\n",
    "- **Filtered Tracts**: 385 census tracts remained after filtering, with 23 tracts excluded from the original 408 Philadelphia tracts\n",
    "- **Boundary**: Philadelphia city limits from GeoJSON\n",
    "- **Projection**: WGS84 (4326) initially, then State Plane (2272) for analysis\n",
    "\n",
    "## EV Station Retrieval\n",
    "- **API**: OpenChargeMap v3 with 10km search radius\n",
    "- **Station Data**: Extracts coordinates, connection points, max power (kW)\n",
    "- **Station Filtering**: Of 150 stations retrieved from the API, 20 were excluded for being outside the city boundary, leaving 130 stations for analysis\n",
    "- **Station Classification**:\n",
    "  - Level 1 (Slow): < 7 kW\n",
    "  - Level 2 (Medium): 7-50 kW\n",
    "  - DC Fast (Rapid): > 50 kW\n",
    "- **Validation**: Tracks excluded stations and reasons (outside boundary, missing coordinates)\n",
    "\n",
    "## Network Analysis\n",
    "- **Networks**: OSMnx-generated walking and driving networks\n",
    "- **Point Snapping**: Stations and census tract centroids are snapped to the nearest edge on the network for accurate routing\n",
    "- **Distance Calculation**: NetworkX-based shortest path routing with straight-line distance fallback\n",
    "- **Distance Filter**: 6km threshold to eliminate unnecessary calculations for distant pairs\n",
    "- **Caching**: Extensive caching of networks, snapped points, distances, service areas, and equity analysis for performance\n",
    "- **Service Areas**:\n",
    "  - Walking: 1,320ft (5min), 2,640ft (10min), 3,960ft (15min)\n",
    "  - Driving: 5,280ft (1mi), 10,560ft (2mi), 15,840ft (3mi)\n",
    "\n",
    "## Methodology: Computing the EV Charging \"Gap Score\"\n",
    "\n",
    "The **gap score** is a single composite metric that ranks each Philadelphia census tract by its need for additional EV charging infrastructure. It blends:\n",
    "\n",
    "1. A **coverage** metric (physical access to existing chargers)  \n",
    "2. Four **socioeconomic vulnerability** metrics (income, poverty, density, education)\n",
    "\n",
    "Together, these capture both *where* chargers are missing **and** *who* is most harmed by those gaps.\n",
    "\n",
    "### 1. Coverage Score (24%)\n",
    "\n",
    "#### What it measures  \n",
    "Per-tract service level based on walking and driving proximity, station quality, and population density.\n",
    "\n",
    "#### Calculation steps  \n",
    "1. **Compute network distances**  \n",
    "   - Shortest-path walking (NetworkX with point snapping) from each tract centroid to all chargers.\n",
    "   - Shortest-path driving likewise.\n",
    "   - Straight-line distance with detour factor (1.3Ã— for walking, 1.5Ã— for driving) as fallback when routing fails.\n",
    "\n",
    "2. **Apply proximity bands**  \n",
    "   - **Walking**:  \n",
    "     - Within 1,320 ft (~5 min): 0.7 score\n",
    "     - Within 2,640 ft (~10 min): 0.5 score\n",
    "     - Within 3,960 ft (~15 min): 0.3 score\n",
    "     - Beyond: 1.0 score (worst)\n",
    "   \n",
    "   - **Driving**:  \n",
    "     - Within 5,280 ft (~1 mile): 0.7 score\n",
    "     - Within 10,560 ft (~2 miles): 0.5 score\n",
    "     - Within 15,840 ft (~3 miles): 0.3 score\n",
    "     - Beyond: 1.0 score (worst)\n",
    "\n",
    "3. **Station quality adjustment**  \n",
    "   - Calculate quality index for each station:\n",
    "     $$\\text{quality\\_index} = \\text{num\\_points} \\times \\sqrt{\\frac{\\text{max\\_power}}{50}}$$\n",
    "   - Normalize to 0.5-1.5 range: \n",
    "     $$\\text{quality\\_factor} = 0.5 + \\frac{\\text{quality\\_index} - \\min(\\text{quality\\_index})}{\\max(\\text{quality\\_index}) - \\min(\\text{quality\\_index})}$$\n",
    "   - Multiply coverage by quality factor\n",
    "\n",
    "4. **Combine modes**  \n",
    "   $$\\text{coverage\\_score}_i = 0.4 \\times \\text{walk\\_weight}_i + 0.6 \\times \\text{drive\\_weight}_i$$\n",
    "\n",
    "5. **Density adjustment**  \n",
    "   - Scale down by up to 30% for high-density tracts:\n",
    "   $$\\text{coverage\\_score}_i \\times= (1 - 0.3 \\times \\frac{\\text{pop\\_density}_i}{\\max(\\text{pop\\_density})})$$\n",
    "\n",
    "6. **Label \"served\" vs. \"unserved\"**  \n",
    "   - **Served** if coverage_score > mean(coverage_score)  \n",
    "   - **Unserved** otherwise  \n",
    "\n",
    "### 2. Socioeconomic Component Scores (Combined 76%)\n",
    "\n",
    "We compute four additional 0â€“1 scores that capture relative vulnerability:\n",
    "\n",
    "| Component      | Weight | Rationale                                                         |\n",
    "|:---------------|:------:|:------------------------------------------------------------------|\n",
    "| **Income**     | 16%    | Lower-income areas may have fewer EV adopters and fewer resources to retrofit homes. |\n",
    "| **Poverty**    | 24%    | High poverty correlates with lower EV uptake and higher energy burden. |\n",
    "| **Density**    | 20%    | More people in a tract â†’ higher absolute charging demand.        |\n",
    "| **Education**  | 16%    | Education level often correlates with technology adoption rates.  |\n",
    "\n",
    "#### How we compute each score  \n",
    "1. Pull ACS census data for each tract.  \n",
    "2. For each metric (e.g. median income), scale the raw value so that the **most vulnerable** tract gets 1.0 and the **least vulnerable** gets 0.0.  \n",
    "   - e.g. `income_score_i = 1 - (income_i âˆ’ min)/ (max âˆ’ min)`  \n",
    "3. Clip to [0, 1] range.\n",
    "\n",
    "### 3. Combining into the Gap Score\n",
    "\n",
    "For each tract $i$, we use a simple linear combination of the normalized components:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{gap\\_score}_i \n",
    "&= w_{\\text{cov}}(1 - \\text{coverage}_i) \\\\\n",
    "&\\quad + w_{\\text{inc}}\\text{income\\_score}_i \n",
    "+ w_{\\text{pov}}\\text{poverty\\_score}_i \\\\\n",
    "&\\quad + w_{\\text{den}}\\text{density\\_score}_i \n",
    "+ w_{\\text{edu}}\\text{education\\_score}_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "This linear model ensures transparency and interpretability in how each factor contributes to the final score.\n",
    "\n",
    "#### Component directions:\n",
    "- **Coverage**: Inverted (1 - score) so higher values indicate worse coverage\n",
    "- **Income**: Inverted so lower incomes result in higher scores\n",
    "- **Poverty**: Higher poverty rates result in higher scores\n",
    "- **Density**: Higher population density results in higher scores\n",
    "- **Education**: Inverted so lower education levels result in higher scores\n",
    "\n",
    "#### Dynamic weights\n",
    "Weights are calculated based on observed disparities:\n",
    "\n",
    "1. We start with base weights that sum to 1.0:\n",
    "   - $w_{\\text{cov}} = 0.30$  \n",
    "   - $w_{\\text{inc}} = 0.20$  \n",
    "   - $w_{\\text{pov}} = 0.20$  \n",
    "   - $w_{\\text{den}} = 0.15$  \n",
    "   - $w_{\\text{edu}} = 0.15$\n",
    "\n",
    "2. We calculate effect sizes (Cohen's d) for each socioeconomic variable between served and unserved areas.\n",
    "\n",
    "3. For metrics with statistically significant disparities (p < 0.05), we adjust weights:\n",
    "   - Small effect (0.2-0.5): +5% weight\n",
    "   - Medium effect (0.5-0.8): +10% weight\n",
    "   - Large effect (>0.8): +15% weight\n",
    "\n",
    "4. We then re-normalize all weights to sum to 1.0.\n",
    "\n",
    "5. In our implementation, this resulted in the following weights:\n",
    "   - $w_{\\text{cov}} = 0.24$  \n",
    "   - $w_{\\text{inc}} = 0.16$  \n",
    "   - $w_{\\text{pov}} = 0.24$  \n",
    "   - $w_{\\text{den}} = 0.20$  \n",
    "   - $w_{\\text{edu}} = 0.16$\n",
    "\n",
    "### 4. Thresholding & Priority Tiers\n",
    "\n",
    "To turn a continuous score into actionable categories, we use fixed thresholds:\n",
    "\n",
    "- **Low Priority**: gap_score â‰¤ 0.45\n",
    "- **Medium Priority**: 0.45 < gap_score â‰¤ 0.55\n",
    "- **High Priority**: 0.55 < gap_score â‰¤ 0.65\n",
    "- **Critical Priority**: gap_score > 0.65\n",
    "\n",
    "These thresholds create a balanced distribution of priority levels, with approximately 30% Low, 28% Medium, 29% High, and 13% Critical priority tracts.\n",
    "\n",
    "### 5. Validation & Outputs\n",
    "\n",
    "The analysis pipeline tracks:\n",
    "- Excluded stations and reasons for exclusion\n",
    "- Demographic coverage across income, education, and density levels\n",
    "- Statistical significance of socioeconomic disparities\n",
    "- Component contributions to the final scores\n",
    "- Distribution of priority categories with tract counts and percentages\n",
    "---\n",
    "\n",
    "**Footnote:**  \n",
    "Tracts with large **groupâ€‘quarters** populations (e.g. university dorms) may show artificially high poverty. We document this caveat but retain them in the analysis to avoid geographic bias.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
